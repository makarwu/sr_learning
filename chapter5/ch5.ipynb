{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ccc2a014-6dd0-47c5-b9a5-b760b36dbd91",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "matplotlib version: 3.9.0\n",
      "numpy version: 1.26.4\n",
      "tiktoken version: 0.7.0\n",
      "torch version: 2.2.2\n",
      "tensorflow version: 2.16.1\n"
     ]
    }
   ],
   "source": [
    "from importlib.metadata import version\n",
    "\n",
    "pkgs = [\"matplotlib\", \n",
    "        \"numpy\", \n",
    "        \"tiktoken\", \n",
    "        \"torch\",\n",
    "        \"tensorflow\" # For OpenAI's pretrained weights\n",
    "       ]\n",
    "for p in pkgs:\n",
    "    print(f\"{p} version: {version(p)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e470b802",
   "metadata": {},
   "source": [
    "### Chapter 5: Pretraining on Unlabeled Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e1f3f792",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from previous_chapters import GPTModel\n",
    "\n",
    "GPT_CONFIG_124M = {\n",
    "    \"vocab_size\": 50257,   # Vocabulary size\n",
    "    \"context_length\": 256, # Shortened context length (orig: 1024)\n",
    "    \"emb_dim\": 768,        # Embedding dimension\n",
    "    \"n_heads\": 12,         # Number of attention heads\n",
    "    \"n_layers\": 12,        # Number of layers\n",
    "    \"drop_rate\": 0.1,      # Dropout rate\n",
    "    \"qkv_bias\": False      # Query-key-value bias\n",
    "}\n",
    "\n",
    "torch.manual_seed(123)\n",
    "model = GPTModel(GPT_CONFIG_124M)\n",
    "model.eval();  # Disable dropout during inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8dde227e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output text:\n",
      " Every effort moves you rentingetic wasnÙ… refres RexMeCHicular stren\n"
     ]
    }
   ],
   "source": [
    "import tiktoken\n",
    "from previous_chapters import generate_text_simple\n",
    "\n",
    "def text_to_token_ids(text, tokenizer):\n",
    "    encoded = tokenizer.encode(text, allowed_special={'<|endoftext|>'})\n",
    "    encoded_tensor = torch.tensor(encoded).unsqueeze(0) # add batch dimension\n",
    "    return encoded_tensor\n",
    "\n",
    "def token_ids_to_text(token_ids, tokenizer):\n",
    "    flat = token_ids.squeeze(0) # remove the batch dimension\n",
    "    return tokenizer.decode(flat.tolist())\n",
    "\n",
    "start_context = \"Every effort moves you\"\n",
    "tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
    "\n",
    "token_ids = generate_text_simple(\n",
    "    model=model,\n",
    "    idx=text_to_token_ids(start_context, tokenizer),\n",
    "    max_new_tokens=10,\n",
    "    context_size=GPT_CONFIG_124M[\"context_length\"]\n",
    ")\n",
    "\n",
    "print(\"Output text:\\n\", token_ids_to_text(token_ids, tokenizer))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24f58b44",
   "metadata": {},
   "source": [
    "- As we can see the model does not produce text, since we didn't rain it yet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "62659b54",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = torch.tensor([[16833, 3626, 6100],   # [\"every effort moves\",\n",
    "                       [40,    1107, 588]])   #  \"I really like\"]\n",
    "\n",
    "targets = torch.tensor([[3626, 6100, 345  ],  # [\" effort moves you\",\n",
    "                        [1107,  588, 11311]]) #  \" really like chocolate\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "290ee9d8",
   "metadata": {},
   "source": [
    "- Feeding the inputs to the model, we obtain the logits vector for the 2 input examples that consist of 3 tokens each\n",
    "- Each of the tokens is a 50,257-dimensional vector corresponding to the size of the vocabulary\n",
    "- Applying softmax, we can turn the logits tensor into a tensor of the same dimension containing probability scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1a9f7ef2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 3, 50257])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    logits = model(inputs)\n",
    "\n",
    "probas = torch.softmax(logits, dim=-1) # Probability of each token in vocabulary\n",
    "probas.shape\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "026f4d97",
   "metadata": {},
   "source": [
    "- The softmax function above produced a 50,257-dimensional vector for each token; the `argmax` function returns the position of the highest probability score in this vector, which is the predicted token ID for the given token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0d09dec9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[16657],\n",
       "         [  339],\n",
       "         [42826]],\n",
       "\n",
       "        [[49906],\n",
       "         [29669],\n",
       "         [41751]]])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token_ids = torch.argmax(probas, dim=-1, keepdim=True)\n",
    "token_ids"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30077e24",
   "metadata": {},
   "source": [
    "- If we decode these tokens, we find that these are quite different from the tokens we want the model to predict, namely the target tokens:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b581b55d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Targets batch 1:  effort moves you\n",
      "Outputs batch 1:  Armed heNetflix\n"
     ]
    }
   ],
   "source": [
    "print(f\"Targets batch 1: {token_ids_to_text(targets[0], tokenizer)}\")\n",
    "print(f\"Outputs batch 1: {token_ids_to_text(token_ids[0].flatten(), tokenizer)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "606b4ab1",
   "metadata": {},
   "source": [
    "- To train the model, we need to know how far it is away from the correct predictions (targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ed5ae003",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "text 1:  tensor([7.4541e-05, 3.1061e-05, 1.1563e-05])\n",
      "text 2:  tensor([1.0337e-05, 5.6776e-05, 4.7559e-06])\n"
     ]
    }
   ],
   "source": [
    "text_idx = 0\n",
    "target_probas_1 = probas[text_idx, [0, 1, 2], targets[text_idx]]\n",
    "print(\"text 1: \", target_probas_1)\n",
    "\n",
    "text_idx = 1\n",
    "target_probas_2 = probas[text_idx, [0, 1, 2], targets[text_idx]]\n",
    "print(\"text 2: \", target_probas_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c318d99a",
   "metadata": {},
   "source": [
    "- We want to maximize all these values, bringing them close to a probability of 1\n",
    "- In the mathematical optimization, it is easier to maximize the logarithm of the probability score than the probability score itself"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "37d7a990",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ -9.5042, -10.3796, -11.3677, -11.4798,  -9.7764, -12.2561])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "log_probas = torch.log(torch.cat((target_probas_1, target_probas_2)))\n",
    "log_probas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f07ef542",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(-10.7940)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "avg_log_probas = torch.mean(log_probas)\n",
    "avg_log_probas"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e119965f",
   "metadata": {},
   "source": [
    "- The goal is to make this average log probability as large as possible by optimizing the model weights\n",
    "- Due to the log, the largest value is 0\n",
    "- Instead, we can minimize the **negative average log-probability value** or with other words the **Cross-Entropy-Loss**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "306d8cea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(10.7940)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "neg_avg_log_probas = avg_log_probas * -1\n",
    "neg_avg_log_probas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0194663c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "logits shape:  torch.Size([2, 3, 50257])\n",
      "targets shape:  torch.Size([2, 3])\n"
     ]
    }
   ],
   "source": [
    "print(\"logits shape: \", logits.shape)\n",
    "print(\"targets shape: \", targets.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b1edc96",
   "metadata": {},
   "source": [
    "- For the cross entropy loss function in torch, we want to flatten these tensors by combining them over the batch dimension:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "67f36085",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Flattened logits: torch.Size([6, 50257])\n",
      "Flattened targets: torch.Size([6])\n"
     ]
    }
   ],
   "source": [
    "logits_flat = logits.flatten(0, 1)\n",
    "targets_flat = targets.flatten()\n",
    "\n",
    "print(\"Flattened logits:\", logits_flat.shape)\n",
    "print(\"Flattened targets:\", targets_flat.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "792dbbe9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(10.7940)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss = torch.nn.functional.cross_entropy(logits_flat, targets_flat)\n",
    "loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "228d89c3",
   "metadata": {},
   "source": [
    "- A concept related to the cross entropy loss is the perplexity of an LLM\n",
    "- It's just the exponential of the cross-entropy-loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e3c63393",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(48725.8203)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "perplexity = torch.exp(loss)\n",
    "perplexity"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ad5cb5c",
   "metadata": {},
   "source": [
    "- It is often considered more interpretable because it can be understood as the effective vocabulary size that the model is uncertain about each step (here: 48,725 tokens)\n",
    "- In other words, perplexity provides a measure of how well the probability distribution predicted by the model matches the actual distribution of the words in the dataset\n",
    "- Similar to the loss, a lower preplexity indicated that the model predictions are closer to the actual distribution\n",
    "\n",
    "### Calculating the training and validation set losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "4aff6ebd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'I HAD always thought Jack Gisburn rather a cheap genius--though a good fellow enough--so it was no '"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import urllib.request\n",
    "\n",
    "with open('the-verdict.txt', 'r', encoding=\"utf-8\") as file:\n",
    "    text_data = file.read()\n",
    "\n",
    "text_data[:99]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "677997ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "total_characters = len(text_data)\n",
    "total_tokens = len(tokenizer.encode(text_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "1227fb34",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20479"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "total_characters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "04a6b957",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5145"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "total_tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03311f46",
   "metadata": {},
   "source": [
    "- Next, we devide the dataset into a training and a validation set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "7d2f7078",
   "metadata": {},
   "outputs": [],
   "source": [
    "from previous_chapters import create_dataloader_v1\n",
    "\n",
    "# Train/validation ratio\n",
    "train_ratio = 0.90\n",
    "split_idx = int(train_ratio * len(text_data))\n",
    "train_data = text_data[:split_idx]\n",
    "val_data = text_data[split_idx:]\n",
    "\n",
    "\n",
    "torch.manual_seed(123)\n",
    "\n",
    "train_loader = create_dataloader_v1(\n",
    "    train_data,\n",
    "    batch_size=2,\n",
    "    max_length=GPT_CONFIG_124M[\"context_length\"],\n",
    "    stride=GPT_CONFIG_124M[\"context_length\"],\n",
    "    drop_last=True,\n",
    "    shuffle=True,\n",
    "    num_workers=0\n",
    ")\n",
    "\n",
    "val_loader = create_dataloader_v1(\n",
    "    val_data,\n",
    "    batch_size=2,\n",
    "    max_length=GPT_CONFIG_124M[\"context_length\"],\n",
    "    stride=GPT_CONFIG_124M[\"context_length\"],\n",
    "    drop_last=False,\n",
    "    shuffle=False,\n",
    "    num_workers=0\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "2179c30b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n"
     ]
    }
   ],
   "source": [
    "for x, y in train_loader:\n",
    "    print(x.shape, y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "17bba50b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 256]) torch.Size([2, 256])\n"
     ]
    }
   ],
   "source": [
    "for x, y in val_loader:\n",
    "    print(x.shape, y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "c03fe401",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5120"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_tokens = 0\n",
    "for input_batch, _ in train_loader:\n",
    "    train_tokens += input_batch.numel()\n",
    "\n",
    "val_tokens = 0\n",
    "for input_batch, _ in val_loader:\n",
    "    val_tokens += input_batch.numel()\n",
    "\n",
    "total_tokens = train_tokens + val_tokens\n",
    "total_tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d5b47f1",
   "metadata": {},
   "source": [
    "- Next, we implement a utility function to calculate the cross entropy loss of a given batch\n",
    "- In addition, we implement a 2nd utility function to compute the loss for a user-specified number of batches in a data loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "e3d19b4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_loss_batch(input_batch, target_batch, model, device):\n",
    "    input_batch, target_batch = input_batch.to(device), target_batch.to(device)\n",
    "    logits = model(input_batch)\n",
    "    loss = torch.nn.functional.cross_entropy(logits.flatten(0, 1), target_batch.flatten())\n",
    "    return loss\n",
    "\n",
    "def calc_loss_loader(data_loader, model, device, num_batches=None):\n",
    "    total_loss = 0.\n",
    "    if len(data_loader) == 0:\n",
    "        return float(\"nan\")\n",
    "    elif num_batches is None:\n",
    "        num_batches = len(data_loader)\n",
    "    else:\n",
    "        # Reduce the number of batches to match the total number of batches in the data loader\n",
    "        # if num_batches exceeds the number of batches in the data loader\n",
    "        num_batches = min(num_batches, len(data_loader))\n",
    "    for i, (input_batch, target_batch) in enumerate(data_loader):\n",
    "        if i < num_batches:\n",
    "            loss = calc_loss_batch(input_batch, target_batch, model, device)\n",
    "            total_loss += loss.item()\n",
    "        else:\n",
    "            break\n",
    "    return total_loss / num_batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "4c7a71cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss:  10.987583584255642\n",
      "val loss:  10.98110580444336\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "torch.manual_seed(123)\n",
    "\n",
    "with torch.no_grad(): # Disable gradient tracking for efficiency because we are not training, yet\n",
    "    train_loss = calc_loss_loader(train_loader, model, device)\n",
    "    val_loss = calc_loss_loader(val_loader, model, device)\n",
    "\n",
    "print(\"train loss: \", train_loss)\n",
    "print(\"val loss: \", val_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f738d80",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
