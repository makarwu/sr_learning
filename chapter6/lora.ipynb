{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction to LoRA\n",
    "- Low-rank adaption (LoRA) is a machine learning technique that modifies a pretrained model to better suit a specific, often smaller, dataset by adjusting a small, low-rank subset of the model's parameters \n",
    "- This approach is important because it allows for efficient finetuning of large models on task-specific data, significantly reducing the computational cost and time required for finetuning\n",
    "- Suppose we have a large weight matrix $W$ for a given layer\n",
    "- During backpropagation, we learn a $\\Delta W$ matrix, which contains information on how much we want to update the original weights to minimize the loss function during training: $W_\\text{updated}=W+\\Delta W$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File downloaded and saved as sms_spam_collection/SMSSpamCollection.tsv\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "from previous_chapters2 import (\n",
    "    download_and_unzip_spam_data,\n",
    "    create_balanced_dataset,\n",
    "    random_split\n",
    ")\n",
    "\n",
    "\n",
    "url = \"https://archive.ics.uci.edu/static/public/228/sms+spam+collection.zip\"\n",
    "zip_path = \"sms_spam_collection.zip\"\n",
    "extracted_path = \"sms_spam_collection\"\n",
    "data_file_path = Path(extracted_path) / \"SMSSpamCollection.tsv\"\n",
    "\n",
    "download_and_unzip_spam_data(url, zip_path, extracted_path, data_file_path)\n",
    "\n",
    "df = pd.read_csv(data_file_path, sep=\"\\t\", header=None, names=[\"Label\", \"Text\"])\n",
    "balanced_df = create_balanced_dataset(df)\n",
    "balanced_df[\"Label\"] = balanced_df[\"Label\"].map({\"ham\": 0, \"spam\": 1})\n",
    "\n",
    "train_df, validation_df, test_df = random_split(balanced_df, 0.7, 0.1)\n",
    "train_df.to_csv(\"train.csv\", index=None)\n",
    "validation_df.to_csv(\"validation.csv\", index=None)\n",
    "test_df.to_csv(\"test.csv\", index=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "import tiktoken\n",
    "from previous_chapters2 import SpamDataset\n",
    "\n",
    "\n",
    "tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
    "train_dataset = SpamDataset(\"train.csv\", max_length=None, tokenizer=tokenizer)\n",
    "val_dataset = SpamDataset(\"validation.csv\", max_length=train_dataset.max_length, tokenizer=tokenizer)\n",
    "test_dataset = SpamDataset(\"test.csv\", max_length=train_dataset.max_length, tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "num_workers = 0\n",
    "batch_size = 8\n",
    "\n",
    "torch.manual_seed(123)\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    dataset=train_dataset,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True,\n",
    "    num_workers=num_workers,\n",
    "    drop_last=True,\n",
    ")\n",
    "\n",
    "val_loader = DataLoader(\n",
    "    dataset=val_dataset,\n",
    "    batch_size=batch_size,\n",
    "    num_workers=num_workers,\n",
    "    drop_last=False,\n",
    ")\n",
    "\n",
    "test_loader = DataLoader(\n",
    "    dataset=test_dataset,\n",
    "    batch_size=batch_size,\n",
    "    num_workers=num_workers,\n",
    "    drop_last=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-06-23 14:04:33.045115: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "checkpoint: 100%|██████████| 77.0/77.0 [00:00<00:00, 16.0kiB/s]\n",
      "encoder.json: 100%|██████████| 1.04M/1.04M [00:00<00:00, 1.16MiB/s]\n",
      "hparams.json: 100%|██████████| 90.0/90.0 [00:00<00:00, 113kiB/s]\n",
      "model.ckpt.data-00000-of-00001: 100%|██████████| 498M/498M [01:54<00:00, 4.36MiB/s]  \n",
      "model.ckpt.index: 100%|██████████| 5.21k/5.21k [00:00<00:00, 1.79MiB/s]\n",
      "model.ckpt.meta: 100%|██████████| 471k/471k [00:00<00:00, 828kiB/s] \n",
      "vocab.bpe: 100%|██████████| 456k/456k [00:00<00:00, 673kiB/s] \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GPTModel(\n",
       "  (tok_emb): Embedding(50257, 768)\n",
       "  (pos_emb): Embedding(1024, 768)\n",
       "  (drop_emb): Dropout(p=0.0, inplace=False)\n",
       "  (trf_blocks): Sequential(\n",
       "    (0): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_resid): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (1): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_resid): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (2): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_resid): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (3): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_resid): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (4): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_resid): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (5): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_resid): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (6): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_resid): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (7): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_resid): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (8): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_resid): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (9): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_resid): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (10): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_resid): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (11): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_resid): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "  )\n",
       "  (final_norm): LayerNorm()\n",
       "  (out_head): Linear(in_features=768, out_features=50257, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from gpt_download import download_and_load_gpt2\n",
    "from previous_chapters2 import GPTModel, load_weights_into_gpt\n",
    "\n",
    "\n",
    "CHOOSE_MODEL = \"gpt2-small (124M)\"\n",
    "INPUT_PROMPT = \"Every effort moves\"\n",
    "\n",
    "BASE_CONFIG = {\n",
    "    \"vocab_size\": 50257,     # Vocabulary size\n",
    "    \"context_length\": 1024,  # Context length\n",
    "    \"drop_rate\": 0.0,        # Dropout rate\n",
    "    \"qkv_bias\": True         # Query-key-value bias\n",
    "}\n",
    "\n",
    "model_configs = {\n",
    "    \"gpt2-small (124M)\": {\"emb_dim\": 768, \"n_layers\": 12, \"n_heads\": 12},\n",
    "    \"gpt2-medium (355M)\": {\"emb_dim\": 1024, \"n_layers\": 24, \"n_heads\": 16},\n",
    "    \"gpt2-large (774M)\": {\"emb_dim\": 1280, \"n_layers\": 36, \"n_heads\": 20},\n",
    "    \"gpt2-xl (1558M)\": {\"emb_dim\": 1600, \"n_layers\": 48, \"n_heads\": 25},\n",
    "}\n",
    "\n",
    "BASE_CONFIG.update(model_configs[CHOOSE_MODEL])\n",
    "\n",
    "model_size = CHOOSE_MODEL.split(\" \")[-1].lstrip(\"(\").rstrip(\")\")\n",
    "settings, params = download_and_load_gpt2(model_size=model_size, models_dir=\"gpt2\")\n",
    "\n",
    "model = GPTModel(BASE_CONFIG)\n",
    "load_weights_into_gpt(model, params)\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Every effort moves you forward.\n",
      "\n",
      "The first step is to understand the importance of your work\n"
     ]
    }
   ],
   "source": [
    "from previous_chapters2 import (\n",
    "    generate_text_simple,\n",
    "    text_to_token_ids,\n",
    "    token_ids_to_text\n",
    ")\n",
    "\n",
    "\n",
    "text_1 = \"Every effort moves you\"\n",
    "\n",
    "token_ids = generate_text_simple(\n",
    "    model=model,\n",
    "    idx=text_to_token_ids(text_1, tokenizer),\n",
    "    max_new_tokens=15,\n",
    "    context_size=BASE_CONFIG[\"context_length\"]\n",
    ")\n",
    "\n",
    "print(token_ids_to_text(token_ids, tokenizer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(123)\n",
    "\n",
    "num_classes = 2\n",
    "model.out_head = torch.nn.Linear(in_features=768, out_features=num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device);  # no assignment model = model.to(device) necessary for nn.Module classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training accuracy: 46.25%\n",
      "Validation accuracy: 45.00%\n",
      "Test accuracy: 48.75%\n"
     ]
    }
   ],
   "source": [
    "from previous_chapters2 import calc_accuracy_loader\n",
    "\n",
    "\n",
    "torch.manual_seed(123)\n",
    "train_accuracy = calc_accuracy_loader(train_loader, model, device, num_batches=10)\n",
    "val_accuracy = calc_accuracy_loader(val_loader, model, device, num_batches=10)\n",
    "test_accuracy = calc_accuracy_loader(test_loader, model, device, num_batches=10)\n",
    "\n",
    "print(f\"Training accuracy: {train_accuracy*100:.2f}%\")\n",
    "print(f\"Validation accuracy: {val_accuracy*100:.2f}%\")\n",
    "print(f\"Test accuracy: {test_accuracy*100:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parameter-efficient finetuning with LoRA\n",
    "- We begin with a LoRA Layer that creates the matrices $A$ and $B$, along with the `alpha` scaling hyperparameter and the `rank` $(r)$ hyperparameters\n",
    "- This layer can accept an input and compute the corresponding output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "class LoRALayer(torch.nn.Module):\n",
    "    def __init__(self, in_dim, out_dim, rank, alpha):\n",
    "        super().__init__()\n",
    "        self.A = torch.nn.Parameter(torch.empty(in_dim, rank))\n",
    "        torch.nn.init.kaiming_uniform_(self.A, a=math.sqrt(5)) # similar to standard weight initialization\n",
    "        self.B = torch.nn.Parameter(torch.zeros(rank, out_dim))\n",
    "        self.alpha = alpha\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.alpha * (x @ self.A @ self.B)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Here, `rank` controls the inner dimensions of the matrices $A$ and $B$\n",
    "- In other words, this parameter controls the number of additional parameters intoduced by LoRA and is a key factor in determining the balance between model adaptability and parameter efficiency\n",
    "- `Alpha` is a scaling hyperparameter applied to the output of the low-rank adaption\n",
    "- it contols the extent to which the adapted layer's output is allowed to influece the original output of the layer being adapted\n",
    "- This can be seen as a way to regulate the impact of the low-rank adaptation on the layer's output\n",
    "- We are usually interested in replacing existing `Linear` layers so that the weight update is applied to the existing pretrained weights\n",
    "\n",
    "\n",
    "- To incorporate the original `Linear` layer weights as shown in the figure above, we implement a `LinearWithLoRA` layer that uses the implemented `LoRALayer` and can be used to replace existing `Linear` layers in a neural network, e.g. self-attention module or feed forward modules in an LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinearWithLoRA(torch.nn.Module):\n",
    "    def __init__(self, linear, rank, alpha):\n",
    "        super().__init__()\n",
    "        self.linear = linear\n",
    "        self.lora = LoRALayer(\n",
    "            linear.in_features, linear.out_features, rank, alpha\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.linear(x) + self.lora(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Note, we initialized the weight matrix $B$ in the `LoRALayer` with zeros values. The matrix multiplication between $A$ and $B$ results in a matrix consisting of 0's and doesn't affect the original weights (since adding 0 weights doesn't modify them)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def replace_linear_with_lora(model, rank, alpha):\n",
    "    for name, module in model.named_children():\n",
    "        if isinstance(module, torch.nn.Linear):\n",
    "            # Replace the Linear layer with LinearWithLoRA\n",
    "            setattr(model, name, LinearWithLoRA(module, rank, alpha))\n",
    "        else:\n",
    "            # Recursively apply the same function to child modules\n",
    "            replace_linear_with_lora(module, rank, alpha)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- We then freeze the original model parameter and use the `replace_linear_with_lora` to replace the said `Linear` layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total trainable parameters before:  7999584\n",
      "Total trainable params after:  0\n"
     ]
    }
   ],
   "source": [
    "total_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "print(\"Total trainable parameters before: \", total_params)\n",
    "\n",
    "for param in model.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "total_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "print(\"Total trainable params after: \", total_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total trainable LoRA parameters: 2,666,528\n"
     ]
    }
   ],
   "source": [
    "replace_linear_with_lora(model, rank=16, alpha=16)\n",
    "\n",
    "total_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "print(f\"Total trainable LoRA parameters: {total_params:,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPTModel(\n",
      "  (tok_emb): Embedding(50257, 768)\n",
      "  (pos_emb): Embedding(1024, 768)\n",
      "  (drop_emb): Dropout(p=0.0, inplace=False)\n",
      "  (trf_blocks): Sequential(\n",
      "    (0): TransformerBlock(\n",
      "      (att): MultiHeadAttention(\n",
      "        (W_query): LinearWithLoRA(\n",
      "          (linear): LinearWithLoRA(\n",
      "            (linear): LinearWithLoRA(\n",
      "              (linear): LinearWithLoRA(\n",
      "                (linear): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (lora): LoRALayer()\n",
      "              )\n",
      "              (lora): LoRALayer()\n",
      "            )\n",
      "            (lora): LoRALayer()\n",
      "          )\n",
      "          (lora): LoRALayer()\n",
      "        )\n",
      "        (W_key): LinearWithLoRA(\n",
      "          (linear): LinearWithLoRA(\n",
      "            (linear): LinearWithLoRA(\n",
      "              (linear): LinearWithLoRA(\n",
      "                (linear): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (lora): LoRALayer()\n",
      "              )\n",
      "              (lora): LoRALayer()\n",
      "            )\n",
      "            (lora): LoRALayer()\n",
      "          )\n",
      "          (lora): LoRALayer()\n",
      "        )\n",
      "        (W_value): LinearWithLoRA(\n",
      "          (linear): LinearWithLoRA(\n",
      "            (linear): LinearWithLoRA(\n",
      "              (linear): LinearWithLoRA(\n",
      "                (linear): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (lora): LoRALayer()\n",
      "              )\n",
      "              (lora): LoRALayer()\n",
      "            )\n",
      "            (lora): LoRALayer()\n",
      "          )\n",
      "          (lora): LoRALayer()\n",
      "        )\n",
      "        (out_proj): LinearWithLoRA(\n",
      "          (linear): LinearWithLoRA(\n",
      "            (linear): LinearWithLoRA(\n",
      "              (linear): LinearWithLoRA(\n",
      "                (linear): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (lora): LoRALayer()\n",
      "              )\n",
      "              (lora): LoRALayer()\n",
      "            )\n",
      "            (lora): LoRALayer()\n",
      "          )\n",
      "          (lora): LoRALayer()\n",
      "        )\n",
      "        (dropout): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (ff): FeedForward(\n",
      "        (layers): Sequential(\n",
      "          (0): LinearWithLoRA(\n",
      "            (linear): LinearWithLoRA(\n",
      "              (linear): LinearWithLoRA(\n",
      "                (linear): LinearWithLoRA(\n",
      "                  (linear): Linear(in_features=768, out_features=3072, bias=True)\n",
      "                  (lora): LoRALayer()\n",
      "                )\n",
      "                (lora): LoRALayer()\n",
      "              )\n",
      "              (lora): LoRALayer()\n",
      "            )\n",
      "            (lora): LoRALayer()\n",
      "          )\n",
      "          (1): GELU()\n",
      "          (2): LinearWithLoRA(\n",
      "            (linear): LinearWithLoRA(\n",
      "              (linear): LinearWithLoRA(\n",
      "                (linear): LinearWithLoRA(\n",
      "                  (linear): Linear(in_features=3072, out_features=768, bias=True)\n",
      "                  (lora): LoRALayer()\n",
      "                )\n",
      "                (lora): LoRALayer()\n",
      "              )\n",
      "              (lora): LoRALayer()\n",
      "            )\n",
      "            (lora): LoRALayer()\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (norm1): LayerNorm()\n",
      "      (norm2): LayerNorm()\n",
      "      (drop_resid): Dropout(p=0.0, inplace=False)\n",
      "    )\n",
      "    (1): TransformerBlock(\n",
      "      (att): MultiHeadAttention(\n",
      "        (W_query): LinearWithLoRA(\n",
      "          (linear): LinearWithLoRA(\n",
      "            (linear): LinearWithLoRA(\n",
      "              (linear): LinearWithLoRA(\n",
      "                (linear): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (lora): LoRALayer()\n",
      "              )\n",
      "              (lora): LoRALayer()\n",
      "            )\n",
      "            (lora): LoRALayer()\n",
      "          )\n",
      "          (lora): LoRALayer()\n",
      "        )\n",
      "        (W_key): LinearWithLoRA(\n",
      "          (linear): LinearWithLoRA(\n",
      "            (linear): LinearWithLoRA(\n",
      "              (linear): LinearWithLoRA(\n",
      "                (linear): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (lora): LoRALayer()\n",
      "              )\n",
      "              (lora): LoRALayer()\n",
      "            )\n",
      "            (lora): LoRALayer()\n",
      "          )\n",
      "          (lora): LoRALayer()\n",
      "        )\n",
      "        (W_value): LinearWithLoRA(\n",
      "          (linear): LinearWithLoRA(\n",
      "            (linear): LinearWithLoRA(\n",
      "              (linear): LinearWithLoRA(\n",
      "                (linear): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (lora): LoRALayer()\n",
      "              )\n",
      "              (lora): LoRALayer()\n",
      "            )\n",
      "            (lora): LoRALayer()\n",
      "          )\n",
      "          (lora): LoRALayer()\n",
      "        )\n",
      "        (out_proj): LinearWithLoRA(\n",
      "          (linear): LinearWithLoRA(\n",
      "            (linear): LinearWithLoRA(\n",
      "              (linear): LinearWithLoRA(\n",
      "                (linear): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (lora): LoRALayer()\n",
      "              )\n",
      "              (lora): LoRALayer()\n",
      "            )\n",
      "            (lora): LoRALayer()\n",
      "          )\n",
      "          (lora): LoRALayer()\n",
      "        )\n",
      "        (dropout): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (ff): FeedForward(\n",
      "        (layers): Sequential(\n",
      "          (0): LinearWithLoRA(\n",
      "            (linear): LinearWithLoRA(\n",
      "              (linear): LinearWithLoRA(\n",
      "                (linear): LinearWithLoRA(\n",
      "                  (linear): Linear(in_features=768, out_features=3072, bias=True)\n",
      "                  (lora): LoRALayer()\n",
      "                )\n",
      "                (lora): LoRALayer()\n",
      "              )\n",
      "              (lora): LoRALayer()\n",
      "            )\n",
      "            (lora): LoRALayer()\n",
      "          )\n",
      "          (1): GELU()\n",
      "          (2): LinearWithLoRA(\n",
      "            (linear): LinearWithLoRA(\n",
      "              (linear): LinearWithLoRA(\n",
      "                (linear): LinearWithLoRA(\n",
      "                  (linear): Linear(in_features=3072, out_features=768, bias=True)\n",
      "                  (lora): LoRALayer()\n",
      "                )\n",
      "                (lora): LoRALayer()\n",
      "              )\n",
      "              (lora): LoRALayer()\n",
      "            )\n",
      "            (lora): LoRALayer()\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (norm1): LayerNorm()\n",
      "      (norm2): LayerNorm()\n",
      "      (drop_resid): Dropout(p=0.0, inplace=False)\n",
      "    )\n",
      "    (2): TransformerBlock(\n",
      "      (att): MultiHeadAttention(\n",
      "        (W_query): LinearWithLoRA(\n",
      "          (linear): LinearWithLoRA(\n",
      "            (linear): LinearWithLoRA(\n",
      "              (linear): LinearWithLoRA(\n",
      "                (linear): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (lora): LoRALayer()\n",
      "              )\n",
      "              (lora): LoRALayer()\n",
      "            )\n",
      "            (lora): LoRALayer()\n",
      "          )\n",
      "          (lora): LoRALayer()\n",
      "        )\n",
      "        (W_key): LinearWithLoRA(\n",
      "          (linear): LinearWithLoRA(\n",
      "            (linear): LinearWithLoRA(\n",
      "              (linear): LinearWithLoRA(\n",
      "                (linear): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (lora): LoRALayer()\n",
      "              )\n",
      "              (lora): LoRALayer()\n",
      "            )\n",
      "            (lora): LoRALayer()\n",
      "          )\n",
      "          (lora): LoRALayer()\n",
      "        )\n",
      "        (W_value): LinearWithLoRA(\n",
      "          (linear): LinearWithLoRA(\n",
      "            (linear): LinearWithLoRA(\n",
      "              (linear): LinearWithLoRA(\n",
      "                (linear): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (lora): LoRALayer()\n",
      "              )\n",
      "              (lora): LoRALayer()\n",
      "            )\n",
      "            (lora): LoRALayer()\n",
      "          )\n",
      "          (lora): LoRALayer()\n",
      "        )\n",
      "        (out_proj): LinearWithLoRA(\n",
      "          (linear): LinearWithLoRA(\n",
      "            (linear): LinearWithLoRA(\n",
      "              (linear): LinearWithLoRA(\n",
      "                (linear): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (lora): LoRALayer()\n",
      "              )\n",
      "              (lora): LoRALayer()\n",
      "            )\n",
      "            (lora): LoRALayer()\n",
      "          )\n",
      "          (lora): LoRALayer()\n",
      "        )\n",
      "        (dropout): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (ff): FeedForward(\n",
      "        (layers): Sequential(\n",
      "          (0): LinearWithLoRA(\n",
      "            (linear): LinearWithLoRA(\n",
      "              (linear): LinearWithLoRA(\n",
      "                (linear): LinearWithLoRA(\n",
      "                  (linear): Linear(in_features=768, out_features=3072, bias=True)\n",
      "                  (lora): LoRALayer()\n",
      "                )\n",
      "                (lora): LoRALayer()\n",
      "              )\n",
      "              (lora): LoRALayer()\n",
      "            )\n",
      "            (lora): LoRALayer()\n",
      "          )\n",
      "          (1): GELU()\n",
      "          (2): LinearWithLoRA(\n",
      "            (linear): LinearWithLoRA(\n",
      "              (linear): LinearWithLoRA(\n",
      "                (linear): LinearWithLoRA(\n",
      "                  (linear): Linear(in_features=3072, out_features=768, bias=True)\n",
      "                  (lora): LoRALayer()\n",
      "                )\n",
      "                (lora): LoRALayer()\n",
      "              )\n",
      "              (lora): LoRALayer()\n",
      "            )\n",
      "            (lora): LoRALayer()\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (norm1): LayerNorm()\n",
      "      (norm2): LayerNorm()\n",
      "      (drop_resid): Dropout(p=0.0, inplace=False)\n",
      "    )\n",
      "    (3): TransformerBlock(\n",
      "      (att): MultiHeadAttention(\n",
      "        (W_query): LinearWithLoRA(\n",
      "          (linear): LinearWithLoRA(\n",
      "            (linear): LinearWithLoRA(\n",
      "              (linear): LinearWithLoRA(\n",
      "                (linear): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (lora): LoRALayer()\n",
      "              )\n",
      "              (lora): LoRALayer()\n",
      "            )\n",
      "            (lora): LoRALayer()\n",
      "          )\n",
      "          (lora): LoRALayer()\n",
      "        )\n",
      "        (W_key): LinearWithLoRA(\n",
      "          (linear): LinearWithLoRA(\n",
      "            (linear): LinearWithLoRA(\n",
      "              (linear): LinearWithLoRA(\n",
      "                (linear): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (lora): LoRALayer()\n",
      "              )\n",
      "              (lora): LoRALayer()\n",
      "            )\n",
      "            (lora): LoRALayer()\n",
      "          )\n",
      "          (lora): LoRALayer()\n",
      "        )\n",
      "        (W_value): LinearWithLoRA(\n",
      "          (linear): LinearWithLoRA(\n",
      "            (linear): LinearWithLoRA(\n",
      "              (linear): LinearWithLoRA(\n",
      "                (linear): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (lora): LoRALayer()\n",
      "              )\n",
      "              (lora): LoRALayer()\n",
      "            )\n",
      "            (lora): LoRALayer()\n",
      "          )\n",
      "          (lora): LoRALayer()\n",
      "        )\n",
      "        (out_proj): LinearWithLoRA(\n",
      "          (linear): LinearWithLoRA(\n",
      "            (linear): LinearWithLoRA(\n",
      "              (linear): LinearWithLoRA(\n",
      "                (linear): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (lora): LoRALayer()\n",
      "              )\n",
      "              (lora): LoRALayer()\n",
      "            )\n",
      "            (lora): LoRALayer()\n",
      "          )\n",
      "          (lora): LoRALayer()\n",
      "        )\n",
      "        (dropout): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (ff): FeedForward(\n",
      "        (layers): Sequential(\n",
      "          (0): LinearWithLoRA(\n",
      "            (linear): LinearWithLoRA(\n",
      "              (linear): LinearWithLoRA(\n",
      "                (linear): LinearWithLoRA(\n",
      "                  (linear): Linear(in_features=768, out_features=3072, bias=True)\n",
      "                  (lora): LoRALayer()\n",
      "                )\n",
      "                (lora): LoRALayer()\n",
      "              )\n",
      "              (lora): LoRALayer()\n",
      "            )\n",
      "            (lora): LoRALayer()\n",
      "          )\n",
      "          (1): GELU()\n",
      "          (2): LinearWithLoRA(\n",
      "            (linear): LinearWithLoRA(\n",
      "              (linear): LinearWithLoRA(\n",
      "                (linear): LinearWithLoRA(\n",
      "                  (linear): Linear(in_features=3072, out_features=768, bias=True)\n",
      "                  (lora): LoRALayer()\n",
      "                )\n",
      "                (lora): LoRALayer()\n",
      "              )\n",
      "              (lora): LoRALayer()\n",
      "            )\n",
      "            (lora): LoRALayer()\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (norm1): LayerNorm()\n",
      "      (norm2): LayerNorm()\n",
      "      (drop_resid): Dropout(p=0.0, inplace=False)\n",
      "    )\n",
      "    (4): TransformerBlock(\n",
      "      (att): MultiHeadAttention(\n",
      "        (W_query): LinearWithLoRA(\n",
      "          (linear): LinearWithLoRA(\n",
      "            (linear): LinearWithLoRA(\n",
      "              (linear): LinearWithLoRA(\n",
      "                (linear): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (lora): LoRALayer()\n",
      "              )\n",
      "              (lora): LoRALayer()\n",
      "            )\n",
      "            (lora): LoRALayer()\n",
      "          )\n",
      "          (lora): LoRALayer()\n",
      "        )\n",
      "        (W_key): LinearWithLoRA(\n",
      "          (linear): LinearWithLoRA(\n",
      "            (linear): LinearWithLoRA(\n",
      "              (linear): LinearWithLoRA(\n",
      "                (linear): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (lora): LoRALayer()\n",
      "              )\n",
      "              (lora): LoRALayer()\n",
      "            )\n",
      "            (lora): LoRALayer()\n",
      "          )\n",
      "          (lora): LoRALayer()\n",
      "        )\n",
      "        (W_value): LinearWithLoRA(\n",
      "          (linear): LinearWithLoRA(\n",
      "            (linear): LinearWithLoRA(\n",
      "              (linear): LinearWithLoRA(\n",
      "                (linear): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (lora): LoRALayer()\n",
      "              )\n",
      "              (lora): LoRALayer()\n",
      "            )\n",
      "            (lora): LoRALayer()\n",
      "          )\n",
      "          (lora): LoRALayer()\n",
      "        )\n",
      "        (out_proj): LinearWithLoRA(\n",
      "          (linear): LinearWithLoRA(\n",
      "            (linear): LinearWithLoRA(\n",
      "              (linear): LinearWithLoRA(\n",
      "                (linear): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (lora): LoRALayer()\n",
      "              )\n",
      "              (lora): LoRALayer()\n",
      "            )\n",
      "            (lora): LoRALayer()\n",
      "          )\n",
      "          (lora): LoRALayer()\n",
      "        )\n",
      "        (dropout): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (ff): FeedForward(\n",
      "        (layers): Sequential(\n",
      "          (0): LinearWithLoRA(\n",
      "            (linear): LinearWithLoRA(\n",
      "              (linear): LinearWithLoRA(\n",
      "                (linear): LinearWithLoRA(\n",
      "                  (linear): Linear(in_features=768, out_features=3072, bias=True)\n",
      "                  (lora): LoRALayer()\n",
      "                )\n",
      "                (lora): LoRALayer()\n",
      "              )\n",
      "              (lora): LoRALayer()\n",
      "            )\n",
      "            (lora): LoRALayer()\n",
      "          )\n",
      "          (1): GELU()\n",
      "          (2): LinearWithLoRA(\n",
      "            (linear): LinearWithLoRA(\n",
      "              (linear): LinearWithLoRA(\n",
      "                (linear): LinearWithLoRA(\n",
      "                  (linear): Linear(in_features=3072, out_features=768, bias=True)\n",
      "                  (lora): LoRALayer()\n",
      "                )\n",
      "                (lora): LoRALayer()\n",
      "              )\n",
      "              (lora): LoRALayer()\n",
      "            )\n",
      "            (lora): LoRALayer()\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (norm1): LayerNorm()\n",
      "      (norm2): LayerNorm()\n",
      "      (drop_resid): Dropout(p=0.0, inplace=False)\n",
      "    )\n",
      "    (5): TransformerBlock(\n",
      "      (att): MultiHeadAttention(\n",
      "        (W_query): LinearWithLoRA(\n",
      "          (linear): LinearWithLoRA(\n",
      "            (linear): LinearWithLoRA(\n",
      "              (linear): LinearWithLoRA(\n",
      "                (linear): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (lora): LoRALayer()\n",
      "              )\n",
      "              (lora): LoRALayer()\n",
      "            )\n",
      "            (lora): LoRALayer()\n",
      "          )\n",
      "          (lora): LoRALayer()\n",
      "        )\n",
      "        (W_key): LinearWithLoRA(\n",
      "          (linear): LinearWithLoRA(\n",
      "            (linear): LinearWithLoRA(\n",
      "              (linear): LinearWithLoRA(\n",
      "                (linear): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (lora): LoRALayer()\n",
      "              )\n",
      "              (lora): LoRALayer()\n",
      "            )\n",
      "            (lora): LoRALayer()\n",
      "          )\n",
      "          (lora): LoRALayer()\n",
      "        )\n",
      "        (W_value): LinearWithLoRA(\n",
      "          (linear): LinearWithLoRA(\n",
      "            (linear): LinearWithLoRA(\n",
      "              (linear): LinearWithLoRA(\n",
      "                (linear): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (lora): LoRALayer()\n",
      "              )\n",
      "              (lora): LoRALayer()\n",
      "            )\n",
      "            (lora): LoRALayer()\n",
      "          )\n",
      "          (lora): LoRALayer()\n",
      "        )\n",
      "        (out_proj): LinearWithLoRA(\n",
      "          (linear): LinearWithLoRA(\n",
      "            (linear): LinearWithLoRA(\n",
      "              (linear): LinearWithLoRA(\n",
      "                (linear): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (lora): LoRALayer()\n",
      "              )\n",
      "              (lora): LoRALayer()\n",
      "            )\n",
      "            (lora): LoRALayer()\n",
      "          )\n",
      "          (lora): LoRALayer()\n",
      "        )\n",
      "        (dropout): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (ff): FeedForward(\n",
      "        (layers): Sequential(\n",
      "          (0): LinearWithLoRA(\n",
      "            (linear): LinearWithLoRA(\n",
      "              (linear): LinearWithLoRA(\n",
      "                (linear): LinearWithLoRA(\n",
      "                  (linear): Linear(in_features=768, out_features=3072, bias=True)\n",
      "                  (lora): LoRALayer()\n",
      "                )\n",
      "                (lora): LoRALayer()\n",
      "              )\n",
      "              (lora): LoRALayer()\n",
      "            )\n",
      "            (lora): LoRALayer()\n",
      "          )\n",
      "          (1): GELU()\n",
      "          (2): LinearWithLoRA(\n",
      "            (linear): LinearWithLoRA(\n",
      "              (linear): LinearWithLoRA(\n",
      "                (linear): LinearWithLoRA(\n",
      "                  (linear): Linear(in_features=3072, out_features=768, bias=True)\n",
      "                  (lora): LoRALayer()\n",
      "                )\n",
      "                (lora): LoRALayer()\n",
      "              )\n",
      "              (lora): LoRALayer()\n",
      "            )\n",
      "            (lora): LoRALayer()\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (norm1): LayerNorm()\n",
      "      (norm2): LayerNorm()\n",
      "      (drop_resid): Dropout(p=0.0, inplace=False)\n",
      "    )\n",
      "    (6): TransformerBlock(\n",
      "      (att): MultiHeadAttention(\n",
      "        (W_query): LinearWithLoRA(\n",
      "          (linear): LinearWithLoRA(\n",
      "            (linear): LinearWithLoRA(\n",
      "              (linear): LinearWithLoRA(\n",
      "                (linear): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (lora): LoRALayer()\n",
      "              )\n",
      "              (lora): LoRALayer()\n",
      "            )\n",
      "            (lora): LoRALayer()\n",
      "          )\n",
      "          (lora): LoRALayer()\n",
      "        )\n",
      "        (W_key): LinearWithLoRA(\n",
      "          (linear): LinearWithLoRA(\n",
      "            (linear): LinearWithLoRA(\n",
      "              (linear): LinearWithLoRA(\n",
      "                (linear): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (lora): LoRALayer()\n",
      "              )\n",
      "              (lora): LoRALayer()\n",
      "            )\n",
      "            (lora): LoRALayer()\n",
      "          )\n",
      "          (lora): LoRALayer()\n",
      "        )\n",
      "        (W_value): LinearWithLoRA(\n",
      "          (linear): LinearWithLoRA(\n",
      "            (linear): LinearWithLoRA(\n",
      "              (linear): LinearWithLoRA(\n",
      "                (linear): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (lora): LoRALayer()\n",
      "              )\n",
      "              (lora): LoRALayer()\n",
      "            )\n",
      "            (lora): LoRALayer()\n",
      "          )\n",
      "          (lora): LoRALayer()\n",
      "        )\n",
      "        (out_proj): LinearWithLoRA(\n",
      "          (linear): LinearWithLoRA(\n",
      "            (linear): LinearWithLoRA(\n",
      "              (linear): LinearWithLoRA(\n",
      "                (linear): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (lora): LoRALayer()\n",
      "              )\n",
      "              (lora): LoRALayer()\n",
      "            )\n",
      "            (lora): LoRALayer()\n",
      "          )\n",
      "          (lora): LoRALayer()\n",
      "        )\n",
      "        (dropout): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (ff): FeedForward(\n",
      "        (layers): Sequential(\n",
      "          (0): LinearWithLoRA(\n",
      "            (linear): LinearWithLoRA(\n",
      "              (linear): LinearWithLoRA(\n",
      "                (linear): LinearWithLoRA(\n",
      "                  (linear): Linear(in_features=768, out_features=3072, bias=True)\n",
      "                  (lora): LoRALayer()\n",
      "                )\n",
      "                (lora): LoRALayer()\n",
      "              )\n",
      "              (lora): LoRALayer()\n",
      "            )\n",
      "            (lora): LoRALayer()\n",
      "          )\n",
      "          (1): GELU()\n",
      "          (2): LinearWithLoRA(\n",
      "            (linear): LinearWithLoRA(\n",
      "              (linear): LinearWithLoRA(\n",
      "                (linear): LinearWithLoRA(\n",
      "                  (linear): Linear(in_features=3072, out_features=768, bias=True)\n",
      "                  (lora): LoRALayer()\n",
      "                )\n",
      "                (lora): LoRALayer()\n",
      "              )\n",
      "              (lora): LoRALayer()\n",
      "            )\n",
      "            (lora): LoRALayer()\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (norm1): LayerNorm()\n",
      "      (norm2): LayerNorm()\n",
      "      (drop_resid): Dropout(p=0.0, inplace=False)\n",
      "    )\n",
      "    (7): TransformerBlock(\n",
      "      (att): MultiHeadAttention(\n",
      "        (W_query): LinearWithLoRA(\n",
      "          (linear): LinearWithLoRA(\n",
      "            (linear): LinearWithLoRA(\n",
      "              (linear): LinearWithLoRA(\n",
      "                (linear): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (lora): LoRALayer()\n",
      "              )\n",
      "              (lora): LoRALayer()\n",
      "            )\n",
      "            (lora): LoRALayer()\n",
      "          )\n",
      "          (lora): LoRALayer()\n",
      "        )\n",
      "        (W_key): LinearWithLoRA(\n",
      "          (linear): LinearWithLoRA(\n",
      "            (linear): LinearWithLoRA(\n",
      "              (linear): LinearWithLoRA(\n",
      "                (linear): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (lora): LoRALayer()\n",
      "              )\n",
      "              (lora): LoRALayer()\n",
      "            )\n",
      "            (lora): LoRALayer()\n",
      "          )\n",
      "          (lora): LoRALayer()\n",
      "        )\n",
      "        (W_value): LinearWithLoRA(\n",
      "          (linear): LinearWithLoRA(\n",
      "            (linear): LinearWithLoRA(\n",
      "              (linear): LinearWithLoRA(\n",
      "                (linear): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (lora): LoRALayer()\n",
      "              )\n",
      "              (lora): LoRALayer()\n",
      "            )\n",
      "            (lora): LoRALayer()\n",
      "          )\n",
      "          (lora): LoRALayer()\n",
      "        )\n",
      "        (out_proj): LinearWithLoRA(\n",
      "          (linear): LinearWithLoRA(\n",
      "            (linear): LinearWithLoRA(\n",
      "              (linear): LinearWithLoRA(\n",
      "                (linear): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (lora): LoRALayer()\n",
      "              )\n",
      "              (lora): LoRALayer()\n",
      "            )\n",
      "            (lora): LoRALayer()\n",
      "          )\n",
      "          (lora): LoRALayer()\n",
      "        )\n",
      "        (dropout): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (ff): FeedForward(\n",
      "        (layers): Sequential(\n",
      "          (0): LinearWithLoRA(\n",
      "            (linear): LinearWithLoRA(\n",
      "              (linear): LinearWithLoRA(\n",
      "                (linear): LinearWithLoRA(\n",
      "                  (linear): Linear(in_features=768, out_features=3072, bias=True)\n",
      "                  (lora): LoRALayer()\n",
      "                )\n",
      "                (lora): LoRALayer()\n",
      "              )\n",
      "              (lora): LoRALayer()\n",
      "            )\n",
      "            (lora): LoRALayer()\n",
      "          )\n",
      "          (1): GELU()\n",
      "          (2): LinearWithLoRA(\n",
      "            (linear): LinearWithLoRA(\n",
      "              (linear): LinearWithLoRA(\n",
      "                (linear): LinearWithLoRA(\n",
      "                  (linear): Linear(in_features=3072, out_features=768, bias=True)\n",
      "                  (lora): LoRALayer()\n",
      "                )\n",
      "                (lora): LoRALayer()\n",
      "              )\n",
      "              (lora): LoRALayer()\n",
      "            )\n",
      "            (lora): LoRALayer()\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (norm1): LayerNorm()\n",
      "      (norm2): LayerNorm()\n",
      "      (drop_resid): Dropout(p=0.0, inplace=False)\n",
      "    )\n",
      "    (8): TransformerBlock(\n",
      "      (att): MultiHeadAttention(\n",
      "        (W_query): LinearWithLoRA(\n",
      "          (linear): LinearWithLoRA(\n",
      "            (linear): LinearWithLoRA(\n",
      "              (linear): LinearWithLoRA(\n",
      "                (linear): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (lora): LoRALayer()\n",
      "              )\n",
      "              (lora): LoRALayer()\n",
      "            )\n",
      "            (lora): LoRALayer()\n",
      "          )\n",
      "          (lora): LoRALayer()\n",
      "        )\n",
      "        (W_key): LinearWithLoRA(\n",
      "          (linear): LinearWithLoRA(\n",
      "            (linear): LinearWithLoRA(\n",
      "              (linear): LinearWithLoRA(\n",
      "                (linear): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (lora): LoRALayer()\n",
      "              )\n",
      "              (lora): LoRALayer()\n",
      "            )\n",
      "            (lora): LoRALayer()\n",
      "          )\n",
      "          (lora): LoRALayer()\n",
      "        )\n",
      "        (W_value): LinearWithLoRA(\n",
      "          (linear): LinearWithLoRA(\n",
      "            (linear): LinearWithLoRA(\n",
      "              (linear): LinearWithLoRA(\n",
      "                (linear): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (lora): LoRALayer()\n",
      "              )\n",
      "              (lora): LoRALayer()\n",
      "            )\n",
      "            (lora): LoRALayer()\n",
      "          )\n",
      "          (lora): LoRALayer()\n",
      "        )\n",
      "        (out_proj): LinearWithLoRA(\n",
      "          (linear): LinearWithLoRA(\n",
      "            (linear): LinearWithLoRA(\n",
      "              (linear): LinearWithLoRA(\n",
      "                (linear): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (lora): LoRALayer()\n",
      "              )\n",
      "              (lora): LoRALayer()\n",
      "            )\n",
      "            (lora): LoRALayer()\n",
      "          )\n",
      "          (lora): LoRALayer()\n",
      "        )\n",
      "        (dropout): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (ff): FeedForward(\n",
      "        (layers): Sequential(\n",
      "          (0): LinearWithLoRA(\n",
      "            (linear): LinearWithLoRA(\n",
      "              (linear): LinearWithLoRA(\n",
      "                (linear): LinearWithLoRA(\n",
      "                  (linear): Linear(in_features=768, out_features=3072, bias=True)\n",
      "                  (lora): LoRALayer()\n",
      "                )\n",
      "                (lora): LoRALayer()\n",
      "              )\n",
      "              (lora): LoRALayer()\n",
      "            )\n",
      "            (lora): LoRALayer()\n",
      "          )\n",
      "          (1): GELU()\n",
      "          (2): LinearWithLoRA(\n",
      "            (linear): LinearWithLoRA(\n",
      "              (linear): LinearWithLoRA(\n",
      "                (linear): LinearWithLoRA(\n",
      "                  (linear): Linear(in_features=3072, out_features=768, bias=True)\n",
      "                  (lora): LoRALayer()\n",
      "                )\n",
      "                (lora): LoRALayer()\n",
      "              )\n",
      "              (lora): LoRALayer()\n",
      "            )\n",
      "            (lora): LoRALayer()\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (norm1): LayerNorm()\n",
      "      (norm2): LayerNorm()\n",
      "      (drop_resid): Dropout(p=0.0, inplace=False)\n",
      "    )\n",
      "    (9): TransformerBlock(\n",
      "      (att): MultiHeadAttention(\n",
      "        (W_query): LinearWithLoRA(\n",
      "          (linear): LinearWithLoRA(\n",
      "            (linear): LinearWithLoRA(\n",
      "              (linear): LinearWithLoRA(\n",
      "                (linear): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (lora): LoRALayer()\n",
      "              )\n",
      "              (lora): LoRALayer()\n",
      "            )\n",
      "            (lora): LoRALayer()\n",
      "          )\n",
      "          (lora): LoRALayer()\n",
      "        )\n",
      "        (W_key): LinearWithLoRA(\n",
      "          (linear): LinearWithLoRA(\n",
      "            (linear): LinearWithLoRA(\n",
      "              (linear): LinearWithLoRA(\n",
      "                (linear): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (lora): LoRALayer()\n",
      "              )\n",
      "              (lora): LoRALayer()\n",
      "            )\n",
      "            (lora): LoRALayer()\n",
      "          )\n",
      "          (lora): LoRALayer()\n",
      "        )\n",
      "        (W_value): LinearWithLoRA(\n",
      "          (linear): LinearWithLoRA(\n",
      "            (linear): LinearWithLoRA(\n",
      "              (linear): LinearWithLoRA(\n",
      "                (linear): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (lora): LoRALayer()\n",
      "              )\n",
      "              (lora): LoRALayer()\n",
      "            )\n",
      "            (lora): LoRALayer()\n",
      "          )\n",
      "          (lora): LoRALayer()\n",
      "        )\n",
      "        (out_proj): LinearWithLoRA(\n",
      "          (linear): LinearWithLoRA(\n",
      "            (linear): LinearWithLoRA(\n",
      "              (linear): LinearWithLoRA(\n",
      "                (linear): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (lora): LoRALayer()\n",
      "              )\n",
      "              (lora): LoRALayer()\n",
      "            )\n",
      "            (lora): LoRALayer()\n",
      "          )\n",
      "          (lora): LoRALayer()\n",
      "        )\n",
      "        (dropout): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (ff): FeedForward(\n",
      "        (layers): Sequential(\n",
      "          (0): LinearWithLoRA(\n",
      "            (linear): LinearWithLoRA(\n",
      "              (linear): LinearWithLoRA(\n",
      "                (linear): LinearWithLoRA(\n",
      "                  (linear): Linear(in_features=768, out_features=3072, bias=True)\n",
      "                  (lora): LoRALayer()\n",
      "                )\n",
      "                (lora): LoRALayer()\n",
      "              )\n",
      "              (lora): LoRALayer()\n",
      "            )\n",
      "            (lora): LoRALayer()\n",
      "          )\n",
      "          (1): GELU()\n",
      "          (2): LinearWithLoRA(\n",
      "            (linear): LinearWithLoRA(\n",
      "              (linear): LinearWithLoRA(\n",
      "                (linear): LinearWithLoRA(\n",
      "                  (linear): Linear(in_features=3072, out_features=768, bias=True)\n",
      "                  (lora): LoRALayer()\n",
      "                )\n",
      "                (lora): LoRALayer()\n",
      "              )\n",
      "              (lora): LoRALayer()\n",
      "            )\n",
      "            (lora): LoRALayer()\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (norm1): LayerNorm()\n",
      "      (norm2): LayerNorm()\n",
      "      (drop_resid): Dropout(p=0.0, inplace=False)\n",
      "    )\n",
      "    (10): TransformerBlock(\n",
      "      (att): MultiHeadAttention(\n",
      "        (W_query): LinearWithLoRA(\n",
      "          (linear): LinearWithLoRA(\n",
      "            (linear): LinearWithLoRA(\n",
      "              (linear): LinearWithLoRA(\n",
      "                (linear): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (lora): LoRALayer()\n",
      "              )\n",
      "              (lora): LoRALayer()\n",
      "            )\n",
      "            (lora): LoRALayer()\n",
      "          )\n",
      "          (lora): LoRALayer()\n",
      "        )\n",
      "        (W_key): LinearWithLoRA(\n",
      "          (linear): LinearWithLoRA(\n",
      "            (linear): LinearWithLoRA(\n",
      "              (linear): LinearWithLoRA(\n",
      "                (linear): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (lora): LoRALayer()\n",
      "              )\n",
      "              (lora): LoRALayer()\n",
      "            )\n",
      "            (lora): LoRALayer()\n",
      "          )\n",
      "          (lora): LoRALayer()\n",
      "        )\n",
      "        (W_value): LinearWithLoRA(\n",
      "          (linear): LinearWithLoRA(\n",
      "            (linear): LinearWithLoRA(\n",
      "              (linear): LinearWithLoRA(\n",
      "                (linear): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (lora): LoRALayer()\n",
      "              )\n",
      "              (lora): LoRALayer()\n",
      "            )\n",
      "            (lora): LoRALayer()\n",
      "          )\n",
      "          (lora): LoRALayer()\n",
      "        )\n",
      "        (out_proj): LinearWithLoRA(\n",
      "          (linear): LinearWithLoRA(\n",
      "            (linear): LinearWithLoRA(\n",
      "              (linear): LinearWithLoRA(\n",
      "                (linear): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (lora): LoRALayer()\n",
      "              )\n",
      "              (lora): LoRALayer()\n",
      "            )\n",
      "            (lora): LoRALayer()\n",
      "          )\n",
      "          (lora): LoRALayer()\n",
      "        )\n",
      "        (dropout): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (ff): FeedForward(\n",
      "        (layers): Sequential(\n",
      "          (0): LinearWithLoRA(\n",
      "            (linear): LinearWithLoRA(\n",
      "              (linear): LinearWithLoRA(\n",
      "                (linear): LinearWithLoRA(\n",
      "                  (linear): Linear(in_features=768, out_features=3072, bias=True)\n",
      "                  (lora): LoRALayer()\n",
      "                )\n",
      "                (lora): LoRALayer()\n",
      "              )\n",
      "              (lora): LoRALayer()\n",
      "            )\n",
      "            (lora): LoRALayer()\n",
      "          )\n",
      "          (1): GELU()\n",
      "          (2): LinearWithLoRA(\n",
      "            (linear): LinearWithLoRA(\n",
      "              (linear): LinearWithLoRA(\n",
      "                (linear): LinearWithLoRA(\n",
      "                  (linear): Linear(in_features=3072, out_features=768, bias=True)\n",
      "                  (lora): LoRALayer()\n",
      "                )\n",
      "                (lora): LoRALayer()\n",
      "              )\n",
      "              (lora): LoRALayer()\n",
      "            )\n",
      "            (lora): LoRALayer()\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (norm1): LayerNorm()\n",
      "      (norm2): LayerNorm()\n",
      "      (drop_resid): Dropout(p=0.0, inplace=False)\n",
      "    )\n",
      "    (11): TransformerBlock(\n",
      "      (att): MultiHeadAttention(\n",
      "        (W_query): LinearWithLoRA(\n",
      "          (linear): LinearWithLoRA(\n",
      "            (linear): LinearWithLoRA(\n",
      "              (linear): LinearWithLoRA(\n",
      "                (linear): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (lora): LoRALayer()\n",
      "              )\n",
      "              (lora): LoRALayer()\n",
      "            )\n",
      "            (lora): LoRALayer()\n",
      "          )\n",
      "          (lora): LoRALayer()\n",
      "        )\n",
      "        (W_key): LinearWithLoRA(\n",
      "          (linear): LinearWithLoRA(\n",
      "            (linear): LinearWithLoRA(\n",
      "              (linear): LinearWithLoRA(\n",
      "                (linear): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (lora): LoRALayer()\n",
      "              )\n",
      "              (lora): LoRALayer()\n",
      "            )\n",
      "            (lora): LoRALayer()\n",
      "          )\n",
      "          (lora): LoRALayer()\n",
      "        )\n",
      "        (W_value): LinearWithLoRA(\n",
      "          (linear): LinearWithLoRA(\n",
      "            (linear): LinearWithLoRA(\n",
      "              (linear): LinearWithLoRA(\n",
      "                (linear): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (lora): LoRALayer()\n",
      "              )\n",
      "              (lora): LoRALayer()\n",
      "            )\n",
      "            (lora): LoRALayer()\n",
      "          )\n",
      "          (lora): LoRALayer()\n",
      "        )\n",
      "        (out_proj): LinearWithLoRA(\n",
      "          (linear): LinearWithLoRA(\n",
      "            (linear): LinearWithLoRA(\n",
      "              (linear): LinearWithLoRA(\n",
      "                (linear): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (lora): LoRALayer()\n",
      "              )\n",
      "              (lora): LoRALayer()\n",
      "            )\n",
      "            (lora): LoRALayer()\n",
      "          )\n",
      "          (lora): LoRALayer()\n",
      "        )\n",
      "        (dropout): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (ff): FeedForward(\n",
      "        (layers): Sequential(\n",
      "          (0): LinearWithLoRA(\n",
      "            (linear): LinearWithLoRA(\n",
      "              (linear): LinearWithLoRA(\n",
      "                (linear): LinearWithLoRA(\n",
      "                  (linear): Linear(in_features=768, out_features=3072, bias=True)\n",
      "                  (lora): LoRALayer()\n",
      "                )\n",
      "                (lora): LoRALayer()\n",
      "              )\n",
      "              (lora): LoRALayer()\n",
      "            )\n",
      "            (lora): LoRALayer()\n",
      "          )\n",
      "          (1): GELU()\n",
      "          (2): LinearWithLoRA(\n",
      "            (linear): LinearWithLoRA(\n",
      "              (linear): LinearWithLoRA(\n",
      "                (linear): LinearWithLoRA(\n",
      "                  (linear): Linear(in_features=3072, out_features=768, bias=True)\n",
      "                  (lora): LoRALayer()\n",
      "                )\n",
      "                (lora): LoRALayer()\n",
      "              )\n",
      "              (lora): LoRALayer()\n",
      "            )\n",
      "            (lora): LoRALayer()\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (norm1): LayerNorm()\n",
      "      (norm2): LayerNorm()\n",
      "      (drop_resid): Dropout(p=0.0, inplace=False)\n",
      "    )\n",
      "  )\n",
      "  (final_norm): LayerNorm()\n",
      "  (out_head): LinearWithLoRA(\n",
      "    (linear): LinearWithLoRA(\n",
      "      (linear): LinearWithLoRA(\n",
      "        (linear): LinearWithLoRA(\n",
      "          (linear): Linear(in_features=768, out_features=2, bias=True)\n",
      "          (lora): LoRALayer()\n",
      "        )\n",
      "        (lora): LoRALayer()\n",
      "      )\n",
      "      (lora): LoRALayer()\n",
      "    )\n",
      "    (lora): LoRALayer()\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- we have now 50x less parameters when using LoRA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training accuracy: 46.25%\n",
      "Validation accuracy: 45.00%\n",
      "Test accuracy: 48.75%\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(123)\n",
    "train_accuracy = calc_accuracy_loader(train_loader, model, device, num_batches=10)\n",
    "val_accuracy = calc_accuracy_loader(val_loader, model, device, num_batches=10)\n",
    "test_accuracy = calc_accuracy_loader(test_loader, model, device, num_batches=10)\n",
    "\n",
    "print(f\"Training accuracy: {train_accuracy*100:.2f}%\")\n",
    "print(f\"Validation accuracy: {val_accuracy*100:.2f}%\")\n",
    "print(f\"Test accuracy: {test_accuracy*100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ep 1 (Step 000000): Train loss 3.563, Val loss 3.216\n",
      "Ep 1 (Step 000050): Train loss 0.355, Val loss 0.330\n",
      "Ep 1 (Step 000100): Train loss 0.166, Val loss 0.389\n",
      "Training accuracy: 97.50% | Validation accuracy: 100.00%\n",
      "Ep 2 (Step 000150): Train loss 0.218, Val loss 0.061\n",
      "Ep 2 (Step 000200): Train loss 0.029, Val loss 0.046\n",
      "Ep 2 (Step 000250): Train loss 0.207, Val loss 0.049\n",
      "Training accuracy: 97.50% | Validation accuracy: 95.00%\n",
      "Ep 3 (Step 000300): Train loss 0.070, Val loss 0.136\n",
      "Ep 3 (Step 000350): Train loss 0.014, Val loss 0.052\n",
      "Training accuracy: 100.00% | Validation accuracy: 97.50%\n",
      "Ep 4 (Step 000400): Train loss 0.006, Val loss 0.127\n",
      "Ep 4 (Step 000450): Train loss 0.043, Val loss 0.532\n",
      "Ep 4 (Step 000500): Train loss 0.030, Val loss 0.069\n",
      "Training accuracy: 97.50% | Validation accuracy: 95.00%\n",
      "Ep 5 (Step 000550): Train loss 0.113, Val loss 0.058\n",
      "Ep 5 (Step 000600): Train loss 0.014, Val loss 0.108\n",
      "Training accuracy: 100.00% | Validation accuracy: 97.50%\n",
      "Training completed in 63.12 minutes.\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "from previous_chapters2 import train_classifier_simple\n",
    "\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "torch.manual_seed(123)\n",
    "\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=5e-5, weight_decay=0.1)\n",
    "\n",
    "num_epochs = 5\n",
    "train_losses, val_losses, train_accs, val_accs, examples_seen = train_classifier_simple(\n",
    "    model, train_loader, val_loader, optimizer, device,\n",
    "    num_epochs=num_epochs, eval_freq=50, eval_iter=5,\n",
    ")\n",
    "\n",
    "end_time = time.time()\n",
    "execution_time_minutes = (end_time - start_time) / 60\n",
    "print(f\"Training completed in {execution_time_minutes:.2f} minutes.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from previous_chapters2 import plot_values\n",
    "\n",
    "epochs_tensor = torch.linspace(0, num_epochs, len(train_losses))\n",
    "examples_seen_tensor = torch.linspace(0, examples_seen, len(train_losses))\n",
    "\n",
    "plot_values(epochs_tensor, examples_seen_tensor, train_losses, val_losses, label=\"loss\")"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "b60f5bfde2dc6003c52249e7ba87cb8994d8effec2917dd9467812f9cc41ae70"
  },
  "kernelspec": {
   "display_name": "Python 3.9.18 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
