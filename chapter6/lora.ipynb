{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction to LoRA\n",
    "- Low-rank adaption (LoRA) is a machine learning technique that modifies a pretrained model to better suit a specific, often smaller, dataset by adjusting a small, low-rank subset of the model's parameters \n",
    "- This approach is important because it allows for efficient finetuning of large models on task-specific data, significantly reducing the computational cost and time required for finetuning\n",
    "- Suppose we have a large weight matrix $W$ for a given layer\n",
    "- During backpropagation, we learn a $\\Delta W$ matrix, which contains information on how much we want to update the original weights to minimize the loss function during training: $W_\\text{updated}=W+\\Delta W$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File downloaded and saved as sms_spam_collection/SMSSpamCollection.tsv\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "from previous_chapters2 import (\n",
    "    download_and_unzip_spam_data,\n",
    "    create_balanced_dataset,\n",
    "    random_split\n",
    ")\n",
    "\n",
    "\n",
    "url = \"https://archive.ics.uci.edu/static/public/228/sms+spam+collection.zip\"\n",
    "zip_path = \"sms_spam_collection.zip\"\n",
    "extracted_path = \"sms_spam_collection\"\n",
    "data_file_path = Path(extracted_path) / \"SMSSpamCollection.tsv\"\n",
    "\n",
    "download_and_unzip_spam_data(url, zip_path, extracted_path, data_file_path)\n",
    "\n",
    "df = pd.read_csv(data_file_path, sep=\"\\t\", header=None, names=[\"Label\", \"Text\"])\n",
    "balanced_df = create_balanced_dataset(df)\n",
    "balanced_df[\"Label\"] = balanced_df[\"Label\"].map({\"ham\": 0, \"spam\": 1})\n",
    "\n",
    "train_df, validation_df, test_df = random_split(balanced_df, 0.7, 0.1)\n",
    "train_df.to_csv(\"train.csv\", index=None)\n",
    "validation_df.to_csv(\"validation.csv\", index=None)\n",
    "test_df.to_csv(\"test.csv\", index=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "import tiktoken\n",
    "from previous_chapters2 import SpamDataset\n",
    "\n",
    "\n",
    "tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
    "train_dataset = SpamDataset(\"train.csv\", max_length=None, tokenizer=tokenizer)\n",
    "val_dataset = SpamDataset(\"validation.csv\", max_length=train_dataset.max_length, tokenizer=tokenizer)\n",
    "test_dataset = SpamDataset(\"test.csv\", max_length=train_dataset.max_length, tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "num_workers = 0\n",
    "batch_size = 8\n",
    "\n",
    "torch.manual_seed(123)\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    dataset=train_dataset,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True,\n",
    "    num_workers=num_workers,\n",
    "    drop_last=True,\n",
    ")\n",
    "\n",
    "val_loader = DataLoader(\n",
    "    dataset=val_dataset,\n",
    "    batch_size=batch_size,\n",
    "    num_workers=num_workers,\n",
    "    drop_last=False,\n",
    ")\n",
    "\n",
    "test_loader = DataLoader(\n",
    "    dataset=test_dataset,\n",
    "    batch_size=batch_size,\n",
    "    num_workers=num_workers,\n",
    "    drop_last=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-06-23 14:04:33.045115: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "checkpoint: 100%|██████████| 77.0/77.0 [00:00<00:00, 16.0kiB/s]\n",
      "encoder.json: 100%|██████████| 1.04M/1.04M [00:00<00:00, 1.16MiB/s]\n",
      "hparams.json: 100%|██████████| 90.0/90.0 [00:00<00:00, 113kiB/s]\n",
      "model.ckpt.data-00000-of-00001: 100%|██████████| 498M/498M [01:54<00:00, 4.36MiB/s]  \n",
      "model.ckpt.index: 100%|██████████| 5.21k/5.21k [00:00<00:00, 1.79MiB/s]\n",
      "model.ckpt.meta: 100%|██████████| 471k/471k [00:00<00:00, 828kiB/s] \n",
      "vocab.bpe: 100%|██████████| 456k/456k [00:00<00:00, 673kiB/s] \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GPTModel(\n",
       "  (tok_emb): Embedding(50257, 768)\n",
       "  (pos_emb): Embedding(1024, 768)\n",
       "  (drop_emb): Dropout(p=0.0, inplace=False)\n",
       "  (trf_blocks): Sequential(\n",
       "    (0): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_resid): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (1): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_resid): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (2): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_resid): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (3): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_resid): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (4): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_resid): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (5): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_resid): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (6): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_resid): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (7): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_resid): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (8): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_resid): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (9): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_resid): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (10): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_resid): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (11): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_resid): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "  )\n",
       "  (final_norm): LayerNorm()\n",
       "  (out_head): Linear(in_features=768, out_features=50257, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from gpt_download import download_and_load_gpt2\n",
    "from previous_chapters2 import GPTModel, load_weights_into_gpt\n",
    "\n",
    "\n",
    "CHOOSE_MODEL = \"gpt2-small (124M)\"\n",
    "INPUT_PROMPT = \"Every effort moves\"\n",
    "\n",
    "BASE_CONFIG = {\n",
    "    \"vocab_size\": 50257,     # Vocabulary size\n",
    "    \"context_length\": 1024,  # Context length\n",
    "    \"drop_rate\": 0.0,        # Dropout rate\n",
    "    \"qkv_bias\": True         # Query-key-value bias\n",
    "}\n",
    "\n",
    "model_configs = {\n",
    "    \"gpt2-small (124M)\": {\"emb_dim\": 768, \"n_layers\": 12, \"n_heads\": 12},\n",
    "    \"gpt2-medium (355M)\": {\"emb_dim\": 1024, \"n_layers\": 24, \"n_heads\": 16},\n",
    "    \"gpt2-large (774M)\": {\"emb_dim\": 1280, \"n_layers\": 36, \"n_heads\": 20},\n",
    "    \"gpt2-xl (1558M)\": {\"emb_dim\": 1600, \"n_layers\": 48, \"n_heads\": 25},\n",
    "}\n",
    "\n",
    "BASE_CONFIG.update(model_configs[CHOOSE_MODEL])\n",
    "\n",
    "model_size = CHOOSE_MODEL.split(\" \")[-1].lstrip(\"(\").rstrip(\")\")\n",
    "settings, params = download_and_load_gpt2(model_size=model_size, models_dir=\"gpt2\")\n",
    "\n",
    "model = GPTModel(BASE_CONFIG)\n",
    "load_weights_into_gpt(model, params)\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Every effort moves you forward.\n",
      "\n",
      "The first step is to understand the importance of your work\n"
     ]
    }
   ],
   "source": [
    "from previous_chapters2 import (\n",
    "    generate_text_simple,\n",
    "    text_to_token_ids,\n",
    "    token_ids_to_text\n",
    ")\n",
    "\n",
    "\n",
    "text_1 = \"Every effort moves you\"\n",
    "\n",
    "token_ids = generate_text_simple(\n",
    "    model=model,\n",
    "    idx=text_to_token_ids(text_1, tokenizer),\n",
    "    max_new_tokens=15,\n",
    "    context_size=BASE_CONFIG[\"context_length\"]\n",
    ")\n",
    "\n",
    "print(token_ids_to_text(token_ids, tokenizer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(123)\n",
    "\n",
    "num_classes = 2\n",
    "model.out_head = torch.nn.Linear(in_features=768, out_features=num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device);  # no assignment model = model.to(device) necessary for nn.Module classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training accuracy: 46.25%\n",
      "Validation accuracy: 45.00%\n",
      "Test accuracy: 48.75%\n"
     ]
    }
   ],
   "source": [
    "from previous_chapters2 import calc_accuracy_loader\n",
    "\n",
    "\n",
    "torch.manual_seed(123)\n",
    "train_accuracy = calc_accuracy_loader(train_loader, model, device, num_batches=10)\n",
    "val_accuracy = calc_accuracy_loader(val_loader, model, device, num_batches=10)\n",
    "test_accuracy = calc_accuracy_loader(test_loader, model, device, num_batches=10)\n",
    "\n",
    "print(f\"Training accuracy: {train_accuracy*100:.2f}%\")\n",
    "print(f\"Validation accuracy: {val_accuracy*100:.2f}%\")\n",
    "print(f\"Test accuracy: {test_accuracy*100:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parameter-efficient finetuning with LoRA\n",
    "- We begin with a LoRA Layer that creates the matrices $A$ and $B$, along with the `alpha` scaling hyperparameter and the `rank` $(r)$ hyperparameters\n",
    "- This layer can accept an input and compute the corresponding output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "class LoRALayer(torch.nn.Module):\n",
    "    def __init__(self, in_dim, out_dim, rank, alpha):\n",
    "        super().__init__()\n",
    "        self.A = torch.nn.Parameter(torch.empty(in_dim, rank))\n",
    "        torch.nn.init.kaiming_uniform_(self.A, a=math.sqrt(5)) # similar to standard weight initialization\n",
    "        self.B = torch.nn.Parameter(torch.zeros(rank, out_dim))\n",
    "        self.alpha = alpha\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.alpha * (x @ self.A @ self.B)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Here, `rank` controls the inner dimensions of the matrices $A$ and $B$\n",
    "- In other words, this parameter controls the number of additional parameters intoduced by LoRA and is a key factor in determining the balance between model adaptability and parameter efficiency\n",
    "- `Alpha` is a scaling hyperparameter applied to the output of the low-rank adaption\n",
    "- it contols the extent to which the adapted layer's output is allowed to influece the original output of the layer being adapted\n",
    "- This can be seen as a way to regulate the impact of the low-rank adaptation on the layer's output\n",
    "- We are usually interested in replacing existing `Linear` layers so that the weight update is applied to the existing pretrained weights\n",
    "\n",
    "\n",
    "- To incorporate the original `Linear` layer weights as shown in the figure above, we implement a `LinearWithLoRA` layer that uses the implemented `LoRALayer` and can be used to replace existing `Linear` layers in a neural network, e.g. self-attention module or feed forward modules in an LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinearWithLoRA(torch.nn.Module):\n",
    "    def __init__(self, linear, rank, alpha):\n",
    "        super().__init__()\n",
    "        self.linear = linear\n",
    "        self.lora = LoRALayer(\n",
    "            linear.in_features, linear.out_features, rank, alpha\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.linear(x) + self.lora(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Note, we initialized the weight matrix $B$ in the `LoRALayer` with zeros values. The matrix multiplication between $A$ and $B$ results in a matrix consisting of 0's and doesn't affect the original weights (since adding 0 weights doesn't modify them)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def replace_linear_with_lora(model, rank, alpha):\n",
    "    for name, module in model.named_children():\n",
    "        if isinstance(module, torch.nn.Linear):\n",
    "            # Replace the Linear layer with LinearWithLoRA\n",
    "            setattr(model, name, LinearWithLoRA(module, rank, alpha))\n",
    "        else:\n",
    "            # Recursively apply the same function to child modules\n",
    "            replace_linear_with_lora(module, rank, alpha)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- We then freeze the original model parameter and use the `replace_linear_with_lora` to replace the said `Linear` layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total trainable parameters before:  7999584\n",
      "Total trainable params after:  0\n"
     ]
    }
   ],
   "source": [
    "total_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "print(\"Total trainable parameters before: \", total_params)\n",
    "\n",
    "for param in model.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "total_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "print(\"Total trainable params after: \", total_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total trainable LoRA parameters: 2,666,528\n"
     ]
    }
   ],
   "source": [
    "replace_linear_with_lora(model, rank=16, alpha=16)\n",
    "\n",
    "total_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "print(f\"Total trainable LoRA parameters: {total_params:,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPTModel(\n",
      "  (tok_emb): Embedding(50257, 768)\n",
      "  (pos_emb): Embedding(1024, 768)\n",
      "  (drop_emb): Dropout(p=0.0, inplace=False)\n",
      "  (trf_blocks): Sequential(\n",
      "    (0): TransformerBlock(\n",
      "      (att): MultiHeadAttention(\n",
      "        (W_query): LinearWithLoRA(\n",
      "          (linear): LinearWithLoRA(\n",
      "            (linear): LinearWithLoRA(\n",
      "              (linear): LinearWithLoRA(\n",
      "                (linear): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (lora): LoRALayer()\n",
      "              )\n",
      "              (lora): LoRALayer()\n",
      "            )\n",
      "            (lora): LoRALayer()\n",
      "          )\n",
      "          (lora): LoRALayer()\n",
      "        )\n",
      "        (W_key): LinearWithLoRA(\n",
      "          (linear): LinearWithLoRA(\n",
      "            (linear): LinearWithLoRA(\n",
      "              (linear): LinearWithLoRA(\n",
      "                (linear): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (lora): LoRALayer()\n",
      "              )\n",
      "              (lora): LoRALayer()\n",
      "            )\n",
      "            (lora): LoRALayer()\n",
      "          )\n",
      "          (lora): LoRALayer()\n",
      "        )\n",
      "        (W_value): LinearWithLoRA(\n",
      "          (linear): LinearWithLoRA(\n",
      "            (linear): LinearWithLoRA(\n",
      "              (linear): LinearWithLoRA(\n",
      "                (linear): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (lora): LoRALayer()\n",
      "              )\n",
      "              (lora): LoRALayer()\n",
      "            )\n",
      "            (lora): LoRALayer()\n",
      "          )\n",
      "          (lora): LoRALayer()\n",
      "        )\n",
      "        (out_proj): LinearWithLoRA(\n",
      "          (linear): LinearWithLoRA(\n",
      "            (linear): LinearWithLoRA(\n",
      "              (linear): LinearWithLoRA(\n",
      "                (linear): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (lora): LoRALayer()\n",
      "              )\n",
      "              (lora): LoRALayer()\n",
      "            )\n",
      "            (lora): LoRALayer()\n",
      "          )\n",
      "          (lora): LoRALayer()\n",
      "        )\n",
      "        (dropout): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (ff): FeedForward(\n",
      "        (layers): Sequential(\n",
      "          (0): LinearWithLoRA(\n",
      "            (linear): LinearWithLoRA(\n",
      "              (linear): LinearWithLoRA(\n",
      "                (linear): LinearWithLoRA(\n",
      "                  (linear): Linear(in_features=768, out_features=3072, bias=True)\n",
      "                  (lora): LoRALayer()\n",
      "                )\n",
      "                (lora): LoRALayer()\n",
      "              )\n",
      "              (lora): LoRALayer()\n",
      "            )\n",
      "            (lora): LoRALayer()\n",
      "          )\n",
      "          (1): GELU()\n",
      "          (2): LinearWithLoRA(\n",
      "            (linear): LinearWithLoRA(\n",
      "              (linear): LinearWithLoRA(\n",
      "                (linear): LinearWithLoRA(\n",
      "                  (linear): Linear(in_features=3072, out_features=768, bias=True)\n",
      "                  (lora): LoRALayer()\n",
      "                )\n",
      "                (lora): LoRALayer()\n",
      "              )\n",
      "              (lora): LoRALayer()\n",
      "            )\n",
      "            (lora): LoRALayer()\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (norm1): LayerNorm()\n",
      "      (norm2): LayerNorm()\n",
      "      (drop_resid): Dropout(p=0.0, inplace=False)\n",
      "    )\n",
      "    (1): TransformerBlock(\n",
      "      (att): MultiHeadAttention(\n",
      "        (W_query): LinearWithLoRA(\n",
      "          (linear): LinearWithLoRA(\n",
      "            (linear): LinearWithLoRA(\n",
      "              (linear): LinearWithLoRA(\n",
      "                (linear): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (lora): LoRALayer()\n",
      "              )\n",
      "              (lora): LoRALayer()\n",
      "            )\n",
      "            (lora): LoRALayer()\n",
      "          )\n",
      "          (lora): LoRALayer()\n",
      "        )\n",
      "        (W_key): LinearWithLoRA(\n",
      "          (linear): LinearWithLoRA(\n",
      "            (linear): LinearWithLoRA(\n",
      "              (linear): LinearWithLoRA(\n",
      "                (linear): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (lora): LoRALayer()\n",
      "              )\n",
      "              (lora): LoRALayer()\n",
      "            )\n",
      "            (lora): LoRALayer()\n",
      "          )\n",
      "          (lora): LoRALayer()\n",
      "        )\n",
      "        (W_value): LinearWithLoRA(\n",
      "          (linear): LinearWithLoRA(\n",
      "            (linear): LinearWithLoRA(\n",
      "              (linear): LinearWithLoRA(\n",
      "                (linear): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (lora): LoRALayer()\n",
      "              )\n",
      "              (lora): LoRALayer()\n",
      "            )\n",
      "            (lora): LoRALayer()\n",
      "          )\n",
      "          (lora): LoRALayer()\n",
      "        )\n",
      "        (out_proj): LinearWithLoRA(\n",
      "          (linear): LinearWithLoRA(\n",
      "            (linear): LinearWithLoRA(\n",
      "              (linear): LinearWithLoRA(\n",
      "                (linear): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (lora): LoRALayer()\n",
      "              )\n",
      "              (lora): LoRALayer()\n",
      "            )\n",
      "            (lora): LoRALayer()\n",
      "          )\n",
      "          (lora): LoRALayer()\n",
      "        )\n",
      "        (dropout): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (ff): FeedForward(\n",
      "        (layers): Sequential(\n",
      "          (0): LinearWithLoRA(\n",
      "            (linear): LinearWithLoRA(\n",
      "              (linear): LinearWithLoRA(\n",
      "                (linear): LinearWithLoRA(\n",
      "                  (linear): Linear(in_features=768, out_features=3072, bias=True)\n",
      "                  (lora): LoRALayer()\n",
      "                )\n",
      "                (lora): LoRALayer()\n",
      "              )\n",
      "              (lora): LoRALayer()\n",
      "            )\n",
      "            (lora): LoRALayer()\n",
      "          )\n",
      "          (1): GELU()\n",
      "          (2): LinearWithLoRA(\n",
      "            (linear): LinearWithLoRA(\n",
      "              (linear): LinearWithLoRA(\n",
      "                (linear): LinearWithLoRA(\n",
      "                  (linear): Linear(in_features=3072, out_features=768, bias=True)\n",
      "                  (lora): LoRALayer()\n",
      "                )\n",
      "                (lora): LoRALayer()\n",
      "              )\n",
      "              (lora): LoRALayer()\n",
      "            )\n",
      "            (lora): LoRALayer()\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (norm1): LayerNorm()\n",
      "      (norm2): LayerNorm()\n",
      "      (drop_resid): Dropout(p=0.0, inplace=False)\n",
      "    )\n",
      "    (2): TransformerBlock(\n",
      "      (att): MultiHeadAttention(\n",
      "        (W_query): LinearWithLoRA(\n",
      "          (linear): LinearWithLoRA(\n",
      "            (linear): LinearWithLoRA(\n",
      "              (linear): LinearWithLoRA(\n",
      "                (linear): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (lora): LoRALayer()\n",
      "              )\n",
      "              (lora): LoRALayer()\n",
      "            )\n",
      "            (lora): LoRALayer()\n",
      "          )\n",
      "          (lora): LoRALayer()\n",
      "        )\n",
      "        (W_key): LinearWithLoRA(\n",
      "          (linear): LinearWithLoRA(\n",
      "            (linear): LinearWithLoRA(\n",
      "              (linear): LinearWithLoRA(\n",
      "                (linear): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (lora): LoRALayer()\n",
      "              )\n",
      "              (lora): LoRALayer()\n",
      "            )\n",
      "            (lora): LoRALayer()\n",
      "          )\n",
      "          (lora): LoRALayer()\n",
      "        )\n",
      "        (W_value): LinearWithLoRA(\n",
      "          (linear): LinearWithLoRA(\n",
      "            (linear): LinearWithLoRA(\n",
      "              (linear): LinearWithLoRA(\n",
      "                (linear): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (lora): LoRALayer()\n",
      "              )\n",
      "              (lora): LoRALayer()\n",
      "            )\n",
      "            (lora): LoRALayer()\n",
      "          )\n",
      "          (lora): LoRALayer()\n",
      "        )\n",
      "        (out_proj): LinearWithLoRA(\n",
      "          (linear): LinearWithLoRA(\n",
      "            (linear): LinearWithLoRA(\n",
      "              (linear): LinearWithLoRA(\n",
      "                (linear): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (lora): LoRALayer()\n",
      "              )\n",
      "              (lora): LoRALayer()\n",
      "            )\n",
      "            (lora): LoRALayer()\n",
      "          )\n",
      "          (lora): LoRALayer()\n",
      "        )\n",
      "        (dropout): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (ff): FeedForward(\n",
      "        (layers): Sequential(\n",
      "          (0): LinearWithLoRA(\n",
      "            (linear): LinearWithLoRA(\n",
      "              (linear): LinearWithLoRA(\n",
      "                (linear): LinearWithLoRA(\n",
      "                  (linear): Linear(in_features=768, out_features=3072, bias=True)\n",
      "                  (lora): LoRALayer()\n",
      "                )\n",
      "                (lora): LoRALayer()\n",
      "              )\n",
      "              (lora): LoRALayer()\n",
      "            )\n",
      "            (lora): LoRALayer()\n",
      "          )\n",
      "          (1): GELU()\n",
      "          (2): LinearWithLoRA(\n",
      "            (linear): LinearWithLoRA(\n",
      "              (linear): LinearWithLoRA(\n",
      "                (linear): LinearWithLoRA(\n",
      "                  (linear): Linear(in_features=3072, out_features=768, bias=True)\n",
      "                  (lora): LoRALayer()\n",
      "                )\n",
      "                (lora): LoRALayer()\n",
      "              )\n",
      "              (lora): LoRALayer()\n",
      "            )\n",
      "            (lora): LoRALayer()\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (norm1): LayerNorm()\n",
      "      (norm2): LayerNorm()\n",
      "      (drop_resid): Dropout(p=0.0, inplace=False)\n",
      "    )\n",
      "    (3): TransformerBlock(\n",
      "      (att): MultiHeadAttention(\n",
      "        (W_query): LinearWithLoRA(\n",
      "          (linear): LinearWithLoRA(\n",
      "            (linear): LinearWithLoRA(\n",
      "              (linear): LinearWithLoRA(\n",
      "                (linear): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (lora): LoRALayer()\n",
      "              )\n",
      "              (lora): LoRALayer()\n",
      "            )\n",
      "            (lora): LoRALayer()\n",
      "          )\n",
      "          (lora): LoRALayer()\n",
      "        )\n",
      "        (W_key): LinearWithLoRA(\n",
      "          (linear): LinearWithLoRA(\n",
      "            (linear): LinearWithLoRA(\n",
      "              (linear): LinearWithLoRA(\n",
      "                (linear): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (lora): LoRALayer()\n",
      "              )\n",
      "              (lora): LoRALayer()\n",
      "            )\n",
      "            (lora): LoRALayer()\n",
      "          )\n",
      "          (lora): LoRALayer()\n",
      "        )\n",
      "        (W_value): LinearWithLoRA(\n",
      "          (linear): LinearWithLoRA(\n",
      "            (linear): LinearWithLoRA(\n",
      "              (linear): LinearWithLoRA(\n",
      "                (linear): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (lora): LoRALayer()\n",
      "              )\n",
      "              (lora): LoRALayer()\n",
      "            )\n",
      "            (lora): LoRALayer()\n",
      "          )\n",
      "          (lora): LoRALayer()\n",
      "        )\n",
      "        (out_proj): LinearWithLoRA(\n",
      "          (linear): LinearWithLoRA(\n",
      "            (linear): LinearWithLoRA(\n",
      "              (linear): LinearWithLoRA(\n",
      "                (linear): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (lora): LoRALayer()\n",
      "              )\n",
      "              (lora): LoRALayer()\n",
      "            )\n",
      "            (lora): LoRALayer()\n",
      "          )\n",
      "          (lora): LoRALayer()\n",
      "        )\n",
      "        (dropout): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (ff): FeedForward(\n",
      "        (layers): Sequential(\n",
      "          (0): LinearWithLoRA(\n",
      "            (linear): LinearWithLoRA(\n",
      "              (linear): LinearWithLoRA(\n",
      "                (linear): LinearWithLoRA(\n",
      "                  (linear): Linear(in_features=768, out_features=3072, bias=True)\n",
      "                  (lora): LoRALayer()\n",
      "                )\n",
      "                (lora): LoRALayer()\n",
      "              )\n",
      "              (lora): LoRALayer()\n",
      "            )\n",
      "            (lora): LoRALayer()\n",
      "          )\n",
      "          (1): GELU()\n",
      "          (2): LinearWithLoRA(\n",
      "            (linear): LinearWithLoRA(\n",
      "              (linear): LinearWithLoRA(\n",
      "                (linear): LinearWithLoRA(\n",
      "                  (linear): Linear(in_features=3072, out_features=768, bias=True)\n",
      "                  (lora): LoRALayer()\n",
      "                )\n",
      "                (lora): LoRALayer()\n",
      "              )\n",
      "              (lora): LoRALayer()\n",
      "            )\n",
      "            (lora): LoRALayer()\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (norm1): LayerNorm()\n",
      "      (norm2): LayerNorm()\n",
      "      (drop_resid): Dropout(p=0.0, inplace=False)\n",
      "    )\n",
      "    (4): TransformerBlock(\n",
      "      (att): MultiHeadAttention(\n",
      "        (W_query): LinearWithLoRA(\n",
      "          (linear): LinearWithLoRA(\n",
      "            (linear): LinearWithLoRA(\n",
      "              (linear): LinearWithLoRA(\n",
      "                (linear): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (lora): LoRALayer()\n",
      "              )\n",
      "              (lora): LoRALayer()\n",
      "            )\n",
      "            (lora): LoRALayer()\n",
      "          )\n",
      "          (lora): LoRALayer()\n",
      "        )\n",
      "        (W_key): LinearWithLoRA(\n",
      "          (linear): LinearWithLoRA(\n",
      "            (linear): LinearWithLoRA(\n",
      "              (linear): LinearWithLoRA(\n",
      "                (linear): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (lora): LoRALayer()\n",
      "              )\n",
      "              (lora): LoRALayer()\n",
      "            )\n",
      "            (lora): LoRALayer()\n",
      "          )\n",
      "          (lora): LoRALayer()\n",
      "        )\n",
      "        (W_value): LinearWithLoRA(\n",
      "          (linear): LinearWithLoRA(\n",
      "            (linear): LinearWithLoRA(\n",
      "              (linear): LinearWithLoRA(\n",
      "                (linear): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (lora): LoRALayer()\n",
      "              )\n",
      "              (lora): LoRALayer()\n",
      "            )\n",
      "            (lora): LoRALayer()\n",
      "          )\n",
      "          (lora): LoRALayer()\n",
      "        )\n",
      "        (out_proj): LinearWithLoRA(\n",
      "          (linear): LinearWithLoRA(\n",
      "            (linear): LinearWithLoRA(\n",
      "              (linear): LinearWithLoRA(\n",
      "                (linear): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (lora): LoRALayer()\n",
      "              )\n",
      "              (lora): LoRALayer()\n",
      "            )\n",
      "            (lora): LoRALayer()\n",
      "          )\n",
      "          (lora): LoRALayer()\n",
      "        )\n",
      "        (dropout): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (ff): FeedForward(\n",
      "        (layers): Sequential(\n",
      "          (0): LinearWithLoRA(\n",
      "            (linear): LinearWithLoRA(\n",
      "              (linear): LinearWithLoRA(\n",
      "                (linear): LinearWithLoRA(\n",
      "                  (linear): Linear(in_features=768, out_features=3072, bias=True)\n",
      "                  (lora): LoRALayer()\n",
      "                )\n",
      "                (lora): LoRALayer()\n",
      "              )\n",
      "              (lora): LoRALayer()\n",
      "            )\n",
      "            (lora): LoRALayer()\n",
      "          )\n",
      "          (1): GELU()\n",
      "          (2): LinearWithLoRA(\n",
      "            (linear): LinearWithLoRA(\n",
      "              (linear): LinearWithLoRA(\n",
      "                (linear): LinearWithLoRA(\n",
      "                  (linear): Linear(in_features=3072, out_features=768, bias=True)\n",
      "                  (lora): LoRALayer()\n",
      "                )\n",
      "                (lora): LoRALayer()\n",
      "              )\n",
      "              (lora): LoRALayer()\n",
      "            )\n",
      "            (lora): LoRALayer()\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (norm1): LayerNorm()\n",
      "      (norm2): LayerNorm()\n",
      "      (drop_resid): Dropout(p=0.0, inplace=False)\n",
      "    )\n",
      "    (5): TransformerBlock(\n",
      "      (att): MultiHeadAttention(\n",
      "        (W_query): LinearWithLoRA(\n",
      "          (linear): LinearWithLoRA(\n",
      "            (linear): LinearWithLoRA(\n",
      "              (linear): LinearWithLoRA(\n",
      "                (linear): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (lora): LoRALayer()\n",
      "              )\n",
      "              (lora): LoRALayer()\n",
      "            )\n",
      "            (lora): LoRALayer()\n",
      "          )\n",
      "          (lora): LoRALayer()\n",
      "        )\n",
      "        (W_key): LinearWithLoRA(\n",
      "          (linear): LinearWithLoRA(\n",
      "            (linear): LinearWithLoRA(\n",
      "              (linear): LinearWithLoRA(\n",
      "                (linear): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (lora): LoRALayer()\n",
      "              )\n",
      "              (lora): LoRALayer()\n",
      "            )\n",
      "            (lora): LoRALayer()\n",
      "          )\n",
      "          (lora): LoRALayer()\n",
      "        )\n",
      "        (W_value): LinearWithLoRA(\n",
      "          (linear): LinearWithLoRA(\n",
      "            (linear): LinearWithLoRA(\n",
      "              (linear): LinearWithLoRA(\n",
      "                (linear): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (lora): LoRALayer()\n",
      "              )\n",
      "              (lora): LoRALayer()\n",
      "            )\n",
      "            (lora): LoRALayer()\n",
      "          )\n",
      "          (lora): LoRALayer()\n",
      "        )\n",
      "        (out_proj): LinearWithLoRA(\n",
      "          (linear): LinearWithLoRA(\n",
      "            (linear): LinearWithLoRA(\n",
      "              (linear): LinearWithLoRA(\n",
      "                (linear): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (lora): LoRALayer()\n",
      "              )\n",
      "              (lora): LoRALayer()\n",
      "            )\n",
      "            (lora): LoRALayer()\n",
      "          )\n",
      "          (lora): LoRALayer()\n",
      "        )\n",
      "        (dropout): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (ff): FeedForward(\n",
      "        (layers): Sequential(\n",
      "          (0): LinearWithLoRA(\n",
      "            (linear): LinearWithLoRA(\n",
      "              (linear): LinearWithLoRA(\n",
      "                (linear): LinearWithLoRA(\n",
      "                  (linear): Linear(in_features=768, out_features=3072, bias=True)\n",
      "                  (lora): LoRALayer()\n",
      "                )\n",
      "                (lora): LoRALayer()\n",
      "              )\n",
      "              (lora): LoRALayer()\n",
      "            )\n",
      "            (lora): LoRALayer()\n",
      "          )\n",
      "          (1): GELU()\n",
      "          (2): LinearWithLoRA(\n",
      "            (linear): LinearWithLoRA(\n",
      "              (linear): LinearWithLoRA(\n",
      "                (linear): LinearWithLoRA(\n",
      "                  (linear): Linear(in_features=3072, out_features=768, bias=True)\n",
      "                  (lora): LoRALayer()\n",
      "                )\n",
      "                (lora): LoRALayer()\n",
      "              )\n",
      "              (lora): LoRALayer()\n",
      "            )\n",
      "            (lora): LoRALayer()\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (norm1): LayerNorm()\n",
      "      (norm2): LayerNorm()\n",
      "      (drop_resid): Dropout(p=0.0, inplace=False)\n",
      "    )\n",
      "    (6): TransformerBlock(\n",
      "      (att): MultiHeadAttention(\n",
      "        (W_query): LinearWithLoRA(\n",
      "          (linear): LinearWithLoRA(\n",
      "            (linear): LinearWithLoRA(\n",
      "              (linear): LinearWithLoRA(\n",
      "                (linear): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (lora): LoRALayer()\n",
      "              )\n",
      "              (lora): LoRALayer()\n",
      "            )\n",
      "            (lora): LoRALayer()\n",
      "          )\n",
      "          (lora): LoRALayer()\n",
      "        )\n",
      "        (W_key): LinearWithLoRA(\n",
      "          (linear): LinearWithLoRA(\n",
      "            (linear): LinearWithLoRA(\n",
      "              (linear): LinearWithLoRA(\n",
      "                (linear): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (lora): LoRALayer()\n",
      "              )\n",
      "              (lora): LoRALayer()\n",
      "            )\n",
      "            (lora): LoRALayer()\n",
      "          )\n",
      "          (lora): LoRALayer()\n",
      "        )\n",
      "        (W_value): LinearWithLoRA(\n",
      "          (linear): LinearWithLoRA(\n",
      "            (linear): LinearWithLoRA(\n",
      "              (linear): LinearWithLoRA(\n",
      "                (linear): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (lora): LoRALayer()\n",
      "              )\n",
      "              (lora): LoRALayer()\n",
      "            )\n",
      "            (lora): LoRALayer()\n",
      "          )\n",
      "          (lora): LoRALayer()\n",
      "        )\n",
      "        (out_proj): LinearWithLoRA(\n",
      "          (linear): LinearWithLoRA(\n",
      "            (linear): LinearWithLoRA(\n",
      "              (linear): LinearWithLoRA(\n",
      "                (linear): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (lora): LoRALayer()\n",
      "              )\n",
      "              (lora): LoRALayer()\n",
      "            )\n",
      "            (lora): LoRALayer()\n",
      "          )\n",
      "          (lora): LoRALayer()\n",
      "        )\n",
      "        (dropout): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (ff): FeedForward(\n",
      "        (layers): Sequential(\n",
      "          (0): LinearWithLoRA(\n",
      "            (linear): LinearWithLoRA(\n",
      "              (linear): LinearWithLoRA(\n",
      "                (linear): LinearWithLoRA(\n",
      "                  (linear): Linear(in_features=768, out_features=3072, bias=True)\n",
      "                  (lora): LoRALayer()\n",
      "                )\n",
      "                (lora): LoRALayer()\n",
      "              )\n",
      "              (lora): LoRALayer()\n",
      "            )\n",
      "            (lora): LoRALayer()\n",
      "          )\n",
      "          (1): GELU()\n",
      "          (2): LinearWithLoRA(\n",
      "            (linear): LinearWithLoRA(\n",
      "              (linear): LinearWithLoRA(\n",
      "                (linear): LinearWithLoRA(\n",
      "                  (linear): Linear(in_features=3072, out_features=768, bias=True)\n",
      "                  (lora): LoRALayer()\n",
      "                )\n",
      "                (lora): LoRALayer()\n",
      "              )\n",
      "              (lora): LoRALayer()\n",
      "            )\n",
      "            (lora): LoRALayer()\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (norm1): LayerNorm()\n",
      "      (norm2): LayerNorm()\n",
      "      (drop_resid): Dropout(p=0.0, inplace=False)\n",
      "    )\n",
      "    (7): TransformerBlock(\n",
      "      (att): MultiHeadAttention(\n",
      "        (W_query): LinearWithLoRA(\n",
      "          (linear): LinearWithLoRA(\n",
      "            (linear): LinearWithLoRA(\n",
      "              (linear): LinearWithLoRA(\n",
      "                (linear): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (lora): LoRALayer()\n",
      "              )\n",
      "              (lora): LoRALayer()\n",
      "            )\n",
      "            (lora): LoRALayer()\n",
      "          )\n",
      "          (lora): LoRALayer()\n",
      "        )\n",
      "        (W_key): LinearWithLoRA(\n",
      "          (linear): LinearWithLoRA(\n",
      "            (linear): LinearWithLoRA(\n",
      "              (linear): LinearWithLoRA(\n",
      "                (linear): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (lora): LoRALayer()\n",
      "              )\n",
      "              (lora): LoRALayer()\n",
      "            )\n",
      "            (lora): LoRALayer()\n",
      "          )\n",
      "          (lora): LoRALayer()\n",
      "        )\n",
      "        (W_value): LinearWithLoRA(\n",
      "          (linear): LinearWithLoRA(\n",
      "            (linear): LinearWithLoRA(\n",
      "              (linear): LinearWithLoRA(\n",
      "                (linear): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (lora): LoRALayer()\n",
      "              )\n",
      "              (lora): LoRALayer()\n",
      "            )\n",
      "            (lora): LoRALayer()\n",
      "          )\n",
      "          (lora): LoRALayer()\n",
      "        )\n",
      "        (out_proj): LinearWithLoRA(\n",
      "          (linear): LinearWithLoRA(\n",
      "            (linear): LinearWithLoRA(\n",
      "              (linear): LinearWithLoRA(\n",
      "                (linear): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (lora): LoRALayer()\n",
      "              )\n",
      "              (lora): LoRALayer()\n",
      "            )\n",
      "            (lora): LoRALayer()\n",
      "          )\n",
      "          (lora): LoRALayer()\n",
      "        )\n",
      "        (dropout): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (ff): FeedForward(\n",
      "        (layers): Sequential(\n",
      "          (0): LinearWithLoRA(\n",
      "            (linear): LinearWithLoRA(\n",
      "              (linear): LinearWithLoRA(\n",
      "                (linear): LinearWithLoRA(\n",
      "                  (linear): Linear(in_features=768, out_features=3072, bias=True)\n",
      "                  (lora): LoRALayer()\n",
      "                )\n",
      "                (lora): LoRALayer()\n",
      "              )\n",
      "              (lora): LoRALayer()\n",
      "            )\n",
      "            (lora): LoRALayer()\n",
      "          )\n",
      "          (1): GELU()\n",
      "          (2): LinearWithLoRA(\n",
      "            (linear): LinearWithLoRA(\n",
      "              (linear): LinearWithLoRA(\n",
      "                (linear): LinearWithLoRA(\n",
      "                  (linear): Linear(in_features=3072, out_features=768, bias=True)\n",
      "                  (lora): LoRALayer()\n",
      "                )\n",
      "                (lora): LoRALayer()\n",
      "              )\n",
      "              (lora): LoRALayer()\n",
      "            )\n",
      "            (lora): LoRALayer()\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (norm1): LayerNorm()\n",
      "      (norm2): LayerNorm()\n",
      "      (drop_resid): Dropout(p=0.0, inplace=False)\n",
      "    )\n",
      "    (8): TransformerBlock(\n",
      "      (att): MultiHeadAttention(\n",
      "        (W_query): LinearWithLoRA(\n",
      "          (linear): LinearWithLoRA(\n",
      "            (linear): LinearWithLoRA(\n",
      "              (linear): LinearWithLoRA(\n",
      "                (linear): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (lora): LoRALayer()\n",
      "              )\n",
      "              (lora): LoRALayer()\n",
      "            )\n",
      "            (lora): LoRALayer()\n",
      "          )\n",
      "          (lora): LoRALayer()\n",
      "        )\n",
      "        (W_key): LinearWithLoRA(\n",
      "          (linear): LinearWithLoRA(\n",
      "            (linear): LinearWithLoRA(\n",
      "              (linear): LinearWithLoRA(\n",
      "                (linear): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (lora): LoRALayer()\n",
      "              )\n",
      "              (lora): LoRALayer()\n",
      "            )\n",
      "            (lora): LoRALayer()\n",
      "          )\n",
      "          (lora): LoRALayer()\n",
      "        )\n",
      "        (W_value): LinearWithLoRA(\n",
      "          (linear): LinearWithLoRA(\n",
      "            (linear): LinearWithLoRA(\n",
      "              (linear): LinearWithLoRA(\n",
      "                (linear): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (lora): LoRALayer()\n",
      "              )\n",
      "              (lora): LoRALayer()\n",
      "            )\n",
      "            (lora): LoRALayer()\n",
      "          )\n",
      "          (lora): LoRALayer()\n",
      "        )\n",
      "        (out_proj): LinearWithLoRA(\n",
      "          (linear): LinearWithLoRA(\n",
      "            (linear): LinearWithLoRA(\n",
      "              (linear): LinearWithLoRA(\n",
      "                (linear): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (lora): LoRALayer()\n",
      "              )\n",
      "              (lora): LoRALayer()\n",
      "            )\n",
      "            (lora): LoRALayer()\n",
      "          )\n",
      "          (lora): LoRALayer()\n",
      "        )\n",
      "        (dropout): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (ff): FeedForward(\n",
      "        (layers): Sequential(\n",
      "          (0): LinearWithLoRA(\n",
      "            (linear): LinearWithLoRA(\n",
      "              (linear): LinearWithLoRA(\n",
      "                (linear): LinearWithLoRA(\n",
      "                  (linear): Linear(in_features=768, out_features=3072, bias=True)\n",
      "                  (lora): LoRALayer()\n",
      "                )\n",
      "                (lora): LoRALayer()\n",
      "              )\n",
      "              (lora): LoRALayer()\n",
      "            )\n",
      "            (lora): LoRALayer()\n",
      "          )\n",
      "          (1): GELU()\n",
      "          (2): LinearWithLoRA(\n",
      "            (linear): LinearWithLoRA(\n",
      "              (linear): LinearWithLoRA(\n",
      "                (linear): LinearWithLoRA(\n",
      "                  (linear): Linear(in_features=3072, out_features=768, bias=True)\n",
      "                  (lora): LoRALayer()\n",
      "                )\n",
      "                (lora): LoRALayer()\n",
      "              )\n",
      "              (lora): LoRALayer()\n",
      "            )\n",
      "            (lora): LoRALayer()\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (norm1): LayerNorm()\n",
      "      (norm2): LayerNorm()\n",
      "      (drop_resid): Dropout(p=0.0, inplace=False)\n",
      "    )\n",
      "    (9): TransformerBlock(\n",
      "      (att): MultiHeadAttention(\n",
      "        (W_query): LinearWithLoRA(\n",
      "          (linear): LinearWithLoRA(\n",
      "            (linear): LinearWithLoRA(\n",
      "              (linear): LinearWithLoRA(\n",
      "                (linear): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (lora): LoRALayer()\n",
      "              )\n",
      "              (lora): LoRALayer()\n",
      "            )\n",
      "            (lora): LoRALayer()\n",
      "          )\n",
      "          (lora): LoRALayer()\n",
      "        )\n",
      "        (W_key): LinearWithLoRA(\n",
      "          (linear): LinearWithLoRA(\n",
      "            (linear): LinearWithLoRA(\n",
      "              (linear): LinearWithLoRA(\n",
      "                (linear): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (lora): LoRALayer()\n",
      "              )\n",
      "              (lora): LoRALayer()\n",
      "            )\n",
      "            (lora): LoRALayer()\n",
      "          )\n",
      "          (lora): LoRALayer()\n",
      "        )\n",
      "        (W_value): LinearWithLoRA(\n",
      "          (linear): LinearWithLoRA(\n",
      "            (linear): LinearWithLoRA(\n",
      "              (linear): LinearWithLoRA(\n",
      "                (linear): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (lora): LoRALayer()\n",
      "              )\n",
      "              (lora): LoRALayer()\n",
      "            )\n",
      "            (lora): LoRALayer()\n",
      "          )\n",
      "          (lora): LoRALayer()\n",
      "        )\n",
      "        (out_proj): LinearWithLoRA(\n",
      "          (linear): LinearWithLoRA(\n",
      "            (linear): LinearWithLoRA(\n",
      "              (linear): LinearWithLoRA(\n",
      "                (linear): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (lora): LoRALayer()\n",
      "              )\n",
      "              (lora): LoRALayer()\n",
      "            )\n",
      "            (lora): LoRALayer()\n",
      "          )\n",
      "          (lora): LoRALayer()\n",
      "        )\n",
      "        (dropout): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (ff): FeedForward(\n",
      "        (layers): Sequential(\n",
      "          (0): LinearWithLoRA(\n",
      "            (linear): LinearWithLoRA(\n",
      "              (linear): LinearWithLoRA(\n",
      "                (linear): LinearWithLoRA(\n",
      "                  (linear): Linear(in_features=768, out_features=3072, bias=True)\n",
      "                  (lora): LoRALayer()\n",
      "                )\n",
      "                (lora): LoRALayer()\n",
      "              )\n",
      "              (lora): LoRALayer()\n",
      "            )\n",
      "            (lora): LoRALayer()\n",
      "          )\n",
      "          (1): GELU()\n",
      "          (2): LinearWithLoRA(\n",
      "            (linear): LinearWithLoRA(\n",
      "              (linear): LinearWithLoRA(\n",
      "                (linear): LinearWithLoRA(\n",
      "                  (linear): Linear(in_features=3072, out_features=768, bias=True)\n",
      "                  (lora): LoRALayer()\n",
      "                )\n",
      "                (lora): LoRALayer()\n",
      "              )\n",
      "              (lora): LoRALayer()\n",
      "            )\n",
      "            (lora): LoRALayer()\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (norm1): LayerNorm()\n",
      "      (norm2): LayerNorm()\n",
      "      (drop_resid): Dropout(p=0.0, inplace=False)\n",
      "    )\n",
      "    (10): TransformerBlock(\n",
      "      (att): MultiHeadAttention(\n",
      "        (W_query): LinearWithLoRA(\n",
      "          (linear): LinearWithLoRA(\n",
      "            (linear): LinearWithLoRA(\n",
      "              (linear): LinearWithLoRA(\n",
      "                (linear): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (lora): LoRALayer()\n",
      "              )\n",
      "              (lora): LoRALayer()\n",
      "            )\n",
      "            (lora): LoRALayer()\n",
      "          )\n",
      "          (lora): LoRALayer()\n",
      "        )\n",
      "        (W_key): LinearWithLoRA(\n",
      "          (linear): LinearWithLoRA(\n",
      "            (linear): LinearWithLoRA(\n",
      "              (linear): LinearWithLoRA(\n",
      "                (linear): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (lora): LoRALayer()\n",
      "              )\n",
      "              (lora): LoRALayer()\n",
      "            )\n",
      "            (lora): LoRALayer()\n",
      "          )\n",
      "          (lora): LoRALayer()\n",
      "        )\n",
      "        (W_value): LinearWithLoRA(\n",
      "          (linear): LinearWithLoRA(\n",
      "            (linear): LinearWithLoRA(\n",
      "              (linear): LinearWithLoRA(\n",
      "                (linear): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (lora): LoRALayer()\n",
      "              )\n",
      "              (lora): LoRALayer()\n",
      "            )\n",
      "            (lora): LoRALayer()\n",
      "          )\n",
      "          (lora): LoRALayer()\n",
      "        )\n",
      "        (out_proj): LinearWithLoRA(\n",
      "          (linear): LinearWithLoRA(\n",
      "            (linear): LinearWithLoRA(\n",
      "              (linear): LinearWithLoRA(\n",
      "                (linear): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (lora): LoRALayer()\n",
      "              )\n",
      "              (lora): LoRALayer()\n",
      "            )\n",
      "            (lora): LoRALayer()\n",
      "          )\n",
      "          (lora): LoRALayer()\n",
      "        )\n",
      "        (dropout): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (ff): FeedForward(\n",
      "        (layers): Sequential(\n",
      "          (0): LinearWithLoRA(\n",
      "            (linear): LinearWithLoRA(\n",
      "              (linear): LinearWithLoRA(\n",
      "                (linear): LinearWithLoRA(\n",
      "                  (linear): Linear(in_features=768, out_features=3072, bias=True)\n",
      "                  (lora): LoRALayer()\n",
      "                )\n",
      "                (lora): LoRALayer()\n",
      "              )\n",
      "              (lora): LoRALayer()\n",
      "            )\n",
      "            (lora): LoRALayer()\n",
      "          )\n",
      "          (1): GELU()\n",
      "          (2): LinearWithLoRA(\n",
      "            (linear): LinearWithLoRA(\n",
      "              (linear): LinearWithLoRA(\n",
      "                (linear): LinearWithLoRA(\n",
      "                  (linear): Linear(in_features=3072, out_features=768, bias=True)\n",
      "                  (lora): LoRALayer()\n",
      "                )\n",
      "                (lora): LoRALayer()\n",
      "              )\n",
      "              (lora): LoRALayer()\n",
      "            )\n",
      "            (lora): LoRALayer()\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (norm1): LayerNorm()\n",
      "      (norm2): LayerNorm()\n",
      "      (drop_resid): Dropout(p=0.0, inplace=False)\n",
      "    )\n",
      "    (11): TransformerBlock(\n",
      "      (att): MultiHeadAttention(\n",
      "        (W_query): LinearWithLoRA(\n",
      "          (linear): LinearWithLoRA(\n",
      "            (linear): LinearWithLoRA(\n",
      "              (linear): LinearWithLoRA(\n",
      "                (linear): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (lora): LoRALayer()\n",
      "              )\n",
      "              (lora): LoRALayer()\n",
      "            )\n",
      "            (lora): LoRALayer()\n",
      "          )\n",
      "          (lora): LoRALayer()\n",
      "        )\n",
      "        (W_key): LinearWithLoRA(\n",
      "          (linear): LinearWithLoRA(\n",
      "            (linear): LinearWithLoRA(\n",
      "              (linear): LinearWithLoRA(\n",
      "                (linear): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (lora): LoRALayer()\n",
      "              )\n",
      "              (lora): LoRALayer()\n",
      "            )\n",
      "            (lora): LoRALayer()\n",
      "          )\n",
      "          (lora): LoRALayer()\n",
      "        )\n",
      "        (W_value): LinearWithLoRA(\n",
      "          (linear): LinearWithLoRA(\n",
      "            (linear): LinearWithLoRA(\n",
      "              (linear): LinearWithLoRA(\n",
      "                (linear): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (lora): LoRALayer()\n",
      "              )\n",
      "              (lora): LoRALayer()\n",
      "            )\n",
      "            (lora): LoRALayer()\n",
      "          )\n",
      "          (lora): LoRALayer()\n",
      "        )\n",
      "        (out_proj): LinearWithLoRA(\n",
      "          (linear): LinearWithLoRA(\n",
      "            (linear): LinearWithLoRA(\n",
      "              (linear): LinearWithLoRA(\n",
      "                (linear): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (lora): LoRALayer()\n",
      "              )\n",
      "              (lora): LoRALayer()\n",
      "            )\n",
      "            (lora): LoRALayer()\n",
      "          )\n",
      "          (lora): LoRALayer()\n",
      "        )\n",
      "        (dropout): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (ff): FeedForward(\n",
      "        (layers): Sequential(\n",
      "          (0): LinearWithLoRA(\n",
      "            (linear): LinearWithLoRA(\n",
      "              (linear): LinearWithLoRA(\n",
      "                (linear): LinearWithLoRA(\n",
      "                  (linear): Linear(in_features=768, out_features=3072, bias=True)\n",
      "                  (lora): LoRALayer()\n",
      "                )\n",
      "                (lora): LoRALayer()\n",
      "              )\n",
      "              (lora): LoRALayer()\n",
      "            )\n",
      "            (lora): LoRALayer()\n",
      "          )\n",
      "          (1): GELU()\n",
      "          (2): LinearWithLoRA(\n",
      "            (linear): LinearWithLoRA(\n",
      "              (linear): LinearWithLoRA(\n",
      "                (linear): LinearWithLoRA(\n",
      "                  (linear): Linear(in_features=3072, out_features=768, bias=True)\n",
      "                  (lora): LoRALayer()\n",
      "                )\n",
      "                (lora): LoRALayer()\n",
      "              )\n",
      "              (lora): LoRALayer()\n",
      "            )\n",
      "            (lora): LoRALayer()\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (norm1): LayerNorm()\n",
      "      (norm2): LayerNorm()\n",
      "      (drop_resid): Dropout(p=0.0, inplace=False)\n",
      "    )\n",
      "  )\n",
      "  (final_norm): LayerNorm()\n",
      "  (out_head): LinearWithLoRA(\n",
      "    (linear): LinearWithLoRA(\n",
      "      (linear): LinearWithLoRA(\n",
      "        (linear): LinearWithLoRA(\n",
      "          (linear): Linear(in_features=768, out_features=2, bias=True)\n",
      "          (lora): LoRALayer()\n",
      "        )\n",
      "        (lora): LoRALayer()\n",
      "      )\n",
      "      (lora): LoRALayer()\n",
      "    )\n",
      "    (lora): LoRALayer()\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- we have now 50x less parameters when using LoRA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training accuracy: 46.25%\n",
      "Validation accuracy: 45.00%\n",
      "Test accuracy: 48.75%\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(123)\n",
    "train_accuracy = calc_accuracy_loader(train_loader, model, device, num_batches=10)\n",
    "val_accuracy = calc_accuracy_loader(val_loader, model, device, num_batches=10)\n",
    "test_accuracy = calc_accuracy_loader(test_loader, model, device, num_batches=10)\n",
    "\n",
    "print(f\"Training accuracy: {train_accuracy*100:.2f}%\")\n",
    "print(f\"Validation accuracy: {val_accuracy*100:.2f}%\")\n",
    "print(f\"Test accuracy: {test_accuracy*100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ep 1 (Step 000000): Train loss 3.563, Val loss 3.216\n",
      "Ep 1 (Step 000050): Train loss 0.355, Val loss 0.330\n",
      "Ep 1 (Step 000100): Train loss 0.166, Val loss 0.389\n",
      "Training accuracy: 97.50% | Validation accuracy: 100.00%\n",
      "Ep 2 (Step 000150): Train loss 0.218, Val loss 0.061\n",
      "Ep 2 (Step 000200): Train loss 0.029, Val loss 0.046\n",
      "Ep 2 (Step 000250): Train loss 0.207, Val loss 0.049\n",
      "Training accuracy: 97.50% | Validation accuracy: 95.00%\n",
      "Ep 3 (Step 000300): Train loss 0.070, Val loss 0.136\n",
      "Ep 3 (Step 000350): Train loss 0.014, Val loss 0.052\n",
      "Training accuracy: 100.00% | Validation accuracy: 97.50%\n",
      "Ep 4 (Step 000400): Train loss 0.006, Val loss 0.127\n",
      "Ep 4 (Step 000450): Train loss 0.043, Val loss 0.532\n",
      "Ep 4 (Step 000500): Train loss 0.030, Val loss 0.069\n",
      "Training accuracy: 97.50% | Validation accuracy: 95.00%\n",
      "Ep 5 (Step 000550): Train loss 0.113, Val loss 0.058\n",
      "Ep 5 (Step 000600): Train loss 0.014, Val loss 0.108\n",
      "Training accuracy: 100.00% | Validation accuracy: 97.50%\n",
      "Training completed in 63.12 minutes.\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "from previous_chapters2 import train_classifier_simple\n",
    "\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "torch.manual_seed(123)\n",
    "\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=5e-5, weight_decay=0.1)\n",
    "\n",
    "num_epochs = 5\n",
    "train_losses, val_losses, train_accs, val_accs, examples_seen = train_classifier_simple(\n",
    "    model, train_loader, val_loader, optimizer, device,\n",
    "    num_epochs=num_epochs, eval_freq=50, eval_iter=5,\n",
    ")\n",
    "\n",
    "end_time = time.time()\n",
    "execution_time_minutes = (end_time - start_time) / 60\n",
    "print(f\"Training completed in {execution_time_minutes:.2f} minutes.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAdwAAAEiCAYAAABTO2OcAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjkuMCwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy80BEi2AAAACXBIWXMAAA9hAAAPYQGoP6dpAABRDElEQVR4nO3dd3xUVdrA8d+UzCST3kghhRZCTQjVSBGlqyj2ZVkFl9UXBRERC6sC4uvG3l1UVHhdFVQU14IgIE16J7TQSQKEJIT0PnPePyaZZKhJSDIpz/fzuZ+ZuffOvc8cwjxz7jn3HI1SSiGEEEKIOqV1dABCCCFEcyAJVwghhKgHknCFEEKIeiAJVwghhKgHknCFEEKIeiAJVwghhKgHknCFEEKIeiAJVwghhKgHknCFEEKIeiAJV4hmaODAgUyZMsXRYQjRrEjCFaIGxo0bh0ajuWgZPny4o0MTQjRQekcHIERjNXz4cObNm2e3zmg0OigaIURDJzVcIWrIaDQSGBhot3h7ewOwevVqDAYD69ats+3/2muv0aJFC86ePQvA0qVL6devH15eXvj6+nLrrbdy9OhR2/4nTpxAo9Hw7bff0r9/f1xcXOjVqxeHDh1i69at9OzZEzc3N0aMGEFaWprtfePGjWPUqFG8+OKL+Pv74+HhwYQJEyguLr7sZykqKmLatGm0bNkSV1dX+vTpw+rVq23bT548yciRI/H29sbV1ZXOnTuzZMmSyx7v3//+NxERETg7OxMQEMDdd99t22axWIiLi6N169a4uLgQHR3NokWL7N6/d+9eRowYgZubGwEBAdx///2kp6fbtg8cOJDJkyfz9NNP4+PjQ2BgILNmzbpsPEI0BJJwhagD5W2k999/P1lZWezcuZMXXniBTz/9lICAAADy8vKYOnUq27ZtY+XKlWi1Wu644w4sFovdsWbOnMnzzz/Pjh070Ov1/PWvf+Xpp5/m3XffZd26dRw5coQZM2bYvWflypUcOHCA1atXs2DBAn744QdefPHFy8Y7adIkNm7cyMKFC9mzZw/33HMPw4cP5/DhwwBMnDiRoqIi1q5dS3x8PK+++ipubm6XPNa2bduYPHkys2fPJiEhgaVLlzJgwADb9ri4OL744gs++ugj9u3bxxNPPMHf/vY31qxZA0BmZiY33XQTMTExbNu2jaVLl3L27Fnuvfdeu/P83//9H66urmzevJnXXnuN2bNns3z58ir+CwnhAEoIUW1jx45VOp1Oubq62i0vv/yybZ+ioiLVrVs3de+996pOnTqphx566IrHTEtLU4CKj49XSil1/PhxBahPP/3Uts+CBQsUoFauXGlbFxcXpyIjI+1i8/HxUXl5ebZ1c+bMUW5ubspsNiullLrhhhvU448/rpRS6uTJk0qn06lTp07ZxTNo0CA1ffp0pZRSXbt2VbNmzapS2Xz//ffKw8NDZWdnX7StsLBQmUwmtWHDBrv148ePV6NHj1ZKKfXSSy+poUOH2m1PSkpSgEpISLDF369fP7t9evXqpZ555pkqxSiEI0gbrhA1dOONNzJnzhy7dT4+PrbnBoOBr776iqioKMLDw3n77bft9j18+DAzZsxg8+bNpKen22q2iYmJdOnSxbZfVFSU7Xl57bhr165261JTU+2OHR0djclksr2OjY0lNzeXpKQkwsPD7faNj4/HbDbTvn17u/VFRUX4+voCMHnyZB555BF+//13Bg8ezF133WUXV2VDhgwhPDycNm3aMHz4cIYPH84dd9yByWTiyJEj5OfnM2TIELv3FBcXExMTA8Du3btZtWrVJWvQR48etcV54fmDgoIuKgchGhJJuELUkKurK+3atbviPhs2bAAgIyODjIwMXF1dbdtGjhxJeHg4c+fOJTg4GIvFQpcuXS5qa3VycrI912g0l1x34WXo6sjNzUWn07F9+3Z0Op3dtvKk949//INhw4bx66+/8vvvvxMXF8ebb77JY489dtHx3N3d2bFjB6tXr+b3339nxowZzJo1i61bt5KbmwvAr7/+SsuWLe3eV97hLDc3l5EjR/Lqq69edOygoCDb88plANdeDkLUNUm4QtSRo0eP8sQTTzB37ly++eYbxo4dy4oVK9BqtZw7d46EhATmzp1L//79Afjzzz9r7dy7d++moKAAFxcXADZt2oSbmxuhoaEX7RsTE4PZbCY1NdUWy6WEhoYyYcIEJkyYwPTp05k7d+4lEy6AXq9n8ODBDB48mJkzZ+Ll5cUff/zBkCFDMBqNJCYmcsMNN1zyvd27d+f777+nVatW6PXyFSWaDvlrFqKGioqKSElJsVun1+vx8/PDbDbzt7/9jWHDhvHggw8yfPhwunbtyptvvslTTz2Ft7c3vr6+fPLJJwQFBZGYmMizzz5ba7EVFxczfvx4nn/+eU6cOMHMmTOZNGkSWu3F/STbt2/PmDFjeOCBB3jzzTeJiYkhLS2NlStXEhUVxS233MKUKVMYMWIE7du35/z586xatYqOHTte8ty//PILx44dY8CAAXh7e7NkyRIsFguRkZG4u7szbdo0nnjiCSwWC/369SMrK4v169fj4eHB2LFjmThxInPnzmX06NG2XshHjhxh4cKFfPrppxfVwoVoLCThClFDS5cutbvECRAZGcnBgwd5+eWXOXnyJL/88gtgvRT6ySefMHr0aIYOHUp0dDQLFy5k8uTJdOnShcjISN577z0GDhxYK7ENGjSIiIgIBgwYQFFREaNHj77ibTPz5s3jf//3f3nyySc5deoUfn5+XHfdddx6660AmM1mJk6cSHJyMh4eHgwfPvyiNulyXl5e/PDDD8yaNYvCwkIiIiJYsGABnTt3BuCll17C39+fuLg4jh07hpeXF927d+ef//wnAMHBwaxfv55nnnmGoUOHUlRURHh4OMOHD7/kDwYhGguNUko5OgghRO0ZN24cmZmZ/Pjjj44ORQhRifxcFEIIIeqBJFwhhBCiHsglZSGEEKIeSA1XCCGEqAeScIUQQoh6IAlXCCGEqAeScMt8+OGHtGrVCmdnZ/r06cOWLVscHVKdWLt2LSNHjiQ4OBiNRnPRrSNKKWbMmEFQUBAuLi4MHjzYNmNMuYyMDMaMGYOHhwdeXl6MHz/eNmRfuT179tC/f3+cnZ0JDQ3ltddeq+uPVmvi4uLo1asX7u7utGjRglGjRpGQkGC3T2FhIRMnTsTX1xc3Nzfuuusu27R75RITE7nlllswmUy0aNGCp556itLSUrt9Vq9eTffu3TEajbRr14758+fX9cerFXPmzCEqKgoPDw88PDyIjY3lt99+s21v7uVzKa+88goajYYpU6bY1kk5waxZs9BoNHZLhw4dbNubVBk5dOqEBmLhwoXKYDCozz//XO3bt0899NBDysvLS509e9bRodW6JUuWqOeee0798MMPClCLFy+22/7KK68oT09P9eOPP6rdu3er2267TbVu3VoVFBTY9hk+fLiKjo5WmzZtUuvWrVPt2rWzzfSilFJZWVkqICBAjRkzRu3du1ctWLBAubi4qI8//ri+PuY1GTZsmJo3b57au3ev2rVrl7r55ptVWFiYys3Nte0zYcIEFRoaqlauXKm2bdumrrvuOnX99dfbtpeWlqouXbqowYMHq507d6olS5YoPz8/2+w7Sil17NgxZTKZ1NSpU9X+/fvV+++/r3Q6nVq6dGm9ft6a+Omnn9Svv/6qDh06pBISEtQ///lP5eTkpPbu3auUkvK50JYtW1SrVq1UVFSUbZYmpaSclFJq5syZqnPnzurMmTO2JS0tzba9KZWRJFylVO/evdXEiRNtr81mswoODlZxcXEOjKruXZhwLRaLCgwMVK+//rptXWZmpjIajWrBggVKKaX279+vALV161bbPr/99pvSaDS26d3+/e9/K29vb1VUVGTb55lnnrGbQq4xSU1NVYBas2aNUspaJk5OTuq7776z7XPgwAEFqI0bNyqlrD9stFqtSklJse0zZ84c5eHhYSuXp59+WnXu3NnuXPfdd58aNmxYXX+kOuHt7a0+/fRTKZ8L5OTkqIiICLV8+XK7aRGlnKxmzpypoqOjL7mtqZVRs7+kXFxczPbt2xk8eLBtnVarZfDgwWzcuNGBkdW/48ePk5KSYlcWnp6e9OnTx1YWGzduxMvLi549e9r2GTx4MFqtls2bN9v2GTBgAAaDwbbPsGHDSEhI4Pz58/X0aWpPVlYWUDH13vbt2ykpKbErpw4dOhAWFmZXTl27drVNpwfWMsjOzmbfvn22fSofo3yfxvZ3ZzabWbhwIXl5ecTGxkr5XGDixInccsstF30WKacKhw8fJjg4mDZt2jBmzBgSExOBpldGzT7hpqenYzab7f6xwDrH6IUD0zd15Z/3SmWRkpJCixYt7Lbr9Xp8fHzs9rnUMSqfo7GwWCxMmTKFvn372uaoTUlJwWAw4OXlZbfvheV0tTK43D7Z2dkUFBTUxcepVfHx8bi5uWE0GpkwYQKLFy+mU6dOUj6VLFy4kB07dhAXF3fRNiknqz59+jB//nyWLl3KnDlzOH78OP379ycnJ6fJlZFMXiDEFUycOJG9e/fW6tR5TUVkZCS7du0iKyuLRYsWMXbsWNasWePosBqMpKQkHn/8cZYvX46zs7Ojw2mwRowYYXseFRVFnz59CA8P59tvv7VNL9lUNPsarp+fHzqd7qJeb2fPniUwMNBBUTlG+ee9UlkEBgaSmppqt720tJSMjAy7fS51jMrnaAwmTZrEL7/8wqpVqwgJCbGtDwwMpLi4mMzMTLv9Lyynq5XB5fbx8PBoFF80BoOBdu3a0aNHD+Li4oiOjubdd9+V8imzfft2UlNT6d69O3q9Hr1ez5o1a3jvvffQ6/UEBARIOV2Cl5cX7du358iRI03ub6nZJ1yDwUCPHj1YuXKlbZ3FYmHlypXExsY6MLL617p1awIDA+3KIjs7m82bN9vKIjY2lszMTLZv327b548//sBisdCnTx/bPmvXrqWkpMS2z/Lly4mMjMTb27uePk3NKaWYNGkSixcv5o8//qB169Z223v06IGTk5NdOSUkJJCYmGhXTvHx8XY/TpYvX46HhwedOnWy7VP5GOX7NNa/O4vFQlFRkZRPmUGDBhEfH8+uXbtsS8+ePRkzZoztuZTTxXJzczl69ChBQUFN72+pXrtoNVALFy5URqNRzZ8/X+3fv189/PDDysvLy67XW1ORk5Ojdu7cqXbu3KkA9dZbb6mdO3eqkydPKqWstwV5eXmp//73v2rPnj3q9ttvv+RtQTExMWrz5s3qzz//VBEREXa3BWVmZqqAgAB1//33q71796qFCxcqk8nUaG4LeuSRR5Snp6davXq13a0K+fn5tn0mTJigwsLC1B9//KG2bdumYmNjVWxsrG17+a0KQ4cOVbt27VJLly5V/v7+l7xV4amnnlIHDhxQH374YaO5nePZZ59Va9asUcePH1d79uxRzz77rNJoNOr3339XSkn5XE7lXspKSTkppdSTTz6pVq9erY4fP67Wr1+vBg8erPz8/FRqaqpSqmmVkSTcMu+//74KCwtTBoNB9e7dW23atMnRIdWJVatWKeCiZezYsUop661BL7zwggoICFBGo1ENGjRIJSQk2B3j3LlzavTo0crNzU15eHioBx98UOXk5Njts3v3btWvXz9lNBpVy5Yt1SuvvFJfH/GaXap8ADVv3jzbPgUFBerRRx9V3t7eymQyqTvuuEOdOXPG7jgnTpxQI0aMUC4uLsrPz089+eSTqqSkxG6fVatWqW7duimDwaDatGljd46G7O9//7sKDw9XBoNB+fv7q0GDBtmSrVJSPpdzYcKVcrLenhMUFKQMBoNq2bKluu+++9SRI0ds25tSGclsQUIIIUQ9aPZtuEIIIUR9kIQrhBBC1ANJuEIIIUQ9kIQrhBBC1ANJuEIIIUQ9kIQrhBBC1ANJuGWKioqYNWsWRUVFjg6lwZIyqhopp6uTMro6KaOra2xlJPfhlsnOzsbT05OsrCw8PDwcHU6DJGVUNVJOVydldHVSRlfX2MpIarhCCCFEPZCEK4QQQtSDRj0fbmlpKTt37iQgIACt9tp+O+Tk5ABw6tQpsrOzayO8JkfKqGqknK5OyujqpIyurqGUkcVi4ezZs8TExKDXXz6tNuo23K1bt9K7d29HhyGEEEKwZcsWevXqddntjbqGGxAQAFg/ZFBQkIOjEUII0RydOXOG3r1723LS5TTqhFt+GTkoKIiQkBAHRyOEEKI5u1rTpnSaEkIIIeqBJFwhhBCiHkjCFUIIIepBo27DFUKIKzGbzZSUlDg6DNHIOTk5odPprvk4knABpRSHzuayOzmTUd1aYtBLxV+IxkwpRUpKCpmZmY4ORTQRXl5eBAYGotFoanwMSbhl7vloA9mFpXQK8qBLS09HhyOEuAblybZFixaYTKZr+pIUzZtSivz8fFJTUwGu6RZUSbiARqMhOtSLdYfT2Z2cKQlXiEbMbDbbkq2vr6+jwxFNgIuLCwCpqam0aNGixpeX5dppmagQa5Ldk5Tl4EiEENeivM3WZDI5OBLRlJT/PV1LnwBJuGWiQrwA2J2c6dA4hBC1Qy4ji9pUG39PknDLRJcl3ENnc8gvLnVsMEIIIZocSbhlAj2daeFuxKJg32mZmUMI0fi1atWKd955p8r7r169Go1GU+e9u+fPn4+Xl1ednqMhkoRbie2yclKmQ+MQQjQvGo3misusWbNqdNytW7fy8MMPV3n/66+/njNnzuDpKR1H64L0Uq4kOsSTFQfOsidZOk4JIerPmTNnbM+/+eYbZsyYQUJCgm2dm5ub7blSCrPZfMV5V8v5+/tXKw6DwUBgYGC13iOqTmq4lUSHegGwRzpOCSHqUWBgoG3x9PREo9HYXh88eBB3d3d+++03evTogdFo5M8//+To0aPcfvvtBAQE4ObmRq9evVixYoXdcS+8pKzRaPj000+54447MJlMRERE8NNPP9m2X3hJufzS77Jly+jYsSNubm4MHz7c7gdCaWkpkydPxsvLC19fX5555hnGjh3LqFGjqlUGc+bMoW3bthgMBiIjI/nPf/5j26aUYtasWYSFhWE0GgkODmby5Mm27f/+97+JiIjA2dmZgIAA7r777mqdu75Iwq2k/NagE+fyycqX4eCEaCqUUuQXl9b7opSqtc/w7LPP8sorr3DgwAGioqLIzc3l5ptvZuXKlezcuZPhw4czcuRIEhMTr3icF198kXvvvZc9e/Zw8803M2bMGDIyMi67f35+Pm+88Qb/+c9/WLt2LYmJiUybNs22/dVXX+Wrr75i3rx5rF+/nuzsbH788cdqfbbFixfz+OOP8+STT7J3717+53/+hwcffJBVq1YB8P333/P222/z8ccfc/jwYX788Ue6du0KwLZt25g8eTKzZ88mISGBpUuXMmDAgGqdv77IJeVKvEwGwn1NnDyXz55TmfSPqN7lGCFEw1RQYqbTjGX1ft79s4dhMtTO1+zs2bMZMmSI7bWPjw/R0dG21y+99BKLFy/mp59+YtKkSZc9zrhx4xg9ejQA//rXv3jvvffYsmULw4cPv+T+JSUlfPTRR7Rt2xaASZMmMXv2bNv2999/n+nTp3PHHXcA8MEHH7BkyZJqfbY33niDcePG8eijjwIwdepUNm3axBtvvMGNN95IYmIigYGBDB48GCcnJ8LCwujduzcAiYmJuLq6cuutt+Lu7k54eDgxMTHVOn99kRruBco7Tkk7rhCiIenZs6fd69zcXKZNm0bHjh3x8vLCzc2NAwcOXLWGGxUVZXvu6uqKh4eHbdjCSzGZTLZkC9ahDcv3z8rK4uzZs7bkB6DT6ejRo0e1PtuBAwfo27ev3bq+ffty4MABAO655x4KCgpo06YNDz30EIsXL6a01Hr75pAhQwgPD6dNmzbcf//9fPXVV+Tn51fr/PVFargXiA7x5Ofdp9klPZWFaDJcnHTsnz3MIeetLa6urnavp02bxvLly3njjTdo164dLi4u3H333RQXF1/xOE5OTnavNRoNFoulWvvX5qXyqggNDSUhIYEVK1awfPlyHn30UV5//XXWrFmDu7s7O3bsYPXq1fz+++/MmDGDWbNmsXXr1gZ365HUcC9QUcPNdGgcQojao9FoMBn09b7U5WhX69evZ9y4cdxxxx107dqVwMBATpw4UWfnuxRPT08CAgLYunWrbZ3ZbGbHjh3VOk7Hjh1Zv3693br169fTqVMn22sXFxdGjhzJe++9x+rVq9m4cSPx8fEA6PV6Bg8ezGuvvcaePXs4ceIEf/zxxzV8srohNdwLdGnpgVYDZ7OLOJtdSICHs6NDEkKIi0RERPDDDz8wcuRINBoNL7zwwhVrqnXlscceIy4ujnbt2tGhQwfef/99zp8/X60fG0899RT33nsvMTExDB48mJ9//pkffvjB1ut6/vz5mM1m+vTpg8lk4ssvv8TFxYXw8HB++eUXjh07xoABA/D29mbJkiVYLBYiIyPr6iPXmNRwL2Ay6Ilo4Q7IABhCiIbrrbfewtvbm+uvv56RI0cybNgwunfvXu9xPPPMM4wePZoHHniA2NhY3NzcGDZsGM7OVa+sjBo1infffZc33niDzp078/HHHzNv3jwGDhwIWOeinTt3Ln379iUqKooVK1bw888/4+vri5eXFz/88AM33XQTHTt25KOPPmLBggV07ty5jj5xzWlUfV+Mr0XJycmEhoaSlJRESEhIrR336UW7+XZbMpNubMe0YQ3vV5IQ4vIKCws5fvw4rVu3rtaXvqgdFouFjh07cu+99/LSSy85Opxac6W/q6rmIqnhltvwPsy9CQ4vl5mDhBCiik6ePMncuXM5dOgQ8fHxPPLIIxw/fpy//vWvjg6twZGEWy4tAU5thxN/2mYOij+VVe+98YQQojHRarXMnz+fXr160bdvX+Lj41mxYgUdO3Z0dGgNjnSaKhd2Hez8DyRtJvJGdww6LZn5JSRm5BPu63r19wshRDMUGhp6UQ9jcWlSwy0XFmt9PLUDAyV0DPYAkPtxhRBC1ApJuOV82oDJD8xFcGY30WXjKsuIU0IIIWqDJNxyGo31sjJA4kYZAEMIIUStcmjCnTNnDlFRUXh4eODh4UFsbCy//fab4wIK7WN9TNxsq+HuPZVNqbn+byYXQgjRtDg04YaEhPDKK6+wfft2tm3bxk033cTtt9/Ovn37HBNQeQ03aTNt/FxxM+opKDFzJC3XMfEIIYRoMhyacEeOHMnNN99MREQE7du35+WXX8bNzY1NmzY5JqCgaNAZIT8d3fljdGlp7Ti1J0nacYUQQlybBtOGazabWbhwIXl5ecTGxjomCL0RWpYNjZa0yXY/rgyAIYRoDAYOHMiUKVNsr1u1asU777xzxfdoNJpqTxhfl8e5klmzZtGtW7c6PUddcnjCjY+Px83NDaPRyIQJE1i8eLHdDBGVFRUVkZ2dbVtycnJqPyBbO+4mmRtXCFEvRo4cedkJ4NetW4dGo2HPnj3VPu7WrVt5+OGHrzU8O5dLemfOnGHEiBG1eq6mxuEJNzIykl27drF582YeeeQRxo4dy/79+y+5b1xcHJ6enrblcon5mlRqx40q6zh14Ew2hSXm2j+XEEIA48ePZ/ny5SQnJ1+0bd68efTs2dNu4viq8vf3x2Qy1UaIVxUYGIjRaKyXczVWDk+4BoOBdu3a0aNHD+Li4oiOjubdd9+95L7Tp08nKyvLtlwuMV+T8hpu+iFCjAX4uBootSgOnMmu/XMJIQRw66234u/vz/z58+3W5+bm8t133zF+/HjOnTvH6NGjadmyJSaTia5du7JgwYIrHvfCS8qHDx9mwIABODs706lTJ5YvX37Re5555hnat2+PyWSiTZs2vPDCC5SUlADWafJefPFFdu/ejUajQaPR2GK+8JJyfHw8N910Ey4uLvj6+vLwww+Tm1vRAXXcuHGMGjWKN954g6CgIHx9fZk4caLtXFVhsViYPXs2ISEhGI1GunXrxtKlS23bi4uLmTRpEkFBQTg7OxMeHk5cXBwASilmzZpFWFgYRqOR4OBgJk+eXOVz10SDG9rRYrFQVFR0yW1Go9HuF1R2dh0kQZMP3PYBtOiExtmTqBBPVieksSc5i5gw79o/nxCi/hTnVf89OiPoyr4qzaXWwXE0WnByufJxDVUfElav1/PAAw8wf/58nnvuOdtcst999x1ms5nRo0eTm5tLjx49eOaZZ/Dw8ODXX3/l/vvvp23btvTu3fuq57BYLNx5550EBASwefNmsrKy7Np7y7m7uzN//nyCg4OJj4/noYcewt3dnaeffpr77ruPvXv3snTpUttctZ6enhcdIy8vj2HDhhEbG8vWrVtJTU3lH//4B5MmTbL7UbFq1SqCgoJYtWoVR44c4b777qNbt2489NBDVSq3d999lzfffJOPP/6YmJgYPv/8c2677Tb27dtHREQE7733Hj/99BPffvstYWFhJCUlkZSUBMD333/P22+/zcKFC+ncuTMpKSns3r27SuetKYcm3OnTpzNixAjCwsLIycnh66+/ZvXq1SxbtsyRYUH3+21Po0K8WJ2QJh2nhGgK/hVc/ffcMx8632F9fvBn+G4chPeDB3+t2OedrpB/zv59s6rX9+Pvf/87r7/+OmvWrLHNAztv3jzuuusuWzPatGnTbPs/9thjLFu2jG+//bZKCXfFihUcPHiQZcuWERxsLYd//etfF7W7Pv/887bnrVq1Ytq0aSxcuJCnn34aFxcX3Nzc0Ov1BAYGXvZcX3/9NYWFhXzxxRe4ulp/eHzwwQeMHDmSV199lYCAAAC8vb354IMP0Ol0dOjQgVtuuYWVK1dWOeG+8cYbPPPMM/zlL38B4NVXX2XVqlW88847fPjhhyQmJhIREUG/fv3QaDSEh4fb3puYmEhgYCCDBw/GycmJsLCwKpXjtXDoJeXU1FQeeOABIiMjGTRoEFu3bmXZsmUMGTLEkWHZ6RYqQzwKIepehw4duP766/n8888BOHLkCOvWrWP8+PGA9U6Ol156ia5du+Lj44ObmxvLli0jMTGxSsc/cOAAoaGhtmQLXPKOkG+++Ya+ffsSGBiIm5sbzz//fJXPUflc0dHRtmQL0LdvXywWCwkJCbZ1nTt3RqfT2V4HBQWRmppapXNkZ2dz+vRp+vbta7e+b9++HDhwALBett61axeRkZFMnjyZ33//3bbfPffcQ0FBAW3atOGhhx5i8eLFlJaWVutzVpdDa7ifffaZI09/eeZSiP/W2nGqv3UC5aNpueQWleJmbHBX4YUQVfXP09V/j65SR6AOI63H0FxQV5kSf21xlRk/fjyPPfYYH374IfPmzaNt27bccMMNALz++uu8++67vPPOO3Tt2hVXV1emTJlCcXFxrZwbYOPGjYwZM4YXX3yRYcOG4enpycKFC3nzzTdr7RyVOTk52b3WaDRYLLU3sl/37t05fvw4v/32GytWrODee+9l8ODBLFq0iNDQUBISElixYgXLly/n0UcftV1huDCu2uLwTlMNklYHv78A2+fjl3OQll4uKAXxUssVonEzuFZ/0VX6ka3TW9dVbr+93HFr4N5770Wr1fL111/zxRdf8Pe//93Wnrt+/Xpuv/12/va3vxEdHU2bNm04dOhQlY/dsWNHkpKSOHPmjG3dhYMMbdiwgfDwcJ577jl69uxJREQEJ0+etP+oBgNm85Xv2ujYsSO7d+8mL6+ibXv9+vVotVoiIyOrHPOVeHh4EBwcfNHUgOvXr7e7g8XDw4P77ruPuXPn8s033/D999+TkZEBgIuLCyNHjuS9995j9erVbNy4kfj42vnxdClSXbsUjQZ6jAVzCZh8iQrRcSqzgD3JmcS29XV0dEKIJsrNzY377ruP6dOnk52dzbhx42zbIiIiWLRoERs2bMDb25u33nqLs2fPVvn2yMGDB9O+fXvGjh3L66+/TnZ2Ns8995zdPhERESQmJrJw4UJ69erFr7/+yuLFi+32adWqFcePH2fXrl2EhITg7u5+0e1AY8aMYebMmYwdO5ZZs2aRlpbGY489xv33329rv60NTz31FDNnzqRt27Z069aNefPmsWvXLr766isA3nrrLYKCgoiJiUGr1fLdd98RGBiIl5cX8+fPx2w206dPH0wmE19++SUuLi527by1TWq4lzNoBgx9CXzb2gbAkI5TQoi6Nn78eM6fP8+wYcPs2luff/55unfvzrBhwxg4cCCBgYGMGjWqysfVarUsXryYgoICevfuzT/+8Q9efvllu31uu+02nnjiCSZNmkS3bt3YsGEDL7zwgt0+d911F8OHD+fGG2/E39//krcmmUwmli1bRkZGBr169eLuu+9m0KBBfPDBB9UrjKuYPHkyU6dO5cknn6Rr164sXbqUn376iYiICMDa4/q1116jZ8+e9OrVixMnTrBkyRK0Wi1eXl7MnTuXvn37EhUVxYoVK/j555/x9a27SpVGKaXq7Oh1LDk5mdDQUJKSkggJCamz82w4ks5fP91MSy8X1j97U52dRwhx7QoLCzl+/DitW7fG2dnZ0eGIJuJKf1dVzUVSw72Swmw4spKu3tb7gk9lFnAu99L3CAshhBBXIgn3Sr6+F768E/fE1bTxt3aCkNuDhBBC1IQk3CsJLbsJOmkT3aQdVwghxDWQhHsloWUTGSRWTGQgNVwhhBA1IQn3SmwTGSQQ42/tW7YnOZNG3M9MCCGEg0jCvRJXX/BrD0DHkgPotRrSc4s5nVXo4MCEEFdTmyMWCVEbf08y8MXVhPaB9EMYTm8hMnAw+05nszspk5ZeLld/rxCi3hkMBrRaLadPn8bf3x+DwWAbrUmI6lJKUVxcTFpaGlqtFoPBUONjScK9mrDrYOd/yiakv9uacJMzublrkKMjE0JcglarpXXr1pw5c4bTp2swdrIQl2AymQgLC0OrrfmFYUm4V1PecerUDmI6urAA2JMkHaeEaMgMBgNhYWGUlpZeddxfIa5Gp9Oh1+uv+UqJJNyr8W0LJj/IT6eXs3Xi4r2nsrBYFFqtXKYSoqHSaDQ4OTnV2cwvQlSXdJq6Go3G1ls5LG8Pzk5acopKOZaed5U3CiGEEBUk4VZFmDXh6pK30CW4/H7cTAcGJIQQorGRhFsVtgEwNhHVUgbAEEIIUX2ScKsiuBu4+kNQFD2CdIAM8SiEEKJ6pNNUVeiNMO0waDR0Ss8DjrLvdDbFpRYMevnNIoQQ4uokW1RVWXfwVr4mPJz1FJdaOHQ2x8FBCSGEaCwk4VaTJj+DKJk5SAghRDVJwq2qkgJ4txu83obegdZVMgCGEEKIqpKEW1VOLqDVAxpiXc8AUsMVQghRddJpqjru/QI8ggktdoZlKzmcmktBsRkXg87RkQkhhGjgpIZbHQGdwMWLQE9nWrgbMVsU+07LZWUhhBBXJwm3hio6TknCFUIIcXWScKtr3Vvw2TCGelonMtidlOnYeIQQQjQKknCrK3krJG2ih+YgIGMqCyGEqBpJuNVVNnNQaM4eAE6cyycrv8SREQkhhGgEJOFWV5h1IgPD6S2EebsAsOdUpgMDEkII0RhIwq2uoG6gM0B+OoMDrXPiysxBQgghrkYSbnU5OUNwdwBucD4KSMcpIYQQVycJtybKJqTvWLofkBquEEKIq5OEWxNlE9L7ZexEq4GU7ELOZhc6OCghhBANmSTcmijrqaw9d4juftZVcllZCCHElUjCrQlXX/CNAOBm70RALisLIYS4Mkm4NVXWjttbdxiQmYOEEEJcmUMTblxcHL169cLd3Z0WLVowatQoEhISHBlS1ZW147YqiAcg/lQWSilHRiSEEKIBc2jCXbNmDRMnTmTTpk0sX76ckpIShg4dSl5eniPDqpqyATBc0/fgqrOQmV9CYka+g4MSQgjRUDl0PtylS5favZ4/fz4tWrRg+/btDBgwwEFRVZFvOxj2LzQte9D+p2J2JuewOzmLcF9XR0cmhBCiAapRDTcpKYnk5GTb6y1btjBlyhQ++eSTawomK8va8cjHx+eS24uKisjOzrYtOTk513S+a6LRQOxECLuOrqHWePdIT2UhhBCXUaOE+9e//pVVq1YBkJKSwpAhQ9iyZQvPPfccs2fPrlEgFouFKVOm0LdvX7p06XLJfeLi4vD09LQtnTp1qtG5alv53LjSU1kIIcTl1Cjh7t27l969ewPw7bff0qVLFzZs2MBXX33F/PnzaxTIxIkT2bt3LwsXLrzsPtOnTycrK8u27N+/v0bnqjUlhbDnOwYlvgco4k9lUWq2ODYmIYQQDVKN2nBLSkowGo0ArFixgttuuw2ADh06cObMmWofb9KkSfzyyy+sXbuWkJCQy+5nNBpt5wXIzs6u9rlq3X8fxdtcTEdDRw4Ut+BIWi4dAj0cHZUQQogGpkY13M6dO/PRRx+xbt06li9fzvDhwwE4ffo0vr6+VT6OUopJkyaxePFi/vjjD1q3bl2TcBzHyRmiR8N1j9I2wAuAPUlyWVkIIcTFapRwX331VT7++GMGDhzI6NGjiY6OBuCnn36yXWquiokTJ/Lll1/y9ddf4+7uTkpKCikpKRQUFNQkLMe47T0YHkfL1h0AGQBDCCHEpdXokvLAgQNJT08nOzsbb29v2/qHH34Yk8lU5ePMmTPHdrzK5s2bx7hx42oSmsNIxykhhBBXUqOEW1BQgFLKlmxPnjzJ4sWL6dixI8OGDavycZrMyEzFefRSe3Ejn4MpGopKzRj1OkdHJYQQogGp0SXl22+/nS+++AKAzMxM+vTpw5tvvsmoUaNstdZmZe4gWiy+m0GmI5SYFQfOOPD+YCGEEA1SjRLujh076N+/PwCLFi0iICCAkydP8sUXX/Dee+/VaoCNQkgPAIa4nQBgj7TjCiGEuECNEm5+fj7u7u4A/P7779x5551otVquu+46Tp48WasBNgplExl0wzrxwi4ZcUoIIcQFapRw27Vrx48//khSUhLLli1j6NChAKSmpuLh0QzvQS2byCAobz9OlErHKSGEEBepUcKdMWMG06ZNo1WrVvTu3ZvY2FjAWtuNiYmp1QAbBd92YPJFZy6ii+Y4R9NyyS0qdXRUQgghGpAaJdy7776bxMREtm3bxrJly2zrBw0axNtvv11rwTUaGg2EWiekH+R6DKUgXmq5QgghKqnxfLiBgYHExMRw+vRp28xBvXv3pkOHDrUWXKNSlnD7GY8A0nFKCCGEvRolXIvFwuzZs/H09CQ8PJzw8HC8vLx46aWXsFia6eD9Ze24kcX7ASXtuEIIIezUaOCL5557js8++4xXXnmFvn37AvDnn38ya9YsCgsLefnll2s1yEYhqBvoDLiUnKeVJoXdyVUfcUsIIUTTV6OE+3//9398+umntlmCAKKiomjZsiWPPvpo80y4Ts4QHANJm+mpPcSi80Gcyy3C18149fcKIYRo8mp0STkjI+OSbbUdOnQgIyPjmoNqtMracW80HQNkXGUhhBAVapRwo6Oj+eCDDy5a/8EHHxAVFXXNQTVaZe24PTSHAJk5SAghRIUaXVJ+7bXXuOWWW1ixYoXtHtyNGzeSlJTEkiVLajXARiW0Dzi5YnELQJ8tA2AIIYSoUKMa7g033MChQ4e44447yMzMJDMzkzvvvJN9+/bxn//8p7ZjbDxc/eDZRFJGfUspevYkZzadGZGEEEJckxrVcAGCg4Mv6hy1e/duPvvsMz755JNrDqzR0unpFOSBXqshPbeY01mFtPRycXRUQgghHKzGA1+Iy3N20tEtwDof7h6ZyEAIIQSScGtfXjq835OvM+/HiVJ2SzuuEEIIJOHWPpMvFGRgUEW01yTLEI9CCCGAarbh3nnnnVfcnpmZeS2xNA0aDfz1OxKK/dj3yV7ck7OwWBRarcbRkQkhhHCgaiVcT0/Pq25/4IEHrimgJiGkB23NFpyd9pNTVMqx9DzatXBzdFRCCCEcqFoJd968eXUVR5Oj12npHOzJ9pPn2ZOcKQlXCCGaOWnDrStrXueN/OcJ05yVATCEEEJIwq0zR5bTOmc7vTQJMsSjEEIISbh1pmwigx7aBPafzqbE3EznCRZCCAFIwq07ZRMZ9NYfpqjUQkJKjoMDEkII4UiScOtKWQ23Hcl4kivtuEII0cxJwq0rrn7g2w6A7trD7JYhHoUQolmThFuXQq2XlXtqpeOUEEI0d5Jw61JYecI9xOHUXAqKzQ4OSAghhKNIwq1LZQk3WnsMraWEfaelHVcIIZorSbh1ybcdmHxxppjOmhMyc5AQQjRjknDrkkZjdz+uzBwkhBDNlyTculaWcHtqD8mtQUII0YxJwq1rlTpOHU/PJSu/xMEBCSGEcARJuHUtqBvc9DwvOU9Dg2LPqUxHRySEEMIBJOHWNSdnGPAUlvB+KLRyWVkIIZophybctWvXMnLkSIKDg9FoNPz444+ODKdORYd4AciIU0II0Uw5NOHm5eURHR3Nhx9+6Mgw6l5xHgPN63lU96PUcIUQopnSO/LkI0aMYMSIEY4MoX6UFBCxZhJPO8FX2YNJzS6khYezo6MSQghRj6QNtz64+kHkLfxkuAUDJTIAhhBCNEONKuEWFRWRnZ1tW3JyGtEcs6O/Zl3EM6ThLQNgCCFEM9SoEm5cXByenp62pVOnTo4OqVqiQr0ApIYrhBDNUKNKuNOnTycrK8u27N+/39EhVUu3QCM9NQfZn5SGUsrR4QghhKhHjSrhGo1GPDw8bIu7u7ujQ6o6pejy/UAWGWcTUniExIx8R0ckhBCiHjm0l3Jubi5HjhyxvT5+/Di7du3Cx8eHsLAwB0ZWBzQaNEHdIOcMPbQJ7E7OItzX1dFRCSGEqCcOreFu27aNmJgYYmJiAJg6dSoxMTHMmDHDkWHVnbBKExnIABhCCNGsOLSGO3DgwObVlhlaMZHBPEm4QgjRrDSqNtxGLzgGi9aAvyaLrDOHMFua0Y8NIYRo5iTh1icnZzTB3QDoXHqAI6m5jo1HCCFEvZGEW880ldpxd8sAGEII0WxIwq1vtnbcBJk5SAghmhFJuPUt1FrDba89xbHEZAcHI4QQor5Iwq1vbv6UeLWxPk3bTlGp2cEBCSGEqA+ScB1A38p6WbkbCRw404gmYBBCCFFjknAdQFPpflyZOUgIIZoHSbiOEHYdpRoDhcrA7sRMR0cjhBCiHkjCdQS/9qy9awdjS55lzymZqk8IIZoDSbiOoNHQNbwFAEfScsktKnVwQEIIIeqaJFwH8Xc3EuzpjJMqYa/UcoUQosmThOsoGcf5Vk1jrXEKuxPPOzoaIYQQdUwSrqO4BxFUkkSg5jzJJw45OhohhBB1zKHT8zVrTs4cGDSPcb/k4HzW2dHRCCGEqGNSw3WgsJ7DSceT5PMFnMstcnQ4Qoj6ohTsWwxzb4I//hdKix0dkagHknAdyMPZiTb+rgBye5AQzcW5o/DlnfDdODi1Hda+Dp8OgtSDjo5M1DFJuI5ksTDN+F++cIoj4Viio6MRQtSlkgJY9S/493Vw9A/QGaDn38HFB1L2wMcDYNMcsFgcHamoI5JwHUmrpW/+Cgbo4ik6vtHR0Qgh6sqh362Jds2rYC6GtoPg0U1w69vw6EZoNwTMRbD0WfjPKCiUK15NkSRcBysJ7g2Ae9oOlFIOjkYIUessFvhjNpw/Ae7BcM//wd++B9+21u3ugTDmO7jlLXAyWdcZ3B0Wrqg70kvZwTzb94Mji+hs3s/prEJaerk4OiQhxLUqLQYU6I2g1cItb8P+H2Hgs2C8RDLVaKDXeGh9Azi5WN8DUJxnrRG7eNdn9KKOSA3XwZxaXw9AtOYo8SdTHReI1K6FqB2Jm+Dj/vDn2xXrQnvBsJcvnWwr82sHni0rXv/+PPz7eji+rm5iFfVKariO5htBvs4Dkzmbs4e2QnR47RxXKSjKgby0iiU3FfLSISgaIodb9zt/0vrlUJQD45dDSM/aOb8QzVX2aUg7CDv+A32ngFMN77MvyoHjayHnNChzrYYoHEMSrqNptZz3jcGUugbLyY3kFY3C1XiZfxaL2dqZwuRTsW7v95ASX5ZQ0+wTbGnhpY/TY1xFwnXxsh4zqBu07FGxT9Yp+1/aQohLs5itt/r4t7e+7nyH9cdt9H01T7ZgrQ3/z1o4sgLaDKxYX5gNzh7XFLJwDEm4DYBTq1hIXUOn7D/535emE+VTShfPQnw7DiAo9i9oNBprh4v3YkDvAs+drnjznm/h0NLLH9zgBq5+4NoCXP3BzR/C+1VsN3rAo5vBrYW1HQms/6HnxEJAF7jhaWu7Uvk2IUSF5O3w6xPWH6iTtlp/DGs0cN2E2jm+wRU63V7xOuM4zL0RYidBvydAq6ud84h6IQm3AfDvNAC2vEIf7UH6aA9CNpANC46f4f01Lbgh0p+bWpsYoixQkg/F+WAo683Yfhh4t7ogqZY9uvpZ/8NeiUYDLTrYr0vaYr1n8OR6+OJ2CO1jTbxtB0niFQIgPwNWzobt8wEFRk9I3Q+t+l3tnddmzzdQcB7+eAkO/w53fAw+rev2nKLWaFQjvhclOTmZ0NBQkpKSCAkJcXQ4NWcuhR8eQmUcpcDJl1MlrhzKdeHXzFCWlJRf5lUEabNoFRbOgA5BDIz0p0Ogu7X2WxeyTsH6d61fKOayYSdb9oAbnoGIoZJ4RfNkscDuBbD8Bcg/Z10XPRqGzLb+0K1rSsHuhbDkKSjOsV7BGh4HMffL/0kHqmoukoTbgBWWmNl47BxrEtJYcyiN4+l5dtsDPZy5ob0/N0T60y/CDw9np9oPIicF1r8H2z6H0gLruqBoGPA0RN5ccfuCEE3d2X3w65OQWDZIjX8HuOXNuq/VXsr5k/DjI9arUGD9vzjyPWuTkah3knCboJPn8lidkMbqhFQ2HjtHYUnFEHA6rYYeYd7cEOnPDe396RzsUbu139xU2PA+bP0MSsoSf0AXGPAUdLxNEq9ouopyYPUr1mEXlRmcXK330173COjq4EduVVnMsPFD6+VlczGY/OC296HDzY6LqZmShNvEFZaY2XI8w5qAD6VyLM2+9uvvbuSG9v4MjPSnfzt/PE219MWQdw42fQibP7Fe0gLw7wh/+api5BwhmgKlrINVLJ0OOWes6zreZr2E69mAvm9S9sIPD0PqPuvr7g/AsH9d/Z5fUWsk4TYzSRn5rD6UxpqEVDYcPUd+ccV9e1oNxIR5M7C9PwMjW9A52AOt9hprv/kZsPkj2PSRdWScx3dfdAtEYYmZjLxiMvKKOZ9fzPn8Es5Xem1bn1dCqcVCr1Y+DIxswfVtfS9/a1QTk5FXzLYTGbgZ9XQJ8aybZgFRMz88bO2kBODdGm5+AyIGOzamyykphFX/Cxs+AJS1I+Udn0BYH0dH1ixIwm3GikrNbDtxntUJqaxOSONwaq7ddj83AwMirG2/AyL88XY1XPWYhSVmMvNLLkqWeVkZ6M8fIZ4IzucXk5lbwLNZs/m1tBffFl9PaQ06wjvpNGXJ158b2regfYBb3XUOq2eFJdZ/m3VH0lh/JJ19p7PtBvlq4+9KdIgXUSGeRIV40TnYA2cnufXDIXZ9DT9Pgf5Tr20Ai/p0fJ21bTcrCTQ6mLjFOnqVqFOScIVN8vl81h5KZ3VCKuuPpJN3Qe03OtSL/u380Ou0ZOQVk5lfTMYFtdHKNeYruVW7kQ8M75OpXOlX9C5FOle8TQZ8XA22Ry+Tk91rb1cDxaUW1h1OY3VCGokZ+XbHDPZ0trVN923nh3sjqgVaLIr9Z7L580g6fx5OZ+uJDIpK7adfax/gRl6RmVOZBRe9X6fV0D7AneiyBBwV4klkoDtOOmkzr3WHllkf2w+zPiplTVxeYY6LqSYKs+C3Z6zjOI9819HRNAuScMUlFZda2HYygzWH0liTkMbBlJwqv1ev1eBlMuDj6mSXLH1MFUnUz1BCu8RvcXFxRR/7P7gZ9WgA4hdBp9usXwJXoJTixLl8VieksuZQGhuPnrNLUHqthu7h3mW1X386BdVy57BakHw+nz8Pp/PnkXQ2HD1HRl6x3fYADyP92vnTP8KP69v50sLdWnM6l1vEnlNZ7EnKYk9yJruTs0jPLbro+Aa9lk5BHrYkHB3qSRs/t2tvJmjO4hfB9+PBLdA6gEVTGMnJYq4YGCPrFCQsgZ7jpYNjHZCEK6rkTFYBaxLS2HIiA6Nea1cb9b4gsbob9TVLboeWwdf3Wqcm6/s49BhrbfetgsISM5uOnbP9QDh2wa1RLco6h91Q253DqiGroISNR8/x55E01h85d9HtW64GHbFtfenbzo/+EX609a/aJXKlFGeyCtmTbE3A5Y/ZhaUX7etm1NOlpQfRIV50DfEkOsSLEG8X63mUsg6WcP4EZJ603lLS80Fw9rS+efkMOPAzRN1n7X1rPbl1ABSvMHALaBpf0uXlkH3KmoAKs6zDL4K1DfTjAdba7cBnrz5gTGNiscCXd8KxVXDdRBj+L0dH1ORIwhUNx/6frBNrZ5+yvnZtYU28PR+s9hdb4rl81hyytk1vOHqOghL7y+Pdw7zLemfXUuewSygutbAj8bytFrsnORNLpf9FOq2GbqFe9CtLsNGhXrV2Cbj8CkDlBLz3VDaqJJ9QTRqhmlTbYxv9Odo5pRNgOYvRbP8jgIdWQcvu1ucb/w3Lptt/GeemwRtlbX86A3iGWpOvbQmveN5QEnJhdkUyzU4uezxVad0p60ht5XRGeP5sxYARpcWgv3p/hkZHKdj2Gaz6F/x9GfhF1OOpFefyiknKyCf5fEHZkk9S2WOJ2UIrX1fa+LnS2s+V1v5utPFzJdjLBd21/t+tPCIfwMkN1nupQ3pBcLdrO/YFJOGKhqW0CHZ9BevehqxE6zqTH1w/CXr9o0a3MBSVmtl6/LwtAV+pc1j/CH98qtA57FKUUiSczbEl2M3HMuwSPUBbf1f6R1jbmK9r41M37cx56dYvDKN7RbLMTUPNiUWTl3bVt6cqL1K0ARS4hnCsw8OERPYgqqUXnpo8OLvXOhyof6R157QE+PJua+JSlisfWGcEr7KEfMub4NPGFhvKYj9Od21IWApndkHkCOsgLAD7/wvfPlC195t8waOl9daeuz5tWrXZKynOs/+se3+Atjde01y7SinO55dYk2iGNYkmny8g6Xy+LblWHi+gqgx6La18TdYk7OdGO28n2puyCXMpwFPloCk4b71TIv8cFGRYn5c/lj8vLYTnUiqupi1+BHZ/DYNmWjvC1aKq5qIGce/Fhx9+yOuvv05KSgrR0dG8//779O7d29FhidqkN0LPv1uHoNu9ENa9Yb3EuWKWdQjJnn8Hj2BrbUpngA63VCThc0etU555hVpvdwAwl2DMTqJfCwP9gvx47qYgTmWXsu5YNn8cymD90XOk5xbzw85T/LDzFBoNRId42e5NjgrxuuIv6JSswrKOTmn8eeTcRW2pfm4G+rbzo187P/q28yPYq2qXyC9LKevgIuWXfDNPWMun7xMVvUx3L7DOj9r5TrhnnnWdyQdNQab1udETvK21z1LPMFK0gSQU+bA924O1qS7sTyux1sQLgD/N8OcWAMJ8TAR6OuPnlouPazy+rkZ83Yz4DvodXxcNAWTgU5KCe+FptFlJkJlYtpy01hrNRXDuiHXRVyqHDe9Zl8o154JM66hl5bVk73Drv3P26UvXTrNOWWe+ejKhoha96ys48JM1UZQnXPcg66OzV1kybVnpMaTitUdwlZszmpzKyfbkRlj0d+t4696tQau3tvdq9ZUWHUqrp0RpySuB3FItS9s8Z0uqHc7+jHf+cX4u7sluZf0bDdGkMUK7mTboCEOLGR2lOh1uLs54urrg5eaCt5sL3u6u+Hi4oFdmMs+dZaXzEI6n53M8PY8R57+iv2Ynn6eN4LezfYCz9NXG85UhrvqfOT+jYtazkB7WsQPKfxA6gMMT7jfffMPUqVP56KOP6NOnD++88w7Dhg0jISGBFi3qYWxSUb90TtD9fuv4s/HfwdrXIeMorHvTfr8p8RUJd9vnsPED62XoIbOt67JPWWdPqqQl8JeyRemdsBicKFZ6CixaCi1aSs7q+Z/kqby7MgxvkxNTAnZzc/EyXDrfDH0ns/nYOTYfTGTY3icpKC4lCLgXxX2A3qjwcNbj6azHw0WPi5MWTa4FdirrMuK1irmE9/8Ea1+DVv2tgySUm9PXOiKQUoCqeLSYrcm29OJeykQMrUi4vu3ANwLcAyu2a3XWKdw8guxqKnogpGwZBDwN5BWVsu90tq1D1p7kTE6eyycxI/+inuGXotV4420KwNftenxcDfgGGPFvrSVcn0lLTRqBlrMUpOnxLczF19WAV1EOGo3WfpCIjKOw8sWrnusi+ekVYxW3vck6raRf+4rtwTEw/RQY3ap/7DpgsShyi0vJLSwlt6iUnMIScsqe5xaWklNYSk5RKXlFpWgAvU6Lk06DXqvFSa/BSatFr9Og12kxlK3X6zQYdFr0OutzJ23Zeyq916C/8r46rcbarq83WhNPxlHrD5rL0ACGssWk9Lx88E7btr84rWOIbgeHtQGkuHYmxNvEEKdkJiR/ffGBSoGssuUSev/zH7YfBJYfv0G76xC+vW6jT4tOHEvPw3I6k/yzzpxXbhUL7pxXbmSWPWYodzB54+bVAi+/QFq0CCb0lI7WxbmE+phw6vUP69U0B3J4wn3rrbd46KGHePDBBwH46KOP+PXXX/n888959tlnHRydqDM6PXQbDVH3Wi9tHf7degnIXGJNSk6Vfo27+oFfpLWtsJzFDAZ3677mi3vyaiwl6CwluAAuYP3m0MCACB9OJeo5n19CauIh/J22suAPN15Y0YFSi8KdfP7pvBsudetrUdlyqS+NwsyK5wUZ1jmKPUPt90lLAEvJFQpFY01O5TU/r3D7pBI5wrpcKKDTFY5ZwdWop3drH3q3rphPOTO/mANnckjPLSIjr5hzuUWcyyvmXK71lrD0vKKyW8WsteNzecWcu6DXtZUJaA3rt9jW6LRD8XMZTsAmHW57N+HrZqSD9jQ3tbgF35IzuBeewbkgBQ0Ki94Fs1swZveWmN2DUe4tsXgEVdROte7oikrRaTVou41F130cWo31n9V6MqdaGWbRYlHkl5jLkmIJOZUSZG6RNWlWTpy5RaVkF5bYJdLcIuvSUJUnZzfdi8SoA6iSAnRY0GNGhxk9FnSaskfMOGHG06jB0+TEbR2CCfF2IcTbRFjGPWQV9WJ2t7swtiobYCPFHTb8BSyllRZz2WNJpedli1YPLj7WJqeyhKvt8SBEDKVNYFfa+JbNhKQ6g2YM2hIzuefyyUvPJTk9j+NpeRxLz+N4ep71boBcrEtyKZBYtlj7VYT5lF+iti5t/Fxp7e9KoIdzvd3p4NA23OLiYkwmE4sWLWLUqFG29WPHjiUzM5P//ve/V3y/tOEKwFpTtJjLkm+xNWlbSiqeV14f0JkSnQu7kjLZs3MzqUd2sinDld2qHeG+Jvq39eIe41baBbjjaiz7Atdoy9ogNZUeL1jXsmfFwPFZpyDtgLVNtPySJ8CJP8ueXOI4Jl9rgm6gnXZKzBbO51dKxLYEXVyWoK2JunxbziV6Ul+KE6W4UEQ2JiqlzyrTaqxfplqNtfam02jQajWV1mG3zvbcts7aCS63rMaZW1RKbX4jOuk0uDs74WbUWxdnPR7OFc/LR1QrNStKzBZKzIpSs4VSi6LYbLE+N5c/V5RarPuUlL0usVjs32uxUFJqocRiPY6lip/Fz81AS28TId4uhJY9hni7EOpjoqWXS6MYfCUzv5jjZcn3eHpZIk6zPr+wz0U5N6Oe+FlDrznhNoo23PT0dMxmMwEBAXbrAwICOHjw4EX7FxUVUVRUUZvJyan6PaSiCdNorDVmnR5rTevKnIBerXzo1WoEMIK0nCJKzJZK7bAxV3h3FXi2rGg3qswRs8rUEiedlhbuzrZ7hq+muNRiTch5RZdP0mW15+yCEjwsCosCs0VhVgpL2ePVkp9FgcVcdmm+Fum1Gtyc9bg763EzOuFuLHvuXDlxViTS8m3uRqdK79Nj1Gsdep+42VKWnMsS8IXJGiDI0xmTweEXO6+Zl8lATJiBmDD7TmBKKVKyC+1qw+WLp4tTvf77NKpSjouL48UXa9D+I8QV+LtfeTAOUX0GvZZAT2cCPa9tOESlVKUkDOay15YLErN1XaXt5e+r9Nz6iP12pTDqtLZE6u7shLuz4xNlbdFpNei0Db92Wpc0Gg1Bni4EebpwfTs/u22Wql4CqCUOTbh+fn7odDrOnj1rt/7s2bMEBgZetP/06dOZOrWiO/epU6fo1Klq7VdCiMZHo9FYOw85OhDRJNX36GwOvVvdYDDQo0cPVq5caVtnsVhYuXIlsbGxF+1vNBrx8PCwLe7uMv2UEEKIxsHhPxynTp3K2LFj6dmzJ7179+add94hLy/P1mtZCCGEaAocnnDvu+8+0tLSmDFjBikpKXTr1o2lS5de1JFKCCGEaMwcnnABJk2axKRJkxwdhhBCCFFnGsCI40IIIUTT1yBquDVlKbuP7MyZMw6ORAghRHNVnoPKc9LlNOqEW347kUx0IIQQwtHOnj1LWFjYZbc36un5SktL2blzJwEBAWivcT7OnJwcOnXqxP79++V2oyqQ8qoeKa/qkzKrHimv6qnN8rJYLJw9e5aYmBj0+svXYxt1wq1N2dnZeHp6kpWVhYeHh6PDafCkvKpHyqv6pMyqR8qrehxRXtJpSgghhKgHknCFEEKIeiAJt4zRaGTmzJkYjTKQfVVIeVWPlFf1SZlVj5RX9TiivKQNVwghhKgHUsMVQggh6oEkXCGEEKIeSMIVQggh6oEk3DIffvghrVq1wtnZmT59+rBlyxZHh9QgrV27lpEjRxIcHIxGo+HHH390dEgNWlxcHL169cLd3Z0WLVowatQoEhISHB1WgzVnzhyioqJsc17Hxsby22+/OTqsRuOVV15Bo9EwZcoUR4fSYM2aNQuNRmO3dOjQoV7OLQkX+Oabb5g6dSozZ85kx44dREdHM2zYMFJTUx0dWoOTl5dHdHQ0H374oaNDaRTWrFnDxIkT2bRpE8uXL6ekpIShQ4eSl5fn6NAapJCQEF555RW2b9/Otm3buOmmm7j99tvZt2+fo0Nr8LZu3crHH39MVFSUo0Np8Dp37syZM2dsy59//lk/J1ZC9e7dW02cONH22mw2q+DgYBUXF+fAqBo+QC1evNjRYTQqqampClBr1qxxdCiNhre3t/r0008dHUaDlpOToyIiItTy5cvVDTfcoB5//HFHh9RgzZw5U0VHRzvk3M2+hltcXMz27dsZPHiwbZ1Wq2Xw4MFs3LjRgZGJpigrKwsAHx8fB0fS8JnNZhYuXEheXh6xsbGODqdBmzhxIrfccovd95i4vMOHDxMcHEybNm0YM2YMiYmJ9XLeRj1bUG1IT0/HbDYTEBBgtz4gIICDBw86KCrRFFksFqZMmULfvn3p0qWLo8NpsOLj44mNjaWwsBA3NzcWL15Mp06dHB1Wg7Vw4UJ27NjB1q1bHR1Ko9CnTx/mz59PZGQkZ86c4cUXX6R///7s3bu3zid9aPYJV4j6MnHiRPbu3Vt/7UWNVGRkJLt27SIrK4tFixYxduxY1qxZI0n3EpKSknj88cdZvnw5zs7Ojg6nURgxYoTteVRUFH369CE8PJxvv/2W8ePH1+m5m33C9fPzQ6fT2ebWLXf27FkCAwMdFJVoaiZNmsQvv/zC2rVrCQkJcXQ4DZrBYKBdu3YA9OjRg61bt/Luu+/y8ccfOziyhmf79u2kpqbSvXt32zqz2czatWv54IMPKCoqQqfTOTDChs/Ly4v27dtz5MiROj9Xs2/DNRgM9OjRg5UrV9rWWSwWVq5cKe1G4poppZg0aRKLFy/mjz/+oHXr1o4OqdGxWCwUFRU5OowGadCgQcTHx7Nr1y7b0rNnT8aMGcOuXbsk2VZBbm4uR48eJSgoqM7P1exruABTp05l7Nix9OzZk969e/POO++Ql5fHgw8+6OjQGpzc3Fy7X4LHjx9n165d+Pj4EBYW5sDIGqaJEyfy9ddf89///hd3d3dSUlIA8PT0xMXFxcHRNTzTp09nxIgRhIWFkZOTw9dff83q1atZtmyZo0NrkNzd3S/qD+Dq6oqvr6/0E7iMadOmMXLkSMLDwzl9+jQzZ85Ep9MxevToOj+3JFzgvvvuIy0tjRkzZpCSkkK3bt1YunTpRR2pBGzbto0bb7zR9nrq1KkAjB07lvnz5zsoqoZrzpw5AAwcONBu/bx58xg3blz9B9TApaam8sADD3DmzBk8PT2Jiopi2bJlDBkyxNGhiSYiOTmZ0aNHc+7cOfz9/enXrx+bNm3C39+/zs8tswUJIYQQ9aDZt+EKIYQQ9UESrhBCCFEPJOEKIYQQ9UASrhBCCFEPJOEKIYQQ9UASrhBCCFEPJOEKIYQQ9UASrhBCCFEPJOEKIapEo9Hw448/OjoMIRotSbhCNALjxo1Do9FctAwfPtzRoQkhqkjGUhaikRg+fDjz5s2zW2c0Gh0UjRCiuqSGK0QjYTQaCQwMtFu8vb0B6+XeOXPmMGLECFxcXGjTpg2LFi2ye398fDw33XQTLi4u+Pr68vDDD5Obm2u3z+eff07nzp0xGo0EBQUxadIku+3p6enccccdmEwmIiIi+Omnn2zbzp8/z5gxY/D398fFxYWIiIiLfiAI0ZxJwhWiiXjhhRe466672L17N2PGjOEvf/kLBw4cACAvL49hw4bh7e3N1q1b+e6771ixYoVdQp0zZw4TJ07k4YcfJj4+np9++sk2EXy5F198kXvvvZc9e/Zw8803M2bMGDIyMmzn379/P7/99hsHDhxgzpw5+Pn51V8BCNHQKSFEgzd27Fil0+mUq6ur3fLyyy8rpZQC1IQJE+ze06dPH/XII48opZT65JNPlLe3t8rNzbVt//XXX5VWq1UpKSlKKaWCg4PVc889d9kYAPX888/bXufm5ipA/fbbb0oppUaOHKkefPDB2vnAQjRB0oYrRCNx44032ubXLefj42N7Hhsba7ctNjaWXbt2AXDgwAGio6NxdXW1be/bty8Wi4WEhAQ0Gg2nT59m0KBBV4whKirK9tzV1RUPDw9SU1MBeOSRR7jrrrvYsWMHQ4cOZdSoUVx//fU1+qxCNEWScIVoJFxdXS+6xFtbXFxcqrSfk5OT3WuNRoPFYgFgxIgRnDx5kiVLlrB8+XIGDRrExIkTeeONN2o9XiEaI2nDFaKJ2LRp00WvO3bsCEDHjh3ZvXs3eXl5tu3r169Hq9USGRmJu7s7rVq1YuXKldcUg7+/P2PHjuXLL7/knXfe4ZNPPrmm4wnRlEgNV4hGoqioiJSUFLt1er3e1jHpu+++o2fPnvTr14+vvvqKLVu28NlnnwEwZswYZs6cydixY5k1axZpaWk89thj3H///QQEBAAwa9YsJkyYQIsWLRgxYgQ5OTmsX7+exx57rErxzZgxgx49etC5c2eKior45ZdfbAlfCCEJV4hGY+nSpQQFBdmti4yM5ODBg4C1B/HChQt59NFHCQoKYsGCBXTq1AkAk8nEsmXLePzxx+nVqxcmk4m77rqLt956y3assWPHUlhYyNtvv820adPw8/Pj7rvvrnJ8BoOB6dOnc+LECVxcXOjfvz8LFy6shU8uRNOgUUopRwchhLg2Go2GxYsXM2rUKEeHIoS4DGnDFUIIIeqBJFwhhBCiHkgbrhBNgLQMCdHwSQ1XCCGEqAeScIUQQoh6IAlXCCGEqAeScIUQQoh6IAlXCCGEqAeScIUQQoh6IAlXCCGEqAeScIUQQoh6IAlXCCGEqAf/D0JNXWTXcOqGAAAAAElFTkSuQmCC",
      "text/plain": [
       "<Figure size 500x300 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from previous_chapters2 import plot_values\n",
    "\n",
    "epochs_tensor = torch.linspace(0, num_epochs, len(train_losses))\n",
    "examples_seen_tensor = torch.linspace(0, examples_seen, len(train_losses))\n",
    "\n",
    "plot_values(epochs_tensor, examples_seen_tensor, train_losses, val_losses, label=\"loss\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training accuracy: 98.56%\n",
      "Validation accuracy: 97.99%\n",
      "Test accuracy: 95.00%\n"
     ]
    }
   ],
   "source": [
    "train_accuracy = calc_accuracy_loader(train_loader, model, device)\n",
    "val_accuracy = calc_accuracy_loader(val_loader, model, device)\n",
    "test_accuracy = calc_accuracy_loader(test_loader, model, device)\n",
    "\n",
    "print(f\"Training accuracy: {train_accuracy*100:.2f}%\")\n",
    "print(f\"Validation accuracy: {val_accuracy*100:.2f}%\")\n",
    "print(f\"Test accuracy: {test_accuracy*100:.2f}%\")"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "b60f5bfde2dc6003c52249e7ba87cb8994d8effec2917dd9467812f9cc41ae70"
  },
  "kernelspec": {
   "display_name": "Python 3.9.18 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
