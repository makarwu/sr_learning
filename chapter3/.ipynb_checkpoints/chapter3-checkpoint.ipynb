{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "76c94037-e63a-4584-8700-782212fbc50a",
   "metadata": {},
   "source": [
    "### Attention Mechanisms - The Engine of LLMs\n",
    "- Through an attention mechanism, the text-generating decoder segment of the network is capable of selectively accessing all input tokens, implying that certain input tokens hold more significance than others in the generation of a specific output token:\n",
    "- Self-attention in transformers is a technique designed to enhance input representations by enabling each position in a sequence to engage with and determine the relevance of every other position within the same sequence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cd2ea013-fd0c-42fe-b5fd-fb76cbfc1ff7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "inputs = torch.tensor(\n",
    "  [[0.43, 0.15, 0.89], # Your     (x^1)\n",
    "   [0.55, 0.87, 0.66], # journey  (x^2)\n",
    "   [0.57, 0.85, 0.64], # starts   (x^3)\n",
    "   [0.22, 0.58, 0.33], # with     (x^4)\n",
    "   [0.77, 0.25, 0.10], # one      (x^5)\n",
    "   [0.05, 0.80, 0.55]] # step     (x^6)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04cede2a-e29c-4e1b-a2cc-139bfa4cd4ce",
   "metadata": {},
   "source": [
    "- (1) let's compute the context vector $z$, with the attention scores $w$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9ea47628-00f1-47a1-9ee4-7db5798dd6e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.9544, 1.4950, 1.4754, 0.8434, 0.7070, 1.0865])\n"
     ]
    }
   ],
   "source": [
    "query = inputs[1] # 2nd input token is the query\n",
    "\n",
    "attn_scores_2 = torch.empty(inputs.shape[0])\n",
    "for i, x_i in enumerate(inputs):\n",
    "    attn_scores_2[i] = torch.dot(x_i, query) # dot product (transpose note necessary here)\n",
    "\n",
    "print(attn_scores_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5cb4def-b56f-4bd7-948c-924c24911023",
   "metadata": {},
   "source": [
    "- Side note: a dot product is essentially a shorthand for multiplying two vectors elements-wise and summing the resulting products"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "61f044db-bffa-4495-9d40-80e3056b4df0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.9544)\n",
      "tensor(0.9544)\n"
     ]
    }
   ],
   "source": [
    "res = 0\n",
    "for idx, elements in enumerate(inputs[0]):\n",
    "    res += inputs[0][idx] * query[idx]\n",
    "\n",
    "print(res)\n",
    "print(torch.dot(inputs[0], query))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "839c623a-0b98-4e0c-ba87-58e48372e984",
   "metadata": {},
   "source": [
    "- (2) Now, we have to normalize the unnormalized attention scores $w$ so that they sum up to 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8b2191c7-3650-4321-b10a-0e6f8cb67dc2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attention weights: tensor([0.1455, 0.2278, 0.2249, 0.1285, 0.1077, 0.1656])\n",
      "Sum: tensor(1.0000)\n"
     ]
    }
   ],
   "source": [
    "attn_weights_2_tmp = attn_scores_2 / attn_scores_2.sum()\n",
    "\n",
    "print(\"Attention weights:\", attn_weights_2_tmp)\n",
    "print(\"Sum:\", attn_weights_2_tmp.sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eceddac3-f02b-433d-ae5a-c47d5d4533d7",
   "metadata": {},
   "source": [
    "- However, in practice, using the softmax function for normalization, which is better at handling extreme values and has more desirable gradient properties during training, is common and recommended."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "eb3168ac-02e1-4652-8975-e79cae5d1daf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attention weights: tensor([0.1385, 0.2379, 0.2333, 0.1240, 0.1082, 0.1581])\n",
      "Sum: tensor(1.)\n"
     ]
    }
   ],
   "source": [
    "def softmax_naive(x):\n",
    "    return torch.exp(x) / torch.exp(x).sum(dim=0)\n",
    "\n",
    "attn_weights_2_naive = softmax_naive(attn_scores_2)\n",
    "print(\"Attention weights:\", attn_weights_2_naive)\n",
    "print(\"Sum:\", attn_weights_2_naive.sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52623f83-61cc-4d8d-bea2-5511d8c981e1",
   "metadata": {},
   "source": [
    "- The naive implementation above can suffer from numerical instability issues for large or small input values due to overflow and underflow issues.\n",
    "- Hence, in practice, it's recommended to use the PyTorch implementation of softmax instead, which has been highly optimized for performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0eb7fc57-3087-4dc0-bc4a-73d163a9a490",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attention weights: tensor([0.1385, 0.2379, 0.2333, 0.1240, 0.1082, 0.1581])\n",
      "Sum: tensor(1.)\n"
     ]
    }
   ],
   "source": [
    "attn_weights_2 = torch.softmax(attn_scores_2, dim=0)\n",
    "\n",
    "print(\"Attention weights:\", attn_weights_2)\n",
    "print(\"Sum:\", attn_weights_2.sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0560dfc7-dab7-4491-99c7-e876dea4bf68",
   "metadata": {},
   "source": [
    "- (3) Compute the context vector $z^\\text{2}$ by multiplying the embedded input tokens, $x^\\text{i}$ with the attention weights and sum the resulting vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9ca468a9-ed18-4ccc-b912-39f698869208",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.4419, 0.6515, 0.5683])\n"
     ]
    }
   ],
   "source": [
    "query = inputs[1] # 2nd input token is the query\n",
    "\n",
    "context_vec_2 = torch.zeros(query.shape)\n",
    "for i, x_i in enumerate(inputs):\n",
    "    context_vec_2 += attn_weights_2[i] * x_i\n",
    "\n",
    "print(context_vec_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "093b44d7-7c8e-4f09-9e9b-f5a85f464336",
   "metadata": {},
   "source": [
    "### Computing attention weights for all input tokens\n",
    "- Now, we have to generalize the mechanism to all input sequence tokens:\n",
    "- Above, we computed the attention weights and context vector for input 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "871c5c64-3cec-4716-a270-15cb35accc44",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.9995, 0.9544, 0.9422, 0.4753, 0.4576, 0.6310],\n",
      "        [0.9544, 1.4950, 1.4754, 0.8434, 0.7070, 1.0865],\n",
      "        [0.9422, 1.4754, 1.4570, 0.8296, 0.7154, 1.0605],\n",
      "        [0.4753, 0.8434, 0.8296, 0.4937, 0.3474, 0.6565],\n",
      "        [0.4576, 0.7070, 0.7154, 0.3474, 0.6654, 0.2935],\n",
      "        [0.6310, 1.0865, 1.0605, 0.6565, 0.2935, 0.9450]])\n"
     ]
    }
   ],
   "source": [
    "attn_scores = torch.empty(6, 6)\n",
    "\n",
    "for i, x_i in enumerate(inputs):\n",
    "    for j, x_j in enumerate(inputs):\n",
    "        attn_scores[i, j] = torch.dot(x_i, x_j)\n",
    "\n",
    "print(attn_scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9ab8271-b7d2-4e18-8fcb-82d3147a2434",
   "metadata": {},
   "source": [
    "- We can achieve the same as above more efficiently via matrix multiplication:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "87ba8601-6626-497e-9218-802969025881",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.9995, 0.9544, 0.9422, 0.4753, 0.4576, 0.6310],\n",
      "        [0.9544, 1.4950, 1.4754, 0.8434, 0.7070, 1.0865],\n",
      "        [0.9422, 1.4754, 1.4570, 0.8296, 0.7154, 1.0605],\n",
      "        [0.4753, 0.8434, 0.8296, 0.4937, 0.3474, 0.6565],\n",
      "        [0.4576, 0.7070, 0.7154, 0.3474, 0.6654, 0.2935],\n",
      "        [0.6310, 1.0865, 1.0605, 0.6565, 0.2935, 0.9450]])\n"
     ]
    }
   ],
   "source": [
    "attn_scores2 = inputs @ inputs.T\n",
    "print(attn_scores2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0a4e5447-a95e-4e52-8302-69a9a83e7792",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.allclose(attn_scores, attn_scores2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05408b1a-9542-4869-8724-fd9f0ba6ea77",
   "metadata": {},
   "source": [
    "- Now, we normalize each row so that the values in each row sum to 1:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "3ef29c5a-4d6f-44f6-868c-eec6115038c1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.2098, 0.2006, 0.1981, 0.1242, 0.1220, 0.1452],\n",
       "        [0.1385, 0.2379, 0.2333, 0.1240, 0.1082, 0.1581],\n",
       "        [0.1390, 0.2369, 0.2326, 0.1242, 0.1108, 0.1565],\n",
       "        [0.1435, 0.2074, 0.2046, 0.1462, 0.1263, 0.1720],\n",
       "        [0.1526, 0.1958, 0.1975, 0.1367, 0.1879, 0.1295],\n",
       "        [0.1385, 0.2184, 0.2128, 0.1420, 0.0988, 0.1896]])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attn_weights = torch.softmax(attn_scores, dim=-1)\n",
    "attn_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "8f2864dc-867e-4e49-a887-783ad5526a6e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9999"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# quick verification\n",
    "row_2_sum = sum([0.2098, 0.2006, 0.1981, 0.1242, 0.1220, 0.1452])\n",
    "row_2_sum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "7d9fd20b-af77-40fc-9720-70c3b66edcf1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attn_weights.sum(dim=-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e590791-7167-4032-839c-ea96db147174",
   "metadata": {},
   "source": [
    "- Apply previous step 3 to compute all context vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "3a970386-61b6-4b4a-b691-0dcdfca34fbf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.4421, 0.5931, 0.5790],\n",
       "        [0.4419, 0.6515, 0.5683],\n",
       "        [0.4431, 0.6496, 0.5671],\n",
       "        [0.4304, 0.6298, 0.5510],\n",
       "        [0.4671, 0.5910, 0.5266],\n",
       "        [0.4177, 0.6503, 0.5645]])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_context_vecs = attn_weights @ inputs\n",
    "all_context_vecs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50ac2f5e-6d9b-44d5-8b21-ffca8ac921c6",
   "metadata": {},
   "source": [
    "- As a sanity check, the previously computed context vector can be found in the 2nd row in above:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "6449cf52-afdf-42c5-9113-cd3c89d71d77",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.allclose(all_context_vecs[1], context_vec_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd8c010a-1355-4de9-9d18-6331f584ba4f",
   "metadata": {},
   "source": [
    "### Implementing self-attention with trainable weights\n",
    "- We will start by introducing the three training weight matrices $W_q$, $W_k$ and $W_v$\n",
    "- The three matrices are used to project the embedded input tokens, $x^\\text{(i)}$, into query, key, and value vectors via matrix multiplication\n",
    "    - Query vector: $q^i = W_qx^i$\n",
    "    - Key vector: $k^i = W_kx^i$\n",
    "    - Value vector: $v^i = W_vx^i$\n",
    "- The embedding dimensions of the input $x$ and the query vector $q$ can be the same or different, depending on the model's design and specific implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "e592cffa-6361-4aa6-965e-ec6e5ece7d29",
   "metadata": {},
   "outputs": [],
   "source": [
    "# here we choose different output dimensions, but usually they are the same\n",
    "\n",
    "x_2 = inputs[1] # second input element\n",
    "d_in = inputs.shape[1] # the input embedding size, d=3\n",
    "d_out = 2 # the output embedding size, d=2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75e397b5-6ab9-4742-bda0-142c6e3dc556",
   "metadata": {},
   "source": [
    "- Below, we initialize the three weight matrices; note that we are setting `requires_grad=False` to reduce clutter in the outputs for illustration purposes, but if we were to use the weight matrices for model training, we would set `requires_grad=True` to update these matrices during model training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "b9b8fa3f-f777-4f6b-b82a-c347a69d38d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(123)\n",
    "\n",
    "W_query = torch.nn.Parameter(torch.rand(d_in, d_out), requires_grad=False)\n",
    "W_key = torch.nn.Parameter(torch.rand(d_in, d_out), requires_grad=False)\n",
    "W_value = torch.nn.Parameter(torch.rand(d_in, d_out), requires_grad=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "b14cf7d6-deac-4571-a7c8-78317225fc9f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.4306, 1.4551])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query_2 = x_2 @ W_query\n",
    "key_2 = x_2 @ W_key\n",
    "value_2 = x_2 @ W_value\n",
    "\n",
    "query_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "10cbd1d9-0bfc-4c9c-bb67-e8121aeef8cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "keys.shape: torch.Size([6, 2]) \n",
      "values.shape: torch.Size([6, 2])\n"
     ]
    }
   ],
   "source": [
    "keys = inputs @ W_key\n",
    "values = inputs @ W_value\n",
    "\n",
    "print(\"keys.shape:\", keys.shape, \"\\nvalues.shape:\", values.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6752a557-3c1e-4830-87aa-c89a73a02a1f",
   "metadata": {},
   "source": [
    "- In the next step, we compute the unnormalized attention score by computing the dot product between the query and each key vector:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "a5b66996-79ab-4d96-bc27-bfd1ff6d53ec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1.8524)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "keys_2 = keys[1] \n",
    "attn_scores_22 = query_2.dot(keys_2)\n",
    "attn_scores_22"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b29e1c15-ece0-4469-ac65-54bf142ecb68",
   "metadata": {},
   "source": [
    "- Since we have 6 inputs, we have 6 attention scores for the given query vector:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "ac08d177-7483-470d-84d2-ff29e3d789b2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1.2705, 1.8524, 1.8111, 1.0795, 0.5577, 1.5440])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attn_scores_2 = query_2 @ keys.T # All attention scores for given query\n",
    "attn_scores_2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "725d6f56-8a5e-4d4e-bac0-81c34b8393c7",
   "metadata": {},
   "source": [
    "- (3) we compute the attention weights (normalized attention scores that sum up to 1) using the softmax function\n",
    "- The difference to earlier is that we now scale the attention scores by dividing them by the square root of the embedding dimension $\\sqrt{d_k}$, which is equivalent to `d_k**0.5`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "36dc8e19-b4ce-4ae8-8012-80335adfd65a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.1500, 0.2264, 0.2199, 0.1311, 0.0906, 0.1820])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d_k = keys.shape[1]\n",
    "attn_weights_2 = torch.softmax(attn_scores_2 / d_k**0.5, dim=-1)\n",
    "attn_weights_2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3edeef9e-8dcb-44b7-8081-22465e0ec7f5",
   "metadata": {},
   "source": [
    "- (4) We now, compute the context vector for input query vector 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "95d1c4e7-76a9-47c1-a157-0159c221c708",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.3061, 0.8210])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "context_vec_2 = attn_weights_2 @ values\n",
    "context_vec_2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b404988c-8215-41ac-a454-4a05bfbfb3bf",
   "metadata": {},
   "source": [
    "### Implementing a compact SelfAttention class\n",
    "- Putting it all together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "39dbbf21-a713-4d81-afaf-6af5c0116a88",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.2996, 0.8053],\n",
      "        [0.3061, 0.8210],\n",
      "        [0.3058, 0.8203],\n",
      "        [0.2948, 0.7939],\n",
      "        [0.2927, 0.7891],\n",
      "        [0.2990, 0.8040]], grad_fn=<MmBackward0>)\n"
     ]
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class SelfAttention_v1(nn.Module):\n",
    "\n",
    "    def __init__(self, d_in, d_out):\n",
    "        super().__init__()\n",
    "        self.W_query = nn.Parameter(torch.rand(d_in, d_out))\n",
    "        self.W_key   = nn.Parameter(torch.rand(d_in, d_out))\n",
    "        self.W_value = nn.Parameter(torch.rand(d_in, d_out))\n",
    "\n",
    "    def forward(self, x):\n",
    "        keys = x @ self.W_key\n",
    "        queries = x @ self.W_query\n",
    "        values = x @ self.W_value\n",
    "        \n",
    "        attn_scores = queries @ keys.T # omega\n",
    "        attn_weights = torch.softmax(\n",
    "            attn_scores / keys.shape[-1]**0.5, dim=-1\n",
    "        )\n",
    "\n",
    "        context_vec = attn_weights @ values\n",
    "        return context_vec\n",
    "\n",
    "torch.manual_seed(123)\n",
    "sa_v1 = SelfAttention_v1(d_in, d_out)\n",
    "print(sa_v1(inputs))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33846362-c857-409d-9efc-2a335fe6fc5a",
   "metadata": {},
   "source": [
    "- We can streamline the implementation above using PyTorch's Linear layers, which are equivalent to a matrix multiplication if we disable the bias units\n",
    "- Another big advantage of using `nn.Linear` over a manual `nn.Parameter(...)` approach is that `nn.Linear` has a preferred weight initialization scheme, which leads to more stable model approaching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "95d8e602-a412-4bb6-bbc2-08f986c20325",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.0739,  0.0713],\n",
      "        [-0.0748,  0.0703],\n",
      "        [-0.0749,  0.0702],\n",
      "        [-0.0760,  0.0685],\n",
      "        [-0.0763,  0.0679],\n",
      "        [-0.0754,  0.0693]], grad_fn=<MmBackward0>)\n"
     ]
    }
   ],
   "source": [
    "class SelfAttention_v2(nn.Module):\n",
    "\n",
    "    def __init__(self, d_in, d_out, qkv_bias=False):\n",
    "        super().__init__()\n",
    "        self.W_query = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "        self.W_key   = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "        self.W_value = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "\n",
    "    def forward(self, x):\n",
    "        keys = self.W_key(x)\n",
    "        queries = self.W_query(x)\n",
    "        values = self.W_value(x)\n",
    "        \n",
    "        attn_scores = queries @ keys.T\n",
    "        attn_weights = torch.softmax(attn_scores / keys.shape[-1]**0.5, dim=-1)\n",
    "\n",
    "        context_vec = attn_weights @ values\n",
    "        return context_vec\n",
    "\n",
    "torch.manual_seed(789)\n",
    "sa_v2 = SelfAttention_v2(d_in, d_out)\n",
    "print(sa_v2(inputs))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "987987b0-f0ca-49f0-90e1-852101fd77fc",
   "metadata": {},
   "source": [
    "- In casual attention, the attention weights above the diagonal are masked, ensuring that for any given input, the LLM is unable to utilize future tokens while calculating the context vectors with the attention weight"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1f0b848-2238-4fc3-8f6c-eb6e901a652e",
   "metadata": {},
   "source": [
    "### Applying a casual attention mask\n",
    "- casual attention ensures that the model's prediction for a certain position in a sequence is only dependent on the known outputs at previous positions, not on future positions\n",
    "- In simpler words, this ensures that each next word prediction should only depend on the preceding words\n",
    "- To achieve this, for each given token, we mask out the future tokens (the ones that come after the current token in the input text):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "1905deca-b35c-4856-b221-69d6cc8ae4ff",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'SelfAttention_v2' object has no attribute 'W_keys'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[36], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Reuse the query and key weight matrices of the from above\u001b[39;00m\n\u001b[1;32m      3\u001b[0m queries \u001b[38;5;241m=\u001b[39m sa_v2\u001b[38;5;241m.\u001b[39mW_query(inputs)\n\u001b[0;32m----> 4\u001b[0m keys \u001b[38;5;241m=\u001b[39m \u001b[43msa_v2\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mW_keys\u001b[49m(inputs)\n\u001b[1;32m      5\u001b[0m attn_scores \u001b[38;5;241m=\u001b[39m queries \u001b[38;5;241m@\u001b[39m keys\u001b[38;5;241m.\u001b[39mT\n\u001b[1;32m      7\u001b[0m attn_weights \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39msoftmax(attn_scores \u001b[38;5;241m/\u001b[39m keys\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m0.5\u001b[39m, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/torch/nn/modules/module.py:1688\u001b[0m, in \u001b[0;36mModule.__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   1686\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m modules:\n\u001b[1;32m   1687\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m modules[name]\n\u001b[0;32m-> 1688\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(\u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m object has no attribute \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'SelfAttention_v2' object has no attribute 'W_keys'"
     ]
    }
   ],
   "source": [
    "# Reuse the query and key weight matrices of the from above\n",
    "\n",
    "queries = sa_v2.W_query(inputs)\n",
    "keys = sa_v2.W_key(inputs)\n",
    "attn_scores = queries @ keys.T\n",
    "\n",
    "attn_weights = torch.softmax(attn_scores / keys.shape[-1]**0.5, dim=-1)\n",
    "attn_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e84594e7-95fc-414a-b751-2decf945b622",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
